{"cells":[{"cell_type":"code","source":["!pip install segmentation-models-pytorch==0.3.3 --quiet\n","!pip install --upgrade torch torchvision --quiet\n","\n","import torch\n","import segmentation_models_pytorch as smp\n","from torch import nn\n","\n","# Recommended safety flags\n","import torch.backends.cudnn as cudnn\n","cudnn.enabled = True\n","cudnn.benchmark = False\n","cudnn.deterministic = True\n","\n","# Device\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","# Recreate model AFTER setting device\n","model = smp.Unet(\n","    encoder_name=\"resnet34\",\n","    encoder_weights=\"imagenet\",\n","    in_channels=3,\n","    classes=1,\n",")\n","model = model.to(device)\n"],"metadata":{"id":"2VFz6My5MYtN","executionInfo":{"status":"ok","timestamp":1745351170950,"user_tz":240,"elapsed":124930,"user":{"displayName":"Ashok Kamath","userId":"01269285531323045015"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"244253aa-c40e-4353-bfff-5967633efdaf"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/58.8 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.8/58.8 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m68.5/68.5 kB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m106.7/106.7 kB\u001b[0m \u001b[31m9.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.2/2.2 MB\u001b[0m \u001b[31m58.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m83.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m85.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m44.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m12.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m11.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m53.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Building wheel for efficientnet-pytorch (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Building wheel for pretrainedmodels (setup.py) ... \u001b[?25l\u001b[?25hdone\n"]},{"output_type":"stream","name":"stderr","text":["Downloading: \"https://download.pytorch.org/models/resnet34-333f7ec4.pth\" to /root/.cache/torch/hub/checkpoints/resnet34-333f7ec4.pth\n","100%|██████████| 83.3M/83.3M [00:00<00:00, 272MB/s]\n"]}]},{"cell_type":"code","execution_count":5,"metadata":{"executionInfo":{"elapsed":576,"status":"ok","timestamp":1745351171536,"user":{"displayName":"Ashok Kamath","userId":"01269285531323045015"},"user_tz":240},"id":"H-XZJlmUSV_y"},"outputs":[],"source":["import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","from torch.utils.data import DataLoader\n","from torchvision import transforms\n","from tqdm import tqdm\n","import numpy as np\n","import os\n","import pandas as pd\n"]},{"cell_type":"code","source":["# Dummy tensor\n","x = torch.randn(1, 3, 256, 256).to(device)\n","with torch.no_grad():\n","    y = model(x)\n","print(\"Output shape:\", y.shape)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"D7WIPOlIMeXK","executionInfo":{"status":"ok","timestamp":1745351172187,"user_tz":240,"elapsed":653,"user":{"displayName":"Ashok Kamath","userId":"01269285531323045015"}},"outputId":"11c888c6-cdec-4f6e-e6d2-442bf35beaff"},"execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":["Output shape: torch.Size([1, 1, 256, 256])\n"]}]},{"cell_type":"code","execution_count":7,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":20,"status":"ok","timestamp":1745351172208,"user":{"displayName":"Ashok Kamath","userId":"01269285531323045015"},"user_tz":240},"id":"mh1ypB2bYeZW","outputId":"905fde76-b548-4156-8d7d-bf80e9dac44c"},"outputs":[{"output_type":"stream","name":"stdout","text":["CUDA Available: True\n","cuDNN Version: 90100\n"]}],"source":["import torch\n","print(\"CUDA Available:\", torch.cuda.is_available())\n","print(\"cuDNN Version:\", torch.backends.cudnn.version())\n"]},{"cell_type":"code","execution_count":8,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":24444,"status":"ok","timestamp":1745351196653,"user":{"displayName":"Ashok Kamath","userId":"01269285531323045015"},"user_tz":240},"id":"-fSjFnTcwRyW","outputId":"c7a2f434-9331-4ce0-8911-4f4bcab4a1a7"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n","\n","📂 DATASET: CVC-ClinicDB\n","TRAIN IMAGES PATH: /content/drive/My Drive/GT/DL/GroupProject/Datasets/CVC-ClinicDB/train/images\n","TRAIN MASKS PATH:  /content/drive/My Drive/GT/DL/GroupProject/Datasets/CVC-ClinicDB/train/masks\n","VAL IMAGES PATH: /content/drive/My Drive/GT/DL/GroupProject/Datasets/CVC-ClinicDB/validation/images\n","VAL MASKS PATH:  /content/drive/My Drive/GT/DL/GroupProject/Datasets/CVC-ClinicDB/validation/masks\n","TEST IMAGES PATH: /content/drive/My Drive/GT/DL/GroupProject/Datasets/CVC-ClinicDB/test/images\n","TEST MASKS PATH:  /content/drive/My Drive/GT/DL/GroupProject/Datasets/CVC-ClinicDB/test/masks\n","\n","📂 DATASET: CVC-ColonDB\n","TRAIN IMAGES PATH: /content/drive/My Drive/GT/DL/GroupProject/Datasets/CVC-ColonDB/train/images\n","TRAIN MASKS PATH:  /content/drive/My Drive/GT/DL/GroupProject/Datasets/CVC-ColonDB/train/masks\n","VAL IMAGES PATH: /content/drive/My Drive/GT/DL/GroupProject/Datasets/CVC-ColonDB/validation/images\n","VAL MASKS PATH:  /content/drive/My Drive/GT/DL/GroupProject/Datasets/CVC-ColonDB/validation/masks\n","TEST IMAGES PATH: /content/drive/My Drive/GT/DL/GroupProject/Datasets/CVC-ColonDB/test/images\n","TEST MASKS PATH:  /content/drive/My Drive/GT/DL/GroupProject/Datasets/CVC-ColonDB/test/masks\n","\n","📂 DATASET: ETIS-LaribPolypDB\n","TRAIN IMAGES PATH: /content/drive/My Drive/GT/DL/GroupProject/Datasets/ETIS-LaribPolypDB/train/images\n","TRAIN MASKS PATH:  /content/drive/My Drive/GT/DL/GroupProject/Datasets/ETIS-LaribPolypDB/train/masks\n","VAL IMAGES PATH: /content/drive/My Drive/GT/DL/GroupProject/Datasets/ETIS-LaribPolypDB/validation/images\n","VAL MASKS PATH:  /content/drive/My Drive/GT/DL/GroupProject/Datasets/ETIS-LaribPolypDB/validation/masks\n","TEST IMAGES PATH: /content/drive/My Drive/GT/DL/GroupProject/Datasets/ETIS-LaribPolypDB/test/images\n","TEST MASKS PATH:  /content/drive/My Drive/GT/DL/GroupProject/Datasets/ETIS-LaribPolypDB/test/masks\n","\n","📂 DATASET: Kvasir-SEG\n","TRAIN IMAGES PATH: /content/drive/My Drive/GT/DL/GroupProject/Datasets/Kvasir-SEG/train/images\n","TRAIN MASKS PATH:  /content/drive/My Drive/GT/DL/GroupProject/Datasets/Kvasir-SEG/train/masks\n","VAL IMAGES PATH: /content/drive/My Drive/GT/DL/GroupProject/Datasets/Kvasir-SEG/validation/images\n","VAL MASKS PATH:  /content/drive/My Drive/GT/DL/GroupProject/Datasets/Kvasir-SEG/validation/masks\n","TEST IMAGES PATH: /content/drive/My Drive/GT/DL/GroupProject/Datasets/Kvasir-SEG/test/images\n","TEST MASKS PATH:  /content/drive/My Drive/GT/DL/GroupProject/Datasets/Kvasir-SEG/test/masks\n"]}],"source":["# 📦 Mount Google Drive\n","from google.colab import drive\n","drive.mount('/content/drive')\n","\n","# 🛤️ Set base path and dataset names\n","import os\n","dataset_names = ['CVC-ClinicDB', 'CVC-ColonDB', 'ETIS-LaribPolypDB', 'Kvasir-SEG']\n","base_path = '/content/drive/My Drive/GT/DL/GroupProject/Datasets/'\n","\n","# 📁 Build paths for each dataset\n","all_dataset_paths = []\n","\n","for dataset in dataset_names:\n","    dataset_path = os.path.join(base_path, dataset)\n","    paths = {\n","        'name': dataset,\n","        'train': {\n","            'images': os.path.join(dataset_path, 'train', 'images'),\n","            'masks': os.path.join(dataset_path, 'train', 'masks')\n","        },\n","        'val': {\n","            'images': os.path.join(dataset_path, 'validation', 'images'),\n","            'masks': os.path.join(dataset_path, 'validation', 'masks')\n","        },\n","        'test': {\n","            'images': os.path.join(dataset_path, 'test', 'images'),\n","            'masks': os.path.join(dataset_path, 'test', 'masks')\n","        }\n","    }\n","    all_dataset_paths.append(paths)\n","\n","# ✅ Print all paths for each dataset\n","for dataset in all_dataset_paths:\n","    print(f\"\\n📂 DATASET: {dataset['name']}\")\n","    for split in ['train', 'val', 'test']:\n","        print(f\"{split.upper()} IMAGES PATH: {dataset[split]['images']}\")\n","        print(f\"{split.upper()} MASKS PATH:  {dataset[split]['masks']}\")\n"]},{"cell_type":"code","execution_count":9,"metadata":{"executionInfo":{"elapsed":0,"status":"ok","timestamp":1745351196655,"user":{"displayName":"Ashok Kamath","userId":"01269285531323045015"},"user_tz":240},"id":"cm6CKpFkOQaT","collapsed":true},"outputs":[],"source":["# import cv2\n","# import os\n","# import matplotlib.pyplot as plt\n","\n","# # Visualize sample pairs for each split\n","# for dataset in all_dataset_paths:\n","#   for split in ['train', 'val', 'test']:\n","#       print(f\"\\n📂 DATASET: {dataset['name']}\")\n","\n","#       image_dir = dataset[split]['images']\n","#       mask_dir = dataset[split]['masks']\n","\n","#       # Get sorted file lists\n","#       image_files = sorted(os.listdir(image_dir))\n","#       mask_files = sorted(os.listdir(mask_dir))\n","\n","#       # Pick a sample index safely (in case folders are small)\n","#       sample_idx = min(30, len(image_files) - 1)\n","\n","#       # Read image and mask\n","#       image_path = os.path.join(image_dir, image_files[sample_idx])\n","#       mask_path = os.path.join(mask_dir, mask_files[sample_idx])\n","\n","#       image = cv2.imread(image_path)\n","#       image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n","\n","#       mask = cv2.imread(mask_path, cv2.IMREAD_GRAYSCALE)\n","\n","#       # Plot\n","#       plt.figure(figsize=(10, 4))\n","#       plt.suptitle(f\"{split.upper()} SET SAMPLE\", fontsize=14)\n","\n","#       plt.subplot(1, 2, 1)\n","#       plt.imshow(image)\n","#       plt.title(\"Image\")\n","#       plt.axis(\"off\")\n","\n","#       plt.subplot(1, 2, 2)\n","#       plt.imshow(mask, cmap='gray')\n","#       plt.title(\"Mask\")\n","#       plt.axis(\"off\")\n","\n","#       plt.show()\n"]},{"cell_type":"code","execution_count":10,"metadata":{"executionInfo":{"elapsed":12,"status":"ok","timestamp":1745351196670,"user":{"displayName":"Ashok Kamath","userId":"01269285531323045015"},"user_tz":240},"id":"3qcODMPbQM_T"},"outputs":[],"source":["import os\n","from torch.utils.data import Dataset\n","from PIL import Image\n","\n","class PolypSegmentationDataset(Dataset):\n","    def __init__(self, images_dir, masks_dir, transform=None, mask_transform=None):\n","        self.images_dir = images_dir\n","        self.masks_dir = masks_dir\n","        self.image_filenames = sorted(os.listdir(images_dir))\n","        self.mask_filenames = sorted(os.listdir(masks_dir))\n","        self.transform = transform\n","        self.mask_transform = mask_transform\n","\n","        assert len(self.image_filenames) == len(self.mask_filenames), \\\n","            \"Mismatch between number of images and masks.\"\n","\n","    def __len__(self):\n","        return len(self.image_filenames)\n","\n","    def __getitem__(self, idx):\n","        # Load image and mask\n","        img_path = os.path.join(self.images_dir, self.image_filenames[idx])\n","        mask_path = os.path.join(self.masks_dir, self.mask_filenames[idx])\n","\n","        image = Image.open(img_path).convert(\"RGB\")\n","        mask = Image.open(mask_path).convert(\"L\")  # grayscale mask\n","\n","        # Apply transformations\n","        if self.transform:\n","            image = self.transform(image)\n","        if self.mask_transform:\n","            mask = self.mask_transform(mask)\n","\n","        return image, mask\n"]},{"cell_type":"code","execution_count":11,"metadata":{"executionInfo":{"elapsed":0,"status":"ok","timestamp":1745351196671,"user":{"displayName":"Ashok Kamath","userId":"01269285531323045015"},"user_tz":240},"id":"eIWxpGqESTHv"},"outputs":[],"source":["image_transform = transforms.Compose([\n","    transforms.Resize((256, 256)),\n","    transforms.ToTensor(),\n","])\n","\n","mask_transform = transforms.Compose([\n","    transforms.Resize((256, 256)),\n","    transforms.ToTensor(),  # Grayscale mask in [0,1]\n","])\n"]},{"cell_type":"code","source":["from torch.utils.data import DataLoader, ConcatDataset\n","\n","def get_data_loaders(all_dataset_paths, image_transform, mask_transform, batch_size=16, num_workers=2, pin_memory=True, combine_datasets=False):\n","    \"\"\"\n","    Returns dataloaders either per dataset or combined, depending on `combine_datasets`.\n","\n","    Args:\n","        all_dataset_paths: list of dicts with dataset paths.\n","        image_transform: transform function for input images.\n","        mask_transform: transform function for target masks.\n","        batch_size: batch size for loaders.\n","        num_workers: number of subprocesses to use for data loading.\n","        pin_memory: whether to pin memory during data transfer to GPU.\n","        combine_datasets: if True, returns combined loaders; otherwise returns one set per dataset.\n","\n","    Returns:\n","        List of loaders or single dictionary of loaders (if combined).\n","    \"\"\"\n","    if combine_datasets:\n","        train_datasets, val_datasets, test_datasets = [], [], []\n","\n","        for dataset_paths in all_dataset_paths:\n","            train_datasets.append(PolypSegmentationDataset(\n","                dataset_paths['train']['images'],\n","                dataset_paths['train']['masks'],\n","                image_transform,\n","                mask_transform\n","            ))\n","            val_datasets.append(PolypSegmentationDataset(\n","                dataset_paths['val']['images'],\n","                dataset_paths['val']['masks'],\n","                image_transform,\n","                mask_transform\n","            ))\n","            test_datasets.append(PolypSegmentationDataset(\n","                dataset_paths['test']['images'],\n","                dataset_paths['test']['masks'],\n","                image_transform,\n","                mask_transform\n","            ))\n","\n","        train_dataset = ConcatDataset(train_datasets)\n","        val_dataset = ConcatDataset(val_datasets)\n","        test_dataset = ConcatDataset(test_datasets)\n","\n","        loaders = {\n","            'name': 'CombinedDataset',\n","            'train_dataset': train_dataset,\n","            'val_dataset': val_dataset,\n","            'test_dataset': test_dataset,\n","            'train_loader': DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=num_workers, pin_memory=pin_memory),\n","            'val_loader': DataLoader(val_dataset, batch_size=batch_size, num_workers=num_workers, pin_memory=pin_memory),\n","            'test_loader': DataLoader(test_dataset, batch_size=batch_size, num_workers=num_workers, pin_memory=pin_memory)\n","        }\n","\n","        # ✅ Print summary for combined dataset\n","        print(f\"\\n✅ Combined dataset loaded:\")\n","        print(f\"Train samples: {len(train_dataset)}\")\n","        print(f\"Val samples:   {len(val_dataset)}\")\n","        print(f\"Test samples:  {len(test_dataset)}\")\n","\n","        return loaders\n","\n","    else:\n","        all_data_loaders = []\n","\n","        for dataset_paths in all_dataset_paths:\n","            dataset_name = dataset_paths['name']\n","\n","            train_dataset = PolypSegmentationDataset(\n","                dataset_paths['train']['images'],\n","                dataset_paths['train']['masks'],\n","                image_transform,\n","                mask_transform\n","            )\n","            val_dataset = PolypSegmentationDataset(\n","                dataset_paths['val']['images'],\n","                dataset_paths['val']['masks'],\n","                image_transform,\n","                mask_transform\n","            )\n","            test_dataset = PolypSegmentationDataset(\n","                dataset_paths['test']['images'],\n","                dataset_paths['test']['masks'],\n","                image_transform,\n","                mask_transform\n","            )\n","\n","            loaders = {\n","                'name': dataset_name,\n","                'train_dataset': train_dataset,\n","                'val_dataset': val_dataset,\n","                'test_dataset': test_dataset,\n","                'train_loader': DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=num_workers, pin_memory=pin_memory),\n","                'val_loader': DataLoader(val_dataset, batch_size=batch_size, num_workers=num_workers, pin_memory=pin_memory),\n","                'test_loader': DataLoader(test_dataset, batch_size=batch_size, num_workers=num_workers, pin_memory=pin_memory)\n","            }\n","\n","            all_data_loaders.append(loaders)\n","\n","            # ✅ Print summary per dataset\n","            print(f\"\\n📦 {dataset_name} loaded:\")\n","            print(f\"Train samples: {len(train_dataset)}\")\n","            print(f\"Val samples:   {len(val_dataset)}\")\n","            print(f\"Test samples:  {len(test_dataset)}\")\n","\n","        return all_data_loaders\n"],"metadata":{"id":"0GKmqrE3dcQX","executionInfo":{"status":"ok","timestamp":1745351196981,"user_tz":240,"elapsed":309,"user":{"displayName":"Ashok Kamath","userId":"01269285531323045015"}}},"execution_count":12,"outputs":[]},{"cell_type":"code","source":["combined_loaders = get_data_loaders(all_dataset_paths, image_transform, mask_transform,batch_size = 32, combine_datasets=True)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Cdyg3GybeI5L","executionInfo":{"status":"ok","timestamp":1745351216935,"user_tz":240,"elapsed":19955,"user":{"displayName":"Ashok Kamath","userId":"01269285531323045015"}},"outputId":"b53b6124-53e3-4d59-ed3c-62766878058d"},"execution_count":13,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","✅ Combined dataset loaded:\n","Train samples: 1748\n","Val samples:   220\n","Test samples:  220\n"]}]},{"cell_type":"code","source":["separate_loaders = get_data_loaders(all_dataset_paths, image_transform, mask_transform, combine_datasets=False)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"JQ2rcTfpezzY","executionInfo":{"status":"ok","timestamp":1745351216940,"user_tz":240,"elapsed":4,"user":{"displayName":"Ashok Kamath","userId":"01269285531323045015"}},"outputId":"92026385-d5ac-4966-82bf-e38766de1ff1"},"execution_count":14,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","📦 CVC-ClinicDB loaded:\n","Train samples: 488\n","Val samples:   62\n","Test samples:  62\n","\n","📦 CVC-ColonDB loaded:\n","Train samples: 304\n","Val samples:   38\n","Test samples:  38\n","\n","📦 ETIS-LaribPolypDB loaded:\n","Train samples: 156\n","Val samples:   20\n","Test samples:  20\n","\n","📦 Kvasir-SEG loaded:\n","Train samples: 800\n","Val samples:   100\n","Test samples:  100\n"]}]},{"cell_type":"code","execution_count":15,"metadata":{"executionInfo":{"elapsed":51,"status":"ok","timestamp":1745351216995,"user":{"displayName":"Ashok Kamath","userId":"01269285531323045015"},"user_tz":240},"id":"PgSaBshOTIz9","colab":{"base_uri":"https://localhost:8080/"},"outputId":"87adff71-0739-4851-c1c4-8e6a37182392"},"outputs":[{"output_type":"stream","name":"stdout","text":["\n","🧠 Combined Train Dataset: 1748 samples\n","\n","📦 CVC-ClinicDB loaded:\n","Train samples: 488\n","Val samples:   62\n","Test samples:  62\n","\n","📦 CVC-ColonDB loaded:\n","Train samples: 304\n","Val samples:   38\n","Test samples:  38\n","\n","📦 ETIS-LaribPolypDB loaded:\n","Train samples: 156\n","Val samples:   20\n","Test samples:  20\n","\n","📦 Kvasir-SEG loaded:\n","Train samples: 800\n","Val samples:   100\n","Test samples:  100\n"]}],"source":["from torch.utils.data import DataLoader, ConcatDataset\n","\n","# 🧠 Assume: all_dataset_paths is already defined as in the previous cell\n","# and image_transform, mask_transform are also defined\n","\n","combined_train_datasets = []\n","all_data_loaders = []\n","\n","for dataset_paths in all_dataset_paths:\n","    dataset_name = dataset_paths['name']\n","\n","    # Create datasets\n","    train_dataset = PolypSegmentationDataset(\n","        dataset_paths['train']['images'],\n","        dataset_paths['train']['masks'],\n","        image_transform,\n","        mask_transform\n","    )\n","    val_dataset = PolypSegmentationDataset(\n","        dataset_paths['val']['images'],\n","        dataset_paths['val']['masks'],\n","        image_transform,\n","        mask_transform\n","    )\n","    test_dataset = PolypSegmentationDataset(\n","        dataset_paths['test']['images'],\n","        dataset_paths['test']['masks'],\n","        image_transform,\n","        mask_transform\n","    )\n","\n","    combined_train_datasets.append(train_dataset)\n","\n","    # Create dataloaders\n","    loaders = {\n","        'name': dataset_name,\n","        'train_dataset': train_dataset,\n","        'val_dataset': val_dataset,\n","        'test_dataset': test_dataset,\n","        'train_loader': DataLoader(train_dataset, batch_size=16, shuffle=True, num_workers = 2, pin_memory = True),\n","        'val_loader': DataLoader(val_dataset, batch_size=16, num_workers = 2, pin_memory = True),\n","        'test_loader': DataLoader(test_dataset, batch_size=16, num_workers = 2, pin_memory = True)\n","    }\n","\n","    all_data_loaders.append(loaders)\n","\n","full_train_dataset = ConcatDataset(combined_train_datasets)\n","train_loader = DataLoader(full_train_dataset, batch_size=16, shuffle=True, num_workers=2, pin_memory=True)\n","\n","#all_data_loaders.append(train_loader)\n","print(f\"\\n🧠 Combined Train Dataset: {len(full_train_dataset)} samples\")\n","\n","# ✅ Print confirmation for each dataset\n","for loaders in all_data_loaders:\n","    print(f\"\\n📦 {loaders['name']} loaded:\")\n","    print(f\"Train samples: {len(loaders['train_dataset'])}\")\n","    print(f\"Val samples:   {len(loaders['val_dataset'])}\")\n","    print(f\"Test samples:  {len(loaders['test_dataset'])}\")\n"]},{"cell_type":"markdown","metadata":{"id":"pJ4lhYqMkJGD"},"source":["# U-Net\n","\n"]},{"cell_type":"code","execution_count":17,"metadata":{"executionInfo":{"elapsed":6,"status":"ok","timestamp":1745351219591,"user":{"displayName":"Ashok Kamath","userId":"01269285531323045015"},"user_tz":240},"id":"064Js6zOT7n3"},"outputs":[],"source":["# Dice + BCE Loss\n","# dice loss is based on dice coefficient, which is a measure of overlap between two samples\n","#  - commonly used in image segmentation tasks\n","#  - Dice loss = 1 - Dice Coefficient\n","#         - Dice coefficient ranges from 0 (no overlap) to 1 (perfect overlap)\n","#\n","def dice_loss(pred, target, smooth=1.):\n","    #\n","    pred = torch.sigmoid(pred).view(-1)\n","\n","    # flatten both tensors to prep them for comparison\n","    pred = pred.view(-1)\n","    target = target.view(-1)\n","\n","    intersection = (pred * target).sum() # pixel wise product\n","    return 1 - ((2. * intersection + smooth) / (pred.sum() + target.sum() + smooth))\n","\n","# binary cross entropy since this is a segmentation task\n","bce_loss = nn.BCEWithLogitsLoss()\n","\n","def combined_loss(pred, target):\n","    return bce_loss(pred, target) + dice_loss(pred, target)\n","\n","def iou_score(preds, masks, threshold=0.5):\n","    preds = torch.sigmoid(preds) > threshold\n","    masks = masks > 0.5\n","    preds = preds.view(-1)\n","    masks = masks.view(-1)\n","\n","    intersection = (preds & masks).float().sum()\n","    union = (preds | masks).float().sum()\n","    return ((intersection + 1e-6) / (union + 1e-6)).item()\n","\n","\n","def compute_metrics(preds, masks, threshold=0.5):\n","    # transform preds from logits to probabilities using torch.sigmoid\n","    # then transform from probabilities to binary using threshold\n","    preds = torch.sigmoid(preds) > threshold\n","\n","    masks = masks > 0.5 # binarize the mask\n","\n","    # flatten the predicted segmentations for the entire batch\n","    # from [B, 1, H, W] to a 1D vector. Basically, treating the entire batch as\n","    # 1 giant image\n","    preds = preds.view(-1)\n","    masks = masks.view(-1)\n","\n","    # use logical & to compute figures necessary for confusion matrix calculations\n","    TP = (preds & masks).sum().float() # bitwise 1 and 1 match\n","    FP = (preds & ~masks).sum().float() # bitwise pred 1 yet mask was 0 (ie false positive)\n","    FN = (~preds & masks).sum().float() # bitwise pred 0 yet mask was 1 (ie false negative)\n","    TN = (~preds & ~masks).sum().float() # bitwise pred 0 and mask 0 (ie true negative)\n","\n","    epsilon = 1e-6 # add epsilon to make sure there's no division by 0\n","    precision = TP / (TP + FP + epsilon)\n","    recall = TP / (TP + FN + epsilon)\n","    accuracy = (TP + TN) / (TP + TN + FP + FN + epsilon)\n","    dice = 2 * TP / (2 * TP + FP + FN + epsilon)\n","    jaccard = TP / (TP + FP + FN + epsilon)\n","\n","    return {\n","        'precision': precision.item(),\n","        'recall': recall.item(),\n","        'accuracy': accuracy.item(),\n","        'dice': dice.item(),\n","        'jaccard': jaccard.item()\n","    }\n","\n"]},{"cell_type":"code","execution_count":18,"metadata":{"executionInfo":{"elapsed":22,"status":"ok","timestamp":1745351219609,"user":{"displayName":"Ashok Kamath","userId":"01269285531323045015"},"user_tz":240},"id":"tamURjMzT9QL"},"outputs":[],"source":["def train_epoch(model, loader, optimizer):\n","    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","    model.train()\n","    running_loss = 0\n","    for imgs, masks in tqdm(loader):\n","        imgs = imgs.cuda() if torch.cuda.is_available() else imgs\n","        masks = masks.cuda() if torch.cuda.is_available() else masks\n","        # print(f\"Image batch shape: {imgs.shape}\")\n","        # print(f\"Mask batch shape: {masks.shape}\")\n","        # print(f\"Image dtype: {imgs.dtype}, device: {imgs.device}\")\n","        # print(f\"Mask dtype: {masks.dtype}, device: {masks.device}\")\n","        preds = model(imgs)\n","        loss = combined_loss(preds, masks)\n","\n","        optimizer.zero_grad()\n","        loss.backward()\n","        optimizer.step()\n","\n","        running_loss += loss.item()\n","\n","    return running_loss / len(loader)\n","def validate_epoch(model, loader):\n","    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","    model.eval()\n","    val_loss = 0\n","    val_iou = 0\n","    metrics_accumulator = {'precision': 0, 'recall': 0, 'accuracy': 0, 'dice': 0, 'jaccard': 0}\n","\n","    with torch.no_grad():\n","        for imgs, masks in loader:\n","            imgs = imgs.to(device)\n","            masks = masks.to(device)\n","\n","            preds = model(imgs)\n","            val_loss += combined_loss(preds, masks).item()\n","            val_iou += iou_score(preds, masks)\n","\n","            metrics = compute_metrics(preds, masks)\n","            for k in metrics_accumulator:\n","                metrics_accumulator[k] += metrics[k]\n","\n","    avg_metrics = {k: v / len(loader) for k, v in metrics_accumulator.items()}\n","    avg_metrics['loss'] = val_loss / len(loader)\n","    avg_metrics['iou'] = val_iou / len(loader)\n","\n","    return avg_metrics\n","\n"]},{"cell_type":"code","source":["if torch.cuda.is_available():\n","    print(f\"🔋 Using GPU: {torch.cuda.get_device_name(0)}\")\n","else:\n","    print(\"⚠️ GPU not available. Using CPU.\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"K1ndRdSQ9CYR","executionInfo":{"status":"ok","timestamp":1745351219610,"user_tz":240,"elapsed":23,"user":{"displayName":"Ashok Kamath","userId":"01269285531323045015"}},"outputId":"01696f54-6d64-4fba-9a11-d1a062518a98"},"execution_count":19,"outputs":[{"output_type":"stream","name":"stdout","text":["🔋 Using GPU: Tesla T4\n"]}]},{"cell_type":"code","source":["import time\n","import torch\n","import pandas as pd\n","import torch.optim as optim\n","\n","def train_and_cross_test(model_class, all_data_loaders, num_epochs=10, lr=1e-4, patience=5):\n","    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","    final_results = []\n","\n","    # ⏱️ Track total training + testing time\n","    total_start_time = time.time()\n","\n","    for i, train_val_data in enumerate(all_data_loaders):\n","        print(f\"\\n🚀 Training on: {train_val_data['name']}\")\n","        model = model_class().to(device)\n","        optimizer = optim.Adam(model.parameters(), lr=lr)\n","\n","        train_loader = train_val_data['train_loader']\n","        val_loader = train_val_data['val_loader']\n","\n","        best_val_loss = float('inf')\n","        best_model_state = None\n","        epochs_without_improvement = 0\n","\n","        for epoch in range(num_epochs):\n","            print(f\"\\nEpoch {epoch+1}/{num_epochs}\")\n","\n","            # ⏱️ Epoch timing\n","            epoch_start_time = time.time()\n","            batch_times = []\n","\n","            model.train()\n","            running_loss = 0\n","            for batch_idx, (imgs, masks) in enumerate(train_loader):\n","                batch_start_time = time.time()\n","\n","                imgs = imgs.to(device)\n","                masks = masks.to(device)\n","\n","                preds = model(imgs)\n","                loss = combined_loss(preds, masks)\n","\n","                optimizer.zero_grad()\n","                loss.backward()\n","                optimizer.step()\n","\n","                running_loss += loss.item()\n","                batch_duration = time.time() - batch_start_time\n","                batch_times.append(batch_duration)\n","\n","                print(f\"  🧪 Batch {batch_idx+1}/{len(train_loader)} — Loss: {loss.item():.4f} — Time: {batch_duration:.2f}s\")\n","\n","            train_loss = running_loss / len(train_loader)\n","            val_metrics = validate_epoch(model, val_loader)\n","            val_loss = val_metrics['loss']\n","\n","            epoch_duration = time.time() - epoch_start_time\n","            avg_batch_time = sum(batch_times) / len(batch_times) if batch_times else 0\n","\n","            print(f\"🕒 Epoch Time: {epoch_duration:.2f}s | Avg Batch Time: {avg_batch_time:.2f}s\")\n","            print(f\"📊 Train Loss: {train_loss:.4f} | \"\n","                  f\"Val Loss: {val_loss:.4f} | \"\n","                  f\"IoU: {val_metrics['iou']:.4f} | \"\n","                  f\"Dice: {val_metrics['dice']:.4f} | \"\n","                  f\"Jaccard: {val_metrics['jaccard']:.4f} | \"\n","                  f\"Precision: {val_metrics['precision']:.4f} | \"\n","                  f\"Recall: {val_metrics['recall']:.4f} | \"\n","                  f\"Accuracy: {val_metrics['accuracy']:.4f}\")\n","\n","            if val_loss < best_val_loss:\n","                best_val_loss = val_loss\n","                best_model_state = model.state_dict()\n","                epochs_without_improvement = 0\n","            else:\n","                epochs_without_improvement += 1\n","                if epochs_without_improvement >= patience:\n","                    print(f\"⏹️ Early stopping triggered after {epoch+1} epochs.\")\n","                    break\n","\n","        # Restore the best model before testing\n","        if best_model_state is not None:\n","            model.load_state_dict(best_model_state)\n","\n","        print(f\"\\n🧪 Testing model trained on {train_val_data['name']} on all datasets...\")\n","\n","        test_results = []\n","        for j, test_data in enumerate(all_data_loaders):\n","            test_loader = test_data['test_loader']\n","            test_metrics = validate_epoch(model, test_loader)\n","\n","            result = {\n","                \"Trained On\": train_val_data['name'],\n","                \"Tested On\": test_data['name'],\n","                \"Loss\": test_metrics['loss'],\n","                \"IoU\": test_metrics['iou'],\n","                \"Dice\": test_metrics['dice'],\n","                \"Jaccard\": test_metrics['jaccard'],\n","                \"Precision\": test_metrics['precision'],\n","                \"Recall\": test_metrics['recall'],\n","                \"Accuracy\": test_metrics['accuracy'],\n","            }\n","\n","            test_results.append(result)\n","            final_results.append(result)\n","\n","        df = pd.DataFrame(test_results)\n","        print(df.to_markdown(index=False))\n","\n","    # ⏱️ Print total time\n","    total_duration = time.time() - total_start_time\n","    print(f\"\\n⏱️ Total Time for Training + Testing: {total_duration:.2f} seconds\")\n","\n","    print(\"\\n📋 Final Cross-Dataset Testing Summary:\")\n","    final_df = pd.DataFrame(final_results)\n","    print(final_df.to_markdown(index=False))\n","    return final_df, model\n"],"metadata":{"id":"kApDtQi03jP2","executionInfo":{"status":"ok","timestamp":1745351219611,"user_tz":240,"elapsed":21,"user":{"displayName":"Ashok Kamath","userId":"01269285531323045015"}}},"execution_count":20,"outputs":[]},{"cell_type":"code","source":["import time\n","import torch\n","import pandas as pd\n","\n","def evaluate_model_on_datasets(trained_model, all_data_loaders):\n","    \"\"\"\n","    Evaluates a trained model on all test sets in the provided data loaders.\n","\n","    Args:\n","        trained_model: A PyTorch model (with weights already loaded).\n","        all_data_loaders: List of dicts containing 'name' and 'test_loader'.\n","\n","    Returns:\n","        DataFrame with test metrics per dataset + appended row of average scores.\n","    \"\"\"\n","    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","    trained_model = trained_model.to(device)\n","    trained_model.eval()\n","\n","    print(\"\\n🧪 Evaluating trained model on all test sets...\\n\")\n","    all_test_results = []\n","\n","    test_start_time = time.time()\n","\n","    for test_data in all_data_loaders:\n","        test_loader = test_data['test_loader']\n","        dataset_name = test_data['name']\n","\n","        print(f\"🔍 Testing on: {dataset_name}\")\n","        test_metrics = validate_epoch(trained_model, test_loader)\n","\n","        result = {\n","            \"Tested On\": dataset_name,\n","            \"Loss\": test_metrics['loss'],\n","            \"IoU\": test_metrics['iou'],\n","            \"Dice\": test_metrics['dice'],\n","            \"Jaccard\": test_metrics['jaccard'],\n","            \"Precision\": test_metrics['precision'],\n","            \"Recall\": test_metrics['recall'],\n","            \"Accuracy\": test_metrics['accuracy'],\n","        }\n","\n","        all_test_results.append(result)\n","\n","    test_duration = time.time() - test_start_time\n","    print(f\"\\n⏱️ Total Evaluation Time: {test_duration:.2f} seconds\")\n","\n","    result_df = pd.DataFrame(all_test_results)\n","\n","    # 🔢 Compute average across all numeric columns\n","    avg_row = result_df.drop(columns=[\"Tested On\"]).mean()\n","    avg_row[\"Tested On\"] = \"Average\"\n","\n","    # Add the average row at the end\n","    result_df = pd.concat([result_df, pd.DataFrame([avg_row])], ignore_index=True)\n","\n","    print(\"\\n📋 Test Results Across Datasets (with Average):\")\n","    print(result_df.to_markdown(index=False))\n","\n","    return result_df\n"],"metadata":{"id":"SqV2NhDdu7Ry","executionInfo":{"status":"ok","timestamp":1745351219619,"user_tz":240,"elapsed":8,"user":{"displayName":"Ashok Kamath","userId":"01269285531323045015"}}},"execution_count":21,"outputs":[]},{"cell_type":"markdown","source":["# U-Net"],"metadata":{"id":"0LL2sGRrH_R1"}},{"cell_type":"code","source":["!pip install segmentation-models-pytorch --quiet\n","\n","def unet_model_factory():\n","    return smp.Unet(\n","        encoder_name=\"efficientnet-b0\",     # encoder backbone\n","        encoder_weights=\"imagenet\",  # pretrained weights\n","        in_channels=3,               # rgb images\n","        classes=1,                   # binary segmentation\n","    )\n"],"metadata":{"id":"yv1j0VJ77sVu","executionInfo":{"status":"ok","timestamp":1745353625460,"user_tz":240,"elapsed":2453,"user":{"displayName":"Ashok Kamath","userId":"01269285531323045015"}}},"execution_count":27,"outputs":[]},{"cell_type":"code","source":["u_net_results, trained_model = train_and_cross_test(model_class=unet_model_factory, all_data_loaders=[combined_loaders], num_epochs=100)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"7L4_fPFkQVxg","outputId":"4637372c-5076-4f22-c3d9-6f06f4f97772","executionInfo":{"status":"ok","timestamp":1745354617291,"user_tz":240,"elapsed":979250,"user":{"displayName":"Ashok Kamath","userId":"01269285531323045015"}}},"execution_count":28,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","🚀 Training on: CombinedDataset\n"]},{"output_type":"stream","name":"stderr","text":["Downloading: \"https://github.com/lukemelas/EfficientNet-PyTorch/releases/download/1.0/efficientnet-b0-355c32eb.pth\" to /root/.cache/torch/hub/checkpoints/efficientnet-b0-355c32eb.pth\n","100%|██████████| 20.4M/20.4M [00:00<00:00, 288MB/s]"]},{"output_type":"stream","name":"stdout","text":["\n","Epoch 1/100\n"]},{"output_type":"stream","name":"stderr","text":["\n"]},{"output_type":"stream","name":"stdout","text":["  🧪 Batch 1/55 — Loss: 1.8745 — Time: 0.96s\n","  🧪 Batch 2/55 — Loss: 1.8415 — Time: 0.47s\n","  🧪 Batch 3/55 — Loss: 1.8808 — Time: 0.49s\n","  🧪 Batch 4/55 — Loss: 1.8054 — Time: 0.47s\n","  🧪 Batch 5/55 — Loss: 1.7843 — Time: 0.53s\n","  🧪 Batch 6/55 — Loss: 1.7477 — Time: 0.46s\n","  🧪 Batch 7/55 — Loss: 1.7971 — Time: 0.52s\n","  🧪 Batch 8/55 — Loss: 1.8287 — Time: 0.44s\n","  🧪 Batch 9/55 — Loss: 1.7617 — Time: 0.53s\n","  🧪 Batch 10/55 — Loss: 1.7694 — Time: 0.46s\n","  🧪 Batch 11/55 — Loss: 1.7631 — Time: 0.52s\n","  🧪 Batch 12/55 — Loss: 1.7226 — Time: 0.44s\n","  🧪 Batch 13/55 — Loss: 1.7748 — Time: 0.47s\n","  🧪 Batch 14/55 — Loss: 1.6163 — Time: 0.46s\n","  🧪 Batch 15/55 — Loss: 1.6513 — Time: 0.44s\n","  🧪 Batch 16/55 — Loss: 1.7000 — Time: 0.46s\n","  🧪 Batch 17/55 — Loss: 1.6433 — Time: 0.44s\n","  🧪 Batch 18/55 — Loss: 1.6629 — Time: 0.45s\n","  🧪 Batch 19/55 — Loss: 1.7349 — Time: 0.45s\n","  🧪 Batch 20/55 — Loss: 1.6336 — Time: 0.50s\n","  🧪 Batch 21/55 — Loss: 1.6076 — Time: 0.46s\n","  🧪 Batch 22/55 — Loss: 1.6307 — Time: 0.48s\n","  🧪 Batch 23/55 — Loss: 1.6642 — Time: 0.56s\n","  🧪 Batch 24/55 — Loss: 1.5223 — Time: 0.48s\n","  🧪 Batch 25/55 — Loss: 1.5958 — Time: 0.46s\n","  🧪 Batch 26/55 — Loss: 1.5059 — Time: 0.46s\n","  🧪 Batch 27/55 — Loss: 1.5289 — Time: 0.52s\n","  🧪 Batch 28/55 — Loss: 1.5832 — Time: 0.48s\n","  🧪 Batch 29/55 — Loss: 1.4269 — Time: 0.52s\n","  🧪 Batch 30/55 — Loss: 1.5186 — Time: 0.46s\n","  🧪 Batch 31/55 — Loss: 1.4990 — Time: 0.50s\n","  🧪 Batch 32/55 — Loss: 1.5741 — Time: 0.51s\n","  🧪 Batch 33/55 — Loss: 1.5295 — Time: 0.45s\n","  🧪 Batch 34/55 — Loss: 1.5259 — Time: 0.48s\n","  🧪 Batch 35/55 — Loss: 1.4772 — Time: 0.47s\n","  🧪 Batch 36/55 — Loss: 1.4826 — Time: 0.47s\n","  🧪 Batch 37/55 — Loss: 1.4213 — Time: 0.46s\n","  🧪 Batch 38/55 — Loss: 1.3581 — Time: 0.49s\n","  🧪 Batch 39/55 — Loss: 1.4404 — Time: 0.56s\n","  🧪 Batch 40/55 — Loss: 1.4062 — Time: 0.51s\n","  🧪 Batch 41/55 — Loss: 1.4028 — Time: 0.49s\n","  🧪 Batch 42/55 — Loss: 1.4774 — Time: 0.49s\n","  🧪 Batch 43/55 — Loss: 1.3358 — Time: 0.44s\n","  🧪 Batch 44/55 — Loss: 1.3937 — Time: 0.48s\n","  🧪 Batch 45/55 — Loss: 1.2843 — Time: 0.47s\n","  🧪 Batch 46/55 — Loss: 1.4329 — Time: 0.47s\n","  🧪 Batch 47/55 — Loss: 1.3722 — Time: 0.49s\n","  🧪 Batch 48/55 — Loss: 1.3399 — Time: 0.53s\n","  🧪 Batch 49/55 — Loss: 1.3501 — Time: 0.46s\n","  🧪 Batch 50/55 — Loss: 1.2093 — Time: 0.52s\n","  🧪 Batch 51/55 — Loss: 1.2769 — Time: 0.49s\n","  🧪 Batch 52/55 — Loss: 1.4293 — Time: 0.53s\n","  🧪 Batch 53/55 — Loss: 1.2277 — Time: 0.49s\n","  🧪 Batch 54/55 — Loss: 1.3143 — Time: 0.53s\n","  🧪 Batch 55/55 — Loss: 1.4439 — Time: 0.51s\n","🕒 Epoch Time: 36.37s | Avg Batch Time: 0.49s\n","📊 Train Loss: 1.5560 | Val Loss: 1.1802 | IoU: 0.4288 | Dice: 0.5966 | Jaccard: 0.4288 | Precision: 0.5414 | Recall: 0.6664 | Accuracy: 0.9054\n","\n","Epoch 2/100\n","  🧪 Batch 1/55 — Loss: 1.2858 — Time: 0.52s\n","  🧪 Batch 2/55 — Loss: 1.3007 — Time: 0.52s\n","  🧪 Batch 3/55 — Loss: 1.2975 — Time: 0.45s\n","  🧪 Batch 4/55 — Loss: 1.1692 — Time: 0.47s\n","  🧪 Batch 5/55 — Loss: 1.2521 — Time: 0.46s\n","  🧪 Batch 6/55 — Loss: 1.1737 — Time: 0.44s\n","  🧪 Batch 7/55 — Loss: 1.1878 — Time: 0.44s\n","  🧪 Batch 8/55 — Loss: 1.1531 — Time: 0.44s\n","  🧪 Batch 9/55 — Loss: 1.3334 — Time: 0.44s\n","  🧪 Batch 10/55 — Loss: 1.2054 — Time: 0.44s\n","  🧪 Batch 11/55 — Loss: 1.0951 — Time: 0.44s\n","  🧪 Batch 12/55 — Loss: 1.1566 — Time: 0.44s\n","  🧪 Batch 13/55 — Loss: 1.2340 — Time: 0.46s\n","  🧪 Batch 14/55 — Loss: 1.1347 — Time: 0.48s\n","  🧪 Batch 15/55 — Loss: 1.2456 — Time: 0.47s\n","  🧪 Batch 16/55 — Loss: 1.0525 — Time: 0.47s\n","  🧪 Batch 17/55 — Loss: 1.1728 — Time: 0.46s\n","  🧪 Batch 18/55 — Loss: 1.1977 — Time: 0.50s\n","  🧪 Batch 19/55 — Loss: 1.0697 — Time: 0.48s\n","  🧪 Batch 20/55 — Loss: 1.0237 — Time: 0.46s\n","  🧪 Batch 21/55 — Loss: 1.1102 — Time: 0.44s\n","  🧪 Batch 22/55 — Loss: 1.2758 — Time: 0.44s\n","  🧪 Batch 23/55 — Loss: 1.0886 — Time: 0.44s\n","  🧪 Batch 24/55 — Loss: 1.0442 — Time: 0.44s\n","  🧪 Batch 25/55 — Loss: 1.1425 — Time: 0.44s\n","  🧪 Batch 26/55 — Loss: 1.0999 — Time: 0.44s\n","  🧪 Batch 27/55 — Loss: 1.0961 — Time: 0.44s\n","  🧪 Batch 28/55 — Loss: 1.0308 — Time: 0.44s\n","  🧪 Batch 29/55 — Loss: 1.1347 — Time: 0.44s\n","  🧪 Batch 30/55 — Loss: 1.0305 — Time: 0.44s\n","  🧪 Batch 31/55 — Loss: 1.2145 — Time: 0.44s\n","  🧪 Batch 32/55 — Loss: 1.0223 — Time: 0.44s\n","  🧪 Batch 33/55 — Loss: 1.0967 — Time: 0.44s\n","  🧪 Batch 34/55 — Loss: 1.1225 — Time: 0.44s\n","  🧪 Batch 35/55 — Loss: 1.0745 — Time: 0.44s\n","  🧪 Batch 36/55 — Loss: 1.0598 — Time: 0.44s\n","  🧪 Batch 37/55 — Loss: 1.0660 — Time: 0.44s\n","  🧪 Batch 38/55 — Loss: 1.0163 — Time: 0.44s\n","  🧪 Batch 39/55 — Loss: 1.0318 — Time: 0.44s\n","  🧪 Batch 40/55 — Loss: 1.0015 — Time: 0.44s\n","  🧪 Batch 41/55 — Loss: 0.9751 — Time: 0.44s\n","  🧪 Batch 42/55 — Loss: 1.0484 — Time: 0.44s\n","  🧪 Batch 43/55 — Loss: 1.0852 — Time: 0.44s\n","  🧪 Batch 44/55 — Loss: 0.9970 — Time: 0.47s\n","  🧪 Batch 45/55 — Loss: 0.9995 — Time: 0.50s\n","  🧪 Batch 46/55 — Loss: 1.0993 — Time: 0.45s\n","  🧪 Batch 47/55 — Loss: 0.9838 — Time: 0.44s\n","  🧪 Batch 48/55 — Loss: 1.0125 — Time: 0.49s\n","  🧪 Batch 49/55 — Loss: 0.9506 — Time: 0.46s\n","  🧪 Batch 50/55 — Loss: 1.0268 — Time: 0.44s\n","  🧪 Batch 51/55 — Loss: 1.0522 — Time: 0.44s\n","  🧪 Batch 52/55 — Loss: 0.9724 — Time: 0.44s\n","  🧪 Batch 53/55 — Loss: 1.0665 — Time: 0.44s\n","  🧪 Batch 54/55 — Loss: 1.0001 — Time: 0.44s\n","  🧪 Batch 55/55 — Loss: 1.0897 — Time: 0.29s\n","🕒 Epoch Time: 27.95s | Avg Batch Time: 0.45s\n","📊 Train Loss: 1.1065 | Val Loss: 0.9615 | IoU: 0.5325 | Dice: 0.6940 | Jaccard: 0.5325 | Precision: 0.8942 | Recall: 0.5684 | Accuracy: 0.9457\n","\n","Epoch 3/100\n","  🧪 Batch 1/55 — Loss: 0.9511 — Time: 0.45s\n","  🧪 Batch 2/55 — Loss: 0.9828 — Time: 0.44s\n","  🧪 Batch 3/55 — Loss: 0.9779 — Time: 0.43s\n","  🧪 Batch 4/55 — Loss: 0.9835 — Time: 0.44s\n","  🧪 Batch 5/55 — Loss: 1.0709 — Time: 0.44s\n","  🧪 Batch 6/55 — Loss: 1.0061 — Time: 0.44s\n","  🧪 Batch 7/55 — Loss: 0.9509 — Time: 0.44s\n","  🧪 Batch 8/55 — Loss: 0.9418 — Time: 0.43s\n","  🧪 Batch 9/55 — Loss: 0.9294 — Time: 0.44s\n","  🧪 Batch 10/55 — Loss: 1.0124 — Time: 0.44s\n","  🧪 Batch 11/55 — Loss: 0.9518 — Time: 0.44s\n","  🧪 Batch 12/55 — Loss: 0.9514 — Time: 0.44s\n","  🧪 Batch 13/55 — Loss: 0.9367 — Time: 0.44s\n","  🧪 Batch 14/55 — Loss: 0.9678 — Time: 0.45s\n","  🧪 Batch 15/55 — Loss: 0.9966 — Time: 0.44s\n","  🧪 Batch 16/55 — Loss: 0.9215 — Time: 0.47s\n","  🧪 Batch 17/55 — Loss: 0.8730 — Time: 0.50s\n","  🧪 Batch 18/55 — Loss: 0.8290 — Time: 0.48s\n","  🧪 Batch 19/55 — Loss: 0.9945 — Time: 0.48s\n","  🧪 Batch 20/55 — Loss: 0.8713 — Time: 0.49s\n","  🧪 Batch 21/55 — Loss: 0.9321 — Time: 0.44s\n","  🧪 Batch 22/55 — Loss: 0.8527 — Time: 0.44s\n","  🧪 Batch 23/55 — Loss: 0.8430 — Time: 0.44s\n","  🧪 Batch 24/55 — Loss: 0.8411 — Time: 0.44s\n","  🧪 Batch 25/55 — Loss: 0.9371 — Time: 0.43s\n","  🧪 Batch 26/55 — Loss: 0.8921 — Time: 0.44s\n","  🧪 Batch 27/55 — Loss: 0.8839 — Time: 0.44s\n","  🧪 Batch 28/55 — Loss: 0.8492 — Time: 0.44s\n","  🧪 Batch 29/55 — Loss: 0.9555 — Time: 0.44s\n","  🧪 Batch 30/55 — Loss: 0.8264 — Time: 0.44s\n","  🧪 Batch 31/55 — Loss: 0.8167 — Time: 0.44s\n","  🧪 Batch 32/55 — Loss: 0.9499 — Time: 0.44s\n","  🧪 Batch 33/55 — Loss: 0.9163 — Time: 0.44s\n","  🧪 Batch 34/55 — Loss: 0.9783 — Time: 0.44s\n","  🧪 Batch 35/55 — Loss: 0.7846 — Time: 0.44s\n","  🧪 Batch 36/55 — Loss: 0.9272 — Time: 0.44s\n","  🧪 Batch 37/55 — Loss: 0.8435 — Time: 0.44s\n","  🧪 Batch 38/55 — Loss: 0.8537 — Time: 0.44s\n","  🧪 Batch 39/55 — Loss: 0.8476 — Time: 0.44s\n","  🧪 Batch 40/55 — Loss: 0.8431 — Time: 0.44s\n","  🧪 Batch 41/55 — Loss: 0.8420 — Time: 0.44s\n","  🧪 Batch 42/55 — Loss: 0.8376 — Time: 0.44s\n","  🧪 Batch 43/55 — Loss: 0.9193 — Time: 0.44s\n","  🧪 Batch 44/55 — Loss: 0.8567 — Time: 0.44s\n","  🧪 Batch 45/55 — Loss: 0.8798 — Time: 0.45s\n","  🧪 Batch 46/55 — Loss: 0.8120 — Time: 0.45s\n","  🧪 Batch 47/55 — Loss: 0.8158 — Time: 0.44s\n","  🧪 Batch 48/55 — Loss: 0.8386 — Time: 0.45s\n","  🧪 Batch 49/55 — Loss: 0.9085 — Time: 0.48s\n","  🧪 Batch 50/55 — Loss: 0.9244 — Time: 0.46s\n","  🧪 Batch 51/55 — Loss: 0.9171 — Time: 0.45s\n","  🧪 Batch 52/55 — Loss: 0.8565 — Time: 0.44s\n","  🧪 Batch 53/55 — Loss: 0.7935 — Time: 0.44s\n","  🧪 Batch 54/55 — Loss: 0.9129 — Time: 0.44s\n","  🧪 Batch 55/55 — Loss: 0.8365 — Time: 0.29s\n","🕒 Epoch Time: 27.38s | Avg Batch Time: 0.44s\n","📊 Train Loss: 0.9023 | Val Loss: 0.8640 | IoU: 0.6334 | Dice: 0.7749 | Jaccard: 0.6334 | Precision: 0.9040 | Recall: 0.6796 | Accuracy: 0.9564\n","\n","Epoch 4/100\n","  🧪 Batch 1/55 — Loss: 0.8231 — Time: 0.45s\n","  🧪 Batch 2/55 — Loss: 0.7419 — Time: 0.45s\n","  🧪 Batch 3/55 — Loss: 0.8532 — Time: 0.44s\n","  🧪 Batch 4/55 — Loss: 0.8042 — Time: 0.45s\n","  🧪 Batch 5/55 — Loss: 0.7954 — Time: 0.44s\n","  🧪 Batch 6/55 — Loss: 0.9050 — Time: 0.45s\n","  🧪 Batch 7/55 — Loss: 0.7553 — Time: 0.44s\n","  🧪 Batch 8/55 — Loss: 0.7535 — Time: 0.44s\n","  🧪 Batch 9/55 — Loss: 0.7932 — Time: 0.44s\n","  🧪 Batch 10/55 — Loss: 0.6855 — Time: 0.44s\n","  🧪 Batch 11/55 — Loss: 0.7548 — Time: 0.44s\n","  🧪 Batch 12/55 — Loss: 0.8342 — Time: 0.44s\n","  🧪 Batch 13/55 — Loss: 0.7818 — Time: 0.44s\n","  🧪 Batch 14/55 — Loss: 0.6741 — Time: 0.44s\n","  🧪 Batch 15/55 — Loss: 0.7858 — Time: 0.45s\n","  🧪 Batch 16/55 — Loss: 0.7306 — Time: 0.44s\n","  🧪 Batch 17/55 — Loss: 0.9055 — Time: 0.44s\n","  🧪 Batch 18/55 — Loss: 0.8089 — Time: 0.46s\n","  🧪 Batch 19/55 — Loss: 0.7877 — Time: 0.44s\n","  🧪 Batch 20/55 — Loss: 0.8568 — Time: 0.50s\n","  🧪 Batch 21/55 — Loss: 0.8231 — Time: 0.47s\n","  🧪 Batch 22/55 — Loss: 0.7533 — Time: 0.46s\n","  🧪 Batch 23/55 — Loss: 0.7867 — Time: 0.50s\n","  🧪 Batch 24/55 — Loss: 0.8531 — Time: 0.44s\n","  🧪 Batch 25/55 — Loss: 0.8496 — Time: 0.44s\n","  🧪 Batch 26/55 — Loss: 0.8025 — Time: 0.44s\n","  🧪 Batch 27/55 — Loss: 0.8001 — Time: 0.45s\n","  🧪 Batch 28/55 — Loss: 0.7542 — Time: 0.44s\n","  🧪 Batch 29/55 — Loss: 0.7654 — Time: 0.44s\n","  🧪 Batch 30/55 — Loss: 0.7459 — Time: 0.44s\n","  🧪 Batch 31/55 — Loss: 0.7974 — Time: 0.44s\n","  🧪 Batch 32/55 — Loss: 0.7438 — Time: 0.44s\n","  🧪 Batch 33/55 — Loss: 0.9557 — Time: 0.44s\n","  🧪 Batch 34/55 — Loss: 0.7350 — Time: 0.44s\n","  🧪 Batch 35/55 — Loss: 0.8271 — Time: 0.44s\n","  🧪 Batch 36/55 — Loss: 0.8198 — Time: 0.44s\n","  🧪 Batch 37/55 — Loss: 0.8237 — Time: 0.44s\n","  🧪 Batch 38/55 — Loss: 0.8686 — Time: 0.44s\n","  🧪 Batch 39/55 — Loss: 0.7201 — Time: 0.44s\n","  🧪 Batch 40/55 — Loss: 0.7542 — Time: 0.44s\n","  🧪 Batch 41/55 — Loss: 0.6669 — Time: 0.44s\n","  🧪 Batch 42/55 — Loss: 0.8323 — Time: 0.44s\n","  🧪 Batch 43/55 — Loss: 0.6980 — Time: 0.44s\n","  🧪 Batch 44/55 — Loss: 0.7053 — Time: 0.44s\n","  🧪 Batch 45/55 — Loss: 0.7759 — Time: 0.44s\n","  🧪 Batch 46/55 — Loss: 0.7242 — Time: 0.44s\n","  🧪 Batch 47/55 — Loss: 0.7777 — Time: 0.58s\n","  🧪 Batch 48/55 — Loss: 0.6720 — Time: 0.56s\n","  🧪 Batch 49/55 — Loss: 0.6798 — Time: 0.64s\n","  🧪 Batch 50/55 — Loss: 0.7359 — Time: 0.65s\n","  🧪 Batch 51/55 — Loss: 0.7501 — Time: 0.52s\n","  🧪 Batch 52/55 — Loss: 0.7298 — Time: 0.62s\n","  🧪 Batch 53/55 — Loss: 0.6934 — Time: 0.46s\n","  🧪 Batch 54/55 — Loss: 0.7939 — Time: 0.44s\n","  🧪 Batch 55/55 — Loss: 0.6662 — Time: 0.28s\n","🕒 Epoch Time: 29.60s | Avg Batch Time: 0.46s\n","📊 Train Loss: 0.7766 | Val Loss: 0.8124 | IoU: 0.6517 | Dice: 0.7877 | Jaccard: 0.6517 | Precision: 0.8375 | Recall: 0.7443 | Accuracy: 0.9532\n","\n","Epoch 5/100\n","  🧪 Batch 1/55 — Loss: 0.7195 — Time: 0.44s\n","  🧪 Batch 2/55 — Loss: 0.6834 — Time: 0.45s\n","  🧪 Batch 3/55 — Loss: 0.6630 — Time: 0.48s\n","  🧪 Batch 4/55 — Loss: 0.7467 — Time: 0.44s\n","  🧪 Batch 5/55 — Loss: 0.6495 — Time: 0.44s\n","  🧪 Batch 6/55 — Loss: 0.6716 — Time: 0.44s\n","  🧪 Batch 7/55 — Loss: 0.7164 — Time: 0.44s\n","  🧪 Batch 8/55 — Loss: 0.6649 — Time: 0.45s\n","  🧪 Batch 9/55 — Loss: 0.7387 — Time: 0.44s\n","  🧪 Batch 10/55 — Loss: 0.7365 — Time: 0.44s\n","  🧪 Batch 11/55 — Loss: 0.7145 — Time: 0.44s\n","  🧪 Batch 12/55 — Loss: 0.6690 — Time: 0.47s\n","  🧪 Batch 13/55 — Loss: 0.7333 — Time: 0.49s\n","  🧪 Batch 14/55 — Loss: 0.6428 — Time: 0.48s\n","  🧪 Batch 15/55 — Loss: 0.7258 — Time: 0.46s\n","  🧪 Batch 16/55 — Loss: 0.7400 — Time: 0.48s\n","  🧪 Batch 17/55 — Loss: 0.6876 — Time: 0.45s\n","  🧪 Batch 18/55 — Loss: 0.6745 — Time: 0.44s\n","  🧪 Batch 19/55 — Loss: 0.6744 — Time: 0.45s\n","  🧪 Batch 20/55 — Loss: 0.7311 — Time: 0.44s\n","  🧪 Batch 21/55 — Loss: 0.7027 — Time: 0.44s\n","  🧪 Batch 22/55 — Loss: 0.6933 — Time: 0.44s\n","  🧪 Batch 23/55 — Loss: 0.6340 — Time: 0.44s\n","  🧪 Batch 24/55 — Loss: 0.5903 — Time: 0.44s\n","  🧪 Batch 25/55 — Loss: 0.6517 — Time: 0.44s\n","  🧪 Batch 26/55 — Loss: 0.5594 — Time: 0.44s\n","  🧪 Batch 27/55 — Loss: 0.6046 — Time: 0.44s\n","  🧪 Batch 28/55 — Loss: 0.6404 — Time: 0.44s\n","  🧪 Batch 29/55 — Loss: 0.6928 — Time: 0.44s\n","  🧪 Batch 30/55 — Loss: 0.7298 — Time: 0.44s\n","  🧪 Batch 31/55 — Loss: 0.6086 — Time: 0.44s\n","  🧪 Batch 32/55 — Loss: 0.7064 — Time: 0.44s\n","  🧪 Batch 33/55 — Loss: 0.6515 — Time: 0.44s\n","  🧪 Batch 34/55 — Loss: 0.6952 — Time: 0.44s\n","  🧪 Batch 35/55 — Loss: 0.6649 — Time: 0.44s\n","  🧪 Batch 36/55 — Loss: 0.5714 — Time: 0.44s\n","  🧪 Batch 37/55 — Loss: 0.5925 — Time: 0.44s\n","  🧪 Batch 38/55 — Loss: 0.6094 — Time: 0.44s\n","  🧪 Batch 39/55 — Loss: 0.6205 — Time: 0.44s\n","  🧪 Batch 40/55 — Loss: 0.5782 — Time: 0.44s\n","  🧪 Batch 41/55 — Loss: 0.5834 — Time: 0.44s\n","  🧪 Batch 42/55 — Loss: 0.5911 — Time: 0.44s\n","  🧪 Batch 43/55 — Loss: 0.6107 — Time: 0.44s\n","  🧪 Batch 44/55 — Loss: 0.6944 — Time: 0.44s\n","  🧪 Batch 45/55 — Loss: 0.5970 — Time: 0.46s\n","  🧪 Batch 46/55 — Loss: 0.6021 — Time: 0.45s\n","  🧪 Batch 47/55 — Loss: 0.5955 — Time: 0.52s\n","  🧪 Batch 48/55 — Loss: 0.6132 — Time: 0.45s\n","  🧪 Batch 49/55 — Loss: 0.5776 — Time: 0.47s\n","  🧪 Batch 50/55 — Loss: 0.6569 — Time: 0.44s\n","  🧪 Batch 51/55 — Loss: 0.5957 — Time: 0.44s\n","  🧪 Batch 52/55 — Loss: 0.6591 — Time: 0.44s\n","  🧪 Batch 53/55 — Loss: 0.6637 — Time: 0.44s\n","  🧪 Batch 54/55 — Loss: 0.6022 — Time: 0.44s\n","  🧪 Batch 55/55 — Loss: 0.6761 — Time: 0.29s\n","🕒 Epoch Time: 27.86s | Avg Batch Time: 0.44s\n","📊 Train Loss: 0.6564 | Val Loss: 0.7189 | IoU: 0.7061 | Dice: 0.8266 | Jaccard: 0.7061 | Precision: 0.9309 | Recall: 0.7445 | Accuracy: 0.9642\n","\n","Epoch 6/100\n","  🧪 Batch 1/55 — Loss: 0.5797 — Time: 0.44s\n","  🧪 Batch 2/55 — Loss: 0.5586 — Time: 0.44s\n","  🧪 Batch 3/55 — Loss: 0.5183 — Time: 0.44s\n","  🧪 Batch 4/55 — Loss: 0.5803 — Time: 0.44s\n","  🧪 Batch 5/55 — Loss: 0.5052 — Time: 0.44s\n","  🧪 Batch 6/55 — Loss: 0.5875 — Time: 0.44s\n","  🧪 Batch 7/55 — Loss: 0.5003 — Time: 0.45s\n","  🧪 Batch 8/55 — Loss: 0.6521 — Time: 0.44s\n","  🧪 Batch 9/55 — Loss: 0.5595 — Time: 0.44s\n","  🧪 Batch 10/55 — Loss: 0.6271 — Time: 0.44s\n","  🧪 Batch 11/55 — Loss: 0.5805 — Time: 0.44s\n","  🧪 Batch 12/55 — Loss: 0.5830 — Time: 0.44s\n","  🧪 Batch 13/55 — Loss: 0.5515 — Time: 0.44s\n","  🧪 Batch 14/55 — Loss: 0.5533 — Time: 0.44s\n","  🧪 Batch 15/55 — Loss: 0.7011 — Time: 0.46s\n","  🧪 Batch 16/55 — Loss: 0.6202 — Time: 0.47s\n","  🧪 Batch 17/55 — Loss: 0.5354 — Time: 0.46s\n","  🧪 Batch 18/55 — Loss: 0.5695 — Time: 0.49s\n","  🧪 Batch 19/55 — Loss: 0.5241 — Time: 0.47s\n","  🧪 Batch 20/55 — Loss: 0.5314 — Time: 0.48s\n","  🧪 Batch 21/55 — Loss: 0.5392 — Time: 0.44s\n","  🧪 Batch 22/55 — Loss: 0.5752 — Time: 0.44s\n","  🧪 Batch 23/55 — Loss: 0.5099 — Time: 0.44s\n","  🧪 Batch 24/55 — Loss: 0.5929 — Time: 0.44s\n","  🧪 Batch 25/55 — Loss: 0.5269 — Time: 0.44s\n","  🧪 Batch 26/55 — Loss: 0.6131 — Time: 0.44s\n","  🧪 Batch 27/55 — Loss: 0.5464 — Time: 0.44s\n","  🧪 Batch 28/55 — Loss: 0.5091 — Time: 0.44s\n","  🧪 Batch 29/55 — Loss: 0.5446 — Time: 0.44s\n","  🧪 Batch 30/55 — Loss: 0.5714 — Time: 0.44s\n","  🧪 Batch 31/55 — Loss: 0.5870 — Time: 0.44s\n","  🧪 Batch 32/55 — Loss: 0.4539 — Time: 0.44s\n","  🧪 Batch 33/55 — Loss: 0.6430 — Time: 0.44s\n","  🧪 Batch 34/55 — Loss: 0.6228 — Time: 0.44s\n","  🧪 Batch 35/55 — Loss: 0.4520 — Time: 0.44s\n","  🧪 Batch 36/55 — Loss: 0.4905 — Time: 0.44s\n","  🧪 Batch 37/55 — Loss: 0.5506 — Time: 0.44s\n","  🧪 Batch 38/55 — Loss: 0.6363 — Time: 0.44s\n","  🧪 Batch 39/55 — Loss: 0.4877 — Time: 0.44s\n","  🧪 Batch 40/55 — Loss: 0.4981 — Time: 0.44s\n","  🧪 Batch 41/55 — Loss: 0.6038 — Time: 0.44s\n","  🧪 Batch 42/55 — Loss: 0.5750 — Time: 0.44s\n","  🧪 Batch 43/55 — Loss: 0.5117 — Time: 0.44s\n","  🧪 Batch 44/55 — Loss: 0.5016 — Time: 0.44s\n","  🧪 Batch 45/55 — Loss: 0.5225 — Time: 0.44s\n","  🧪 Batch 46/55 — Loss: 0.4924 — Time: 0.44s\n","  🧪 Batch 47/55 — Loss: 0.5898 — Time: 0.44s\n","  🧪 Batch 48/55 — Loss: 0.4587 — Time: 0.44s\n","  🧪 Batch 49/55 — Loss: 0.5694 — Time: 0.44s\n","  🧪 Batch 50/55 — Loss: 0.6637 — Time: 0.49s\n","  🧪 Batch 51/55 — Loss: 0.5273 — Time: 0.46s\n","  🧪 Batch 52/55 — Loss: 0.5360 — Time: 0.44s\n","  🧪 Batch 53/55 — Loss: 0.6424 — Time: 0.44s\n","  🧪 Batch 54/55 — Loss: 0.4601 — Time: 0.44s\n","  🧪 Batch 55/55 — Loss: 0.5984 — Time: 0.29s\n","🕒 Epoch Time: 27.30s | Avg Batch Time: 0.44s\n","📊 Train Loss: 0.5568 | Val Loss: 0.6410 | IoU: 0.7198 | Dice: 0.8359 | Jaccard: 0.7198 | Precision: 0.9273 | Recall: 0.7617 | Accuracy: 0.9657\n","\n","Epoch 7/100\n","  🧪 Batch 1/55 — Loss: 0.5182 — Time: 0.44s\n","  🧪 Batch 2/55 — Loss: 0.5108 — Time: 0.44s\n","  🧪 Batch 3/55 — Loss: 0.4649 — Time: 0.44s\n","  🧪 Batch 4/55 — Loss: 0.5781 — Time: 0.45s\n","  🧪 Batch 5/55 — Loss: 0.4514 — Time: 0.44s\n","  🧪 Batch 6/55 — Loss: 0.4783 — Time: 0.45s\n","  🧪 Batch 7/55 — Loss: 0.4424 — Time: 0.44s\n","  🧪 Batch 8/55 — Loss: 0.5748 — Time: 0.44s\n","  🧪 Batch 9/55 — Loss: 0.5415 — Time: 0.44s\n","  🧪 Batch 10/55 — Loss: 0.5346 — Time: 0.44s\n","  🧪 Batch 11/55 — Loss: 0.5459 — Time: 0.44s\n","  🧪 Batch 12/55 — Loss: 0.5670 — Time: 0.44s\n","  🧪 Batch 13/55 — Loss: 0.4409 — Time: 0.44s\n","  🧪 Batch 14/55 — Loss: 0.4995 — Time: 0.44s\n","  🧪 Batch 15/55 — Loss: 0.5289 — Time: 0.44s\n","  🧪 Batch 16/55 — Loss: 0.4352 — Time: 0.45s\n","  🧪 Batch 17/55 — Loss: 0.5162 — Time: 0.45s\n","  🧪 Batch 18/55 — Loss: 0.4601 — Time: 0.47s\n","  🧪 Batch 19/55 — Loss: 0.4588 — Time: 0.45s\n","  🧪 Batch 20/55 — Loss: 0.5052 — Time: 0.47s\n","  🧪 Batch 21/55 — Loss: 0.4571 — Time: 0.47s\n","  🧪 Batch 22/55 — Loss: 0.5115 — Time: 0.45s\n","  🧪 Batch 23/55 — Loss: 0.4073 — Time: 0.44s\n","  🧪 Batch 24/55 — Loss: 0.4845 — Time: 0.44s\n","  🧪 Batch 25/55 — Loss: 0.4081 — Time: 0.44s\n","  🧪 Batch 26/55 — Loss: 0.3981 — Time: 0.44s\n","  🧪 Batch 27/55 — Loss: 0.4960 — Time: 0.44s\n","  🧪 Batch 28/55 — Loss: 0.3754 — Time: 0.44s\n","  🧪 Batch 29/55 — Loss: 0.5929 — Time: 0.44s\n","  🧪 Batch 30/55 — Loss: 0.5629 — Time: 0.44s\n","  🧪 Batch 31/55 — Loss: 0.4651 — Time: 0.44s\n","  🧪 Batch 32/55 — Loss: 0.4341 — Time: 0.44s\n","  🧪 Batch 33/55 — Loss: 0.4423 — Time: 0.44s\n","  🧪 Batch 34/55 — Loss: 0.4377 — Time: 0.44s\n","  🧪 Batch 35/55 — Loss: 0.3960 — Time: 0.44s\n","  🧪 Batch 36/55 — Loss: 0.4012 — Time: 0.44s\n","  🧪 Batch 37/55 — Loss: 0.4691 — Time: 0.44s\n","  🧪 Batch 38/55 — Loss: 0.3736 — Time: 0.44s\n","  🧪 Batch 39/55 — Loss: 0.4520 — Time: 0.44s\n","  🧪 Batch 40/55 — Loss: 0.4443 — Time: 0.44s\n","  🧪 Batch 41/55 — Loss: 0.4158 — Time: 0.44s\n","  🧪 Batch 42/55 — Loss: 0.4187 — Time: 0.44s\n","  🧪 Batch 43/55 — Loss: 0.4245 — Time: 0.44s\n","  🧪 Batch 44/55 — Loss: 0.4991 — Time: 0.44s\n","  🧪 Batch 45/55 — Loss: 0.5336 — Time: 0.44s\n","  🧪 Batch 46/55 — Loss: 0.3987 — Time: 0.44s\n","  🧪 Batch 47/55 — Loss: 0.4054 — Time: 0.44s\n","  🧪 Batch 48/55 — Loss: 0.4329 — Time: 0.47s\n","  🧪 Batch 49/55 — Loss: 0.4938 — Time: 0.44s\n","  🧪 Batch 50/55 — Loss: 0.4531 — Time: 0.47s\n","  🧪 Batch 51/55 — Loss: 0.4052 — Time: 0.44s\n","  🧪 Batch 52/55 — Loss: 0.4089 — Time: 0.45s\n","  🧪 Batch 53/55 — Loss: 0.4598 — Time: 0.44s\n","  🧪 Batch 54/55 — Loss: 0.4206 — Time: 0.44s\n","  🧪 Batch 55/55 — Loss: 0.4601 — Time: 0.29s\n","🕒 Epoch Time: 27.09s | Avg Batch Time: 0.44s\n","📊 Train Loss: 0.4671 | Val Loss: 0.5535 | IoU: 0.7579 | Dice: 0.8608 | Jaccard: 0.7579 | Precision: 0.9183 | Recall: 0.8121 | Accuracy: 0.9694\n","\n","Epoch 8/100\n","  🧪 Batch 1/55 — Loss: 0.4392 — Time: 0.44s\n","  🧪 Batch 2/55 — Loss: 0.4512 — Time: 0.44s\n","  🧪 Batch 3/55 — Loss: 0.5866 — Time: 0.44s\n","  🧪 Batch 4/55 — Loss: 0.3943 — Time: 0.45s\n","  🧪 Batch 5/55 — Loss: 0.4928 — Time: 0.44s\n","  🧪 Batch 6/55 — Loss: 0.3995 — Time: 0.44s\n","  🧪 Batch 7/55 — Loss: 0.4091 — Time: 0.44s\n","  🧪 Batch 8/55 — Loss: 0.4254 — Time: 0.44s\n","  🧪 Batch 9/55 — Loss: 0.3924 — Time: 0.45s\n","  🧪 Batch 10/55 — Loss: 0.4189 — Time: 0.44s\n","  🧪 Batch 11/55 — Loss: 0.3899 — Time: 0.44s\n","  🧪 Batch 12/55 — Loss: 0.4136 — Time: 0.44s\n","  🧪 Batch 13/55 — Loss: 0.4483 — Time: 0.44s\n","  🧪 Batch 14/55 — Loss: 0.4150 — Time: 0.44s\n","  🧪 Batch 15/55 — Loss: 0.3393 — Time: 0.44s\n","  🧪 Batch 16/55 — Loss: 0.4042 — Time: 0.44s\n","  🧪 Batch 17/55 — Loss: 0.3644 — Time: 0.45s\n","  🧪 Batch 18/55 — Loss: 0.3838 — Time: 0.50s\n","  🧪 Batch 19/55 — Loss: 0.3618 — Time: 0.47s\n","  🧪 Batch 20/55 — Loss: 0.4177 — Time: 0.44s\n","  🧪 Batch 21/55 — Loss: 0.4860 — Time: 0.49s\n","  🧪 Batch 22/55 — Loss: 0.3987 — Time: 0.47s\n","  🧪 Batch 23/55 — Loss: 0.4613 — Time: 0.44s\n","  🧪 Batch 24/55 — Loss: 0.3416 — Time: 0.44s\n","  🧪 Batch 25/55 — Loss: 0.3199 — Time: 0.44s\n","  🧪 Batch 26/55 — Loss: 0.4410 — Time: 0.44s\n","  🧪 Batch 27/55 — Loss: 0.4177 — Time: 0.44s\n","  🧪 Batch 28/55 — Loss: 0.5144 — Time: 0.44s\n","  🧪 Batch 29/55 — Loss: 0.3423 — Time: 0.44s\n","  🧪 Batch 30/55 — Loss: 0.4235 — Time: 0.44s\n","  🧪 Batch 31/55 — Loss: 0.4148 — Time: 0.44s\n","  🧪 Batch 32/55 — Loss: 0.4143 — Time: 0.44s\n","  🧪 Batch 33/55 — Loss: 0.3616 — Time: 0.44s\n","  🧪 Batch 34/55 — Loss: 0.4058 — Time: 0.44s\n","  🧪 Batch 35/55 — Loss: 0.3863 — Time: 0.44s\n","  🧪 Batch 36/55 — Loss: 0.3374 — Time: 0.44s\n","  🧪 Batch 37/55 — Loss: 0.3742 — Time: 0.44s\n","  🧪 Batch 38/55 — Loss: 0.3529 — Time: 0.44s\n","  🧪 Batch 39/55 — Loss: 0.3398 — Time: 0.44s\n","  🧪 Batch 40/55 — Loss: 0.4090 — Time: 0.44s\n","  🧪 Batch 41/55 — Loss: 0.3236 — Time: 0.44s\n","  🧪 Batch 42/55 — Loss: 0.4672 — Time: 0.44s\n","  🧪 Batch 43/55 — Loss: 0.4213 — Time: 0.44s\n","  🧪 Batch 44/55 — Loss: 0.3801 — Time: 0.44s\n","  🧪 Batch 45/55 — Loss: 0.3896 — Time: 0.44s\n","  🧪 Batch 46/55 — Loss: 0.3799 — Time: 0.44s\n","  🧪 Batch 47/55 — Loss: 0.3523 — Time: 0.44s\n","  🧪 Batch 48/55 — Loss: 0.3200 — Time: 0.44s\n","  🧪 Batch 49/55 — Loss: 0.4529 — Time: 0.47s\n","  🧪 Batch 50/55 — Loss: 0.4106 — Time: 0.44s\n","  🧪 Batch 51/55 — Loss: 0.3399 — Time: 0.45s\n","  🧪 Batch 52/55 — Loss: 0.3233 — Time: 0.47s\n","  🧪 Batch 53/55 — Loss: 0.4290 — Time: 0.44s\n","  🧪 Batch 54/55 — Loss: 0.3543 — Time: 0.44s\n","  🧪 Batch 55/55 — Loss: 0.3496 — Time: 0.28s\n","🕒 Epoch Time: 27.19s | Avg Batch Time: 0.44s\n","📊 Train Loss: 0.3997 | Val Loss: 0.4801 | IoU: 0.7767 | Dice: 0.8734 | Jaccard: 0.7767 | Precision: 0.9182 | Recall: 0.8342 | Accuracy: 0.9718\n","\n","Epoch 9/100\n","  🧪 Batch 1/55 — Loss: 0.3605 — Time: 0.44s\n","  🧪 Batch 2/55 — Loss: 0.3874 — Time: 0.44s\n","  🧪 Batch 3/55 — Loss: 0.3335 — Time: 0.44s\n","  🧪 Batch 4/55 — Loss: 0.4260 — Time: 0.44s\n","  🧪 Batch 5/55 — Loss: 0.3259 — Time: 0.44s\n","  🧪 Batch 6/55 — Loss: 0.3708 — Time: 0.44s\n","  🧪 Batch 7/55 — Loss: 0.4848 — Time: 0.44s\n","  🧪 Batch 8/55 — Loss: 0.3896 — Time: 0.44s\n","  🧪 Batch 9/55 — Loss: 0.3979 — Time: 0.44s\n","  🧪 Batch 10/55 — Loss: 0.3912 — Time: 0.44s\n","  🧪 Batch 11/55 — Loss: 0.3518 — Time: 0.44s\n","  🧪 Batch 12/55 — Loss: 0.3710 — Time: 0.44s\n","  🧪 Batch 13/55 — Loss: 0.3521 — Time: 0.44s\n","  🧪 Batch 14/55 — Loss: 0.3329 — Time: 0.44s\n","  🧪 Batch 15/55 — Loss: 0.3043 — Time: 0.44s\n","  🧪 Batch 16/55 — Loss: 0.3619 — Time: 0.44s\n","  🧪 Batch 17/55 — Loss: 0.3070 — Time: 0.44s\n","  🧪 Batch 18/55 — Loss: 0.3376 — Time: 0.45s\n","  🧪 Batch 19/55 — Loss: 0.3111 — Time: 0.45s\n","  🧪 Batch 20/55 — Loss: 0.3903 — Time: 0.47s\n","  🧪 Batch 21/55 — Loss: 0.3286 — Time: 0.48s\n","  🧪 Batch 22/55 — Loss: 0.3602 — Time: 0.48s\n","  🧪 Batch 23/55 — Loss: 0.3077 — Time: 0.47s\n","  🧪 Batch 24/55 — Loss: 0.3679 — Time: 0.44s\n","  🧪 Batch 25/55 — Loss: 0.3382 — Time: 0.44s\n","  🧪 Batch 26/55 — Loss: 0.3389 — Time: 0.44s\n","  🧪 Batch 27/55 — Loss: 0.3048 — Time: 0.44s\n","  🧪 Batch 28/55 — Loss: 0.3011 — Time: 0.44s\n","  🧪 Batch 29/55 — Loss: 0.3815 — Time: 0.44s\n","  🧪 Batch 30/55 — Loss: 0.2721 — Time: 0.44s\n","  🧪 Batch 31/55 — Loss: 0.3072 — Time: 0.44s\n","  🧪 Batch 32/55 — Loss: 0.3586 — Time: 0.44s\n","  🧪 Batch 33/55 — Loss: 0.3330 — Time: 0.44s\n","  🧪 Batch 34/55 — Loss: 0.3248 — Time: 0.44s\n","  🧪 Batch 35/55 — Loss: 0.3114 — Time: 0.44s\n","  🧪 Batch 36/55 — Loss: 0.2996 — Time: 0.44s\n","  🧪 Batch 37/55 — Loss: 0.2930 — Time: 0.44s\n","  🧪 Batch 38/55 — Loss: 0.2710 — Time: 0.44s\n","  🧪 Batch 39/55 — Loss: 0.2882 — Time: 0.44s\n","  🧪 Batch 40/55 — Loss: 0.3116 — Time: 0.44s\n","  🧪 Batch 41/55 — Loss: 0.3556 — Time: 0.44s\n","  🧪 Batch 42/55 — Loss: 0.3985 — Time: 0.44s\n","  🧪 Batch 43/55 — Loss: 0.2773 — Time: 0.44s\n","  🧪 Batch 44/55 — Loss: 0.3193 — Time: 0.44s\n","  🧪 Batch 45/55 — Loss: 0.3082 — Time: 0.44s\n","  🧪 Batch 46/55 — Loss: 0.3216 — Time: 0.44s\n","  🧪 Batch 47/55 — Loss: 0.3402 — Time: 0.44s\n","  🧪 Batch 48/55 — Loss: 0.3207 — Time: 0.44s\n","  🧪 Batch 49/55 — Loss: 0.3093 — Time: 0.46s\n","  🧪 Batch 50/55 — Loss: 0.3365 — Time: 0.44s\n","  🧪 Batch 51/55 — Loss: 0.3771 — Time: 0.45s\n","  🧪 Batch 52/55 — Loss: 0.3523 — Time: 0.44s\n","  🧪 Batch 53/55 — Loss: 0.3717 — Time: 0.44s\n","  🧪 Batch 54/55 — Loss: 0.3952 — Time: 0.44s\n","  🧪 Batch 55/55 — Loss: 0.3017 — Time: 0.29s\n","🕒 Epoch Time: 27.25s | Avg Batch Time: 0.44s\n","📊 Train Loss: 0.3413 | Val Loss: 0.4535 | IoU: 0.7714 | Dice: 0.8693 | Jaccard: 0.7714 | Precision: 0.9310 | Recall: 0.8172 | Accuracy: 0.9710\n","\n","Epoch 10/100\n","  🧪 Batch 1/55 — Loss: 0.2843 — Time: 0.47s\n","  🧪 Batch 2/55 — Loss: 0.2807 — Time: 0.44s\n","  🧪 Batch 3/55 — Loss: 0.3130 — Time: 0.44s\n","  🧪 Batch 4/55 — Loss: 0.2671 — Time: 0.44s\n","  🧪 Batch 5/55 — Loss: 0.2843 — Time: 0.46s\n","  🧪 Batch 6/55 — Loss: 0.3017 — Time: 0.44s\n","  🧪 Batch 7/55 — Loss: 0.2793 — Time: 0.44s\n","  🧪 Batch 8/55 — Loss: 0.3361 — Time: 0.44s\n","  🧪 Batch 9/55 — Loss: 0.3378 — Time: 0.44s\n","  🧪 Batch 10/55 — Loss: 0.4105 — Time: 0.44s\n","  🧪 Batch 11/55 — Loss: 0.3700 — Time: 0.44s\n","  🧪 Batch 12/55 — Loss: 0.3442 — Time: 0.44s\n","  🧪 Batch 13/55 — Loss: 0.2749 — Time: 0.44s\n","  🧪 Batch 14/55 — Loss: 0.3122 — Time: 0.44s\n","  🧪 Batch 15/55 — Loss: 0.4476 — Time: 0.44s\n","  🧪 Batch 16/55 — Loss: 0.3218 — Time: 0.44s\n","  🧪 Batch 17/55 — Loss: 0.2911 — Time: 0.45s\n","  🧪 Batch 18/55 — Loss: 0.2859 — Time: 0.44s\n","  🧪 Batch 19/55 — Loss: 0.3262 — Time: 0.44s\n","  🧪 Batch 20/55 — Loss: 0.3147 — Time: 0.46s\n","  🧪 Batch 21/55 — Loss: 0.3273 — Time: 0.44s\n","  🧪 Batch 22/55 — Loss: 0.3148 — Time: 0.44s\n","  🧪 Batch 23/55 — Loss: 0.3428 — Time: 0.48s\n","  🧪 Batch 24/55 — Loss: 0.3630 — Time: 0.48s\n","  🧪 Batch 25/55 — Loss: 0.2892 — Time: 0.45s\n","  🧪 Batch 26/55 — Loss: 0.2719 — Time: 0.47s\n","  🧪 Batch 27/55 — Loss: 0.3125 — Time: 0.44s\n","  🧪 Batch 28/55 — Loss: 0.3147 — Time: 0.44s\n","  🧪 Batch 29/55 — Loss: 0.3162 — Time: 0.44s\n","  🧪 Batch 30/55 — Loss: 0.2761 — Time: 0.44s\n","  🧪 Batch 31/55 — Loss: 0.3056 — Time: 0.44s\n","  🧪 Batch 32/55 — Loss: 0.2864 — Time: 0.44s\n","  🧪 Batch 33/55 — Loss: 0.2848 — Time: 0.44s\n","  🧪 Batch 34/55 — Loss: 0.3129 — Time: 0.44s\n","  🧪 Batch 35/55 — Loss: 0.2588 — Time: 0.44s\n","  🧪 Batch 36/55 — Loss: 0.3460 — Time: 0.44s\n","  🧪 Batch 37/55 — Loss: 0.2830 — Time: 0.44s\n","  🧪 Batch 38/55 — Loss: 0.2515 — Time: 0.44s\n","  🧪 Batch 39/55 — Loss: 0.2948 — Time: 0.44s\n","  🧪 Batch 40/55 — Loss: 0.3430 — Time: 0.44s\n","  🧪 Batch 41/55 — Loss: 0.2459 — Time: 0.44s\n","  🧪 Batch 42/55 — Loss: 0.3131 — Time: 0.44s\n","  🧪 Batch 43/55 — Loss: 0.3279 — Time: 0.44s\n","  🧪 Batch 44/55 — Loss: 0.2611 — Time: 0.44s\n","  🧪 Batch 45/55 — Loss: 0.3505 — Time: 0.44s\n","  🧪 Batch 46/55 — Loss: 0.2477 — Time: 0.44s\n","  🧪 Batch 47/55 — Loss: 0.2692 — Time: 0.44s\n","  🧪 Batch 48/55 — Loss: 0.3430 — Time: 0.44s\n","  🧪 Batch 49/55 — Loss: 0.2729 — Time: 0.44s\n","  🧪 Batch 50/55 — Loss: 0.2662 — Time: 0.44s\n","  🧪 Batch 51/55 — Loss: 0.2689 — Time: 0.49s\n","  🧪 Batch 52/55 — Loss: 0.2627 — Time: 0.45s\n","  🧪 Batch 53/55 — Loss: 0.2939 — Time: 0.44s\n","  🧪 Batch 54/55 — Loss: 0.3674 — Time: 0.44s\n","  🧪 Batch 55/55 — Loss: 0.3604 — Time: 0.29s\n","🕒 Epoch Time: 27.49s | Avg Batch Time: 0.44s\n","📊 Train Loss: 0.3078 | Val Loss: 0.3903 | IoU: 0.7976 | Dice: 0.8859 | Jaccard: 0.7976 | Precision: 0.8832 | Recall: 0.8907 | Accuracy: 0.9724\n","\n","Epoch 11/100\n","  🧪 Batch 1/55 — Loss: 0.2479 — Time: 0.46s\n","  🧪 Batch 2/55 — Loss: 0.2421 — Time: 0.44s\n","  🧪 Batch 3/55 — Loss: 0.2441 — Time: 0.44s\n","  🧪 Batch 4/55 — Loss: 0.2509 — Time: 0.44s\n","  🧪 Batch 5/55 — Loss: 0.2529 — Time: 0.44s\n","  🧪 Batch 6/55 — Loss: 0.2454 — Time: 0.44s\n","  🧪 Batch 7/55 — Loss: 0.2494 — Time: 0.44s\n","  🧪 Batch 8/55 — Loss: 0.2557 — Time: 0.44s\n","  🧪 Batch 9/55 — Loss: 0.3378 — Time: 0.44s\n","  🧪 Batch 10/55 — Loss: 0.3242 — Time: 0.44s\n","  🧪 Batch 11/55 — Loss: 0.2099 — Time: 0.44s\n","  🧪 Batch 12/55 — Loss: 0.2585 — Time: 0.44s\n","  🧪 Batch 13/55 — Loss: 0.2570 — Time: 0.44s\n","  🧪 Batch 14/55 — Loss: 0.2536 — Time: 0.44s\n","  🧪 Batch 15/55 — Loss: 0.2945 — Time: 0.44s\n","  🧪 Batch 16/55 — Loss: 0.3039 — Time: 0.44s\n","  🧪 Batch 17/55 — Loss: 0.3296 — Time: 0.44s\n","  🧪 Batch 18/55 — Loss: 0.2421 — Time: 0.44s\n","  🧪 Batch 19/55 — Loss: 0.3270 — Time: 0.45s\n","  🧪 Batch 20/55 — Loss: 0.2558 — Time: 0.47s\n","  🧪 Batch 21/55 — Loss: 0.3452 — Time: 0.47s\n","  🧪 Batch 22/55 — Loss: 0.2592 — Time: 0.45s\n","  🧪 Batch 23/55 — Loss: 0.2748 — Time: 0.48s\n","  🧪 Batch 24/55 — Loss: 0.2528 — Time: 0.52s\n","  🧪 Batch 25/55 — Loss: 0.2538 — Time: 0.50s\n","  🧪 Batch 26/55 — Loss: 0.2616 — Time: 0.45s\n","  🧪 Batch 27/55 — Loss: 0.2791 — Time: 0.44s\n","  🧪 Batch 28/55 — Loss: 0.2739 — Time: 0.44s\n","  🧪 Batch 29/55 — Loss: 0.2105 — Time: 0.44s\n","  🧪 Batch 30/55 — Loss: 0.3003 — Time: 0.44s\n","  🧪 Batch 31/55 — Loss: 0.2365 — Time: 0.44s\n","  🧪 Batch 32/55 — Loss: 0.2250 — Time: 0.44s\n","  🧪 Batch 33/55 — Loss: 0.2705 — Time: 0.44s\n","  🧪 Batch 34/55 — Loss: 0.2233 — Time: 0.44s\n","  🧪 Batch 35/55 — Loss: 0.2449 — Time: 0.44s\n","  🧪 Batch 36/55 — Loss: 0.2868 — Time: 0.44s\n","  🧪 Batch 37/55 — Loss: 0.2367 — Time: 0.44s\n","  🧪 Batch 38/55 — Loss: 0.3550 — Time: 0.44s\n","  🧪 Batch 39/55 — Loss: 0.2580 — Time: 0.44s\n","  🧪 Batch 40/55 — Loss: 0.2301 — Time: 0.44s\n","  🧪 Batch 41/55 — Loss: 0.2117 — Time: 0.44s\n","  🧪 Batch 42/55 — Loss: 0.2480 — Time: 0.44s\n","  🧪 Batch 43/55 — Loss: 0.2240 — Time: 0.44s\n","  🧪 Batch 44/55 — Loss: 0.2409 — Time: 0.44s\n","  🧪 Batch 45/55 — Loss: 0.2658 — Time: 0.44s\n","  🧪 Batch 46/55 — Loss: 0.2399 — Time: 0.44s\n","  🧪 Batch 47/55 — Loss: 0.2695 — Time: 0.44s\n","  🧪 Batch 48/55 — Loss: 0.2257 — Time: 0.44s\n","  🧪 Batch 49/55 — Loss: 0.2063 — Time: 0.44s\n","  🧪 Batch 50/55 — Loss: 0.2196 — Time: 0.44s\n","  🧪 Batch 51/55 — Loss: 0.2749 — Time: 0.48s\n","  🧪 Batch 52/55 — Loss: 0.2643 — Time: 0.45s\n","  🧪 Batch 53/55 — Loss: 0.2474 — Time: 0.44s\n","  🧪 Batch 54/55 — Loss: 0.2298 — Time: 0.44s\n","  🧪 Batch 55/55 — Loss: 0.2300 — Time: 0.29s\n","🕒 Epoch Time: 27.52s | Avg Batch Time: 0.44s\n","📊 Train Loss: 0.2592 | Val Loss: 0.3615 | IoU: 0.8011 | Dice: 0.8877 | Jaccard: 0.8011 | Precision: 0.9110 | Recall: 0.8671 | Accuracy: 0.9735\n","\n","Epoch 12/100\n","  🧪 Batch 1/55 — Loss: 0.2165 — Time: 0.44s\n","  🧪 Batch 2/55 — Loss: 0.2193 — Time: 0.43s\n","  🧪 Batch 3/55 — Loss: 0.2654 — Time: 0.44s\n","  🧪 Batch 4/55 — Loss: 0.2384 — Time: 0.44s\n","  🧪 Batch 5/55 — Loss: 0.2448 — Time: 0.44s\n","  🧪 Batch 6/55 — Loss: 0.2008 — Time: 0.44s\n","  🧪 Batch 7/55 — Loss: 0.2044 — Time: 0.44s\n","  🧪 Batch 8/55 — Loss: 0.1821 — Time: 0.44s\n","  🧪 Batch 9/55 — Loss: 0.2096 — Time: 0.44s\n","  🧪 Batch 10/55 — Loss: 0.2190 — Time: 0.44s\n","  🧪 Batch 11/55 — Loss: 0.2737 — Time: 0.44s\n","  🧪 Batch 12/55 — Loss: 0.2116 — Time: 0.44s\n","  🧪 Batch 13/55 — Loss: 0.3192 — Time: 0.44s\n","  🧪 Batch 14/55 — Loss: 0.2396 — Time: 0.44s\n","  🧪 Batch 15/55 — Loss: 0.2139 — Time: 0.44s\n","  🧪 Batch 16/55 — Loss: 0.1912 — Time: 0.44s\n","  🧪 Batch 17/55 — Loss: 0.2763 — Time: 0.44s\n","  🧪 Batch 18/55 — Loss: 0.2407 — Time: 0.44s\n","  🧪 Batch 19/55 — Loss: 0.2586 — Time: 0.44s\n","  🧪 Batch 20/55 — Loss: 0.2508 — Time: 0.46s\n","  🧪 Batch 21/55 — Loss: 0.2256 — Time: 0.47s\n","  🧪 Batch 22/55 — Loss: 0.2314 — Time: 0.44s\n","  🧪 Batch 23/55 — Loss: 0.2000 — Time: 0.44s\n","  🧪 Batch 24/55 — Loss: 0.1856 — Time: 0.48s\n","  🧪 Batch 25/55 — Loss: 0.2039 — Time: 0.47s\n","  🧪 Batch 26/55 — Loss: 0.2160 — Time: 0.45s\n","  🧪 Batch 27/55 — Loss: 0.2629 — Time: 0.44s\n","  🧪 Batch 28/55 — Loss: 0.2108 — Time: 0.44s\n","  🧪 Batch 29/55 — Loss: 0.1895 — Time: 0.44s\n","  🧪 Batch 30/55 — Loss: 0.2443 — Time: 0.44s\n","  🧪 Batch 31/55 — Loss: 0.2236 — Time: 0.45s\n","  🧪 Batch 32/55 — Loss: 0.2353 — Time: 0.44s\n","  🧪 Batch 33/55 — Loss: 0.2167 — Time: 0.44s\n","  🧪 Batch 34/55 — Loss: 0.2194 — Time: 0.44s\n","  🧪 Batch 35/55 — Loss: 0.2269 — Time: 0.44s\n","  🧪 Batch 36/55 — Loss: 0.2408 — Time: 0.44s\n","  🧪 Batch 37/55 — Loss: 0.2861 — Time: 0.44s\n","  🧪 Batch 38/55 — Loss: 0.2154 — Time: 0.44s\n","  🧪 Batch 39/55 — Loss: 0.1899 — Time: 0.44s\n","  🧪 Batch 40/55 — Loss: 0.2098 — Time: 0.44s\n","  🧪 Batch 41/55 — Loss: 0.2412 — Time: 0.44s\n","  🧪 Batch 42/55 — Loss: 0.2293 — Time: 0.44s\n","  🧪 Batch 43/55 — Loss: 0.2702 — Time: 0.44s\n","  🧪 Batch 44/55 — Loss: 0.2295 — Time: 0.44s\n","  🧪 Batch 45/55 — Loss: 0.2309 — Time: 0.44s\n","  🧪 Batch 46/55 — Loss: 0.2151 — Time: 0.44s\n","  🧪 Batch 47/55 — Loss: 0.2067 — Time: 0.44s\n","  🧪 Batch 48/55 — Loss: 0.2221 — Time: 0.44s\n","  🧪 Batch 49/55 — Loss: 0.2587 — Time: 0.44s\n","  🧪 Batch 50/55 — Loss: 0.2128 — Time: 0.44s\n","  🧪 Batch 51/55 — Loss: 0.1948 — Time: 0.44s\n","  🧪 Batch 52/55 — Loss: 0.1936 — Time: 0.44s\n","  🧪 Batch 53/55 — Loss: 0.1889 — Time: 0.44s\n","  🧪 Batch 54/55 — Loss: 0.1977 — Time: 0.44s\n","  🧪 Batch 55/55 — Loss: 0.2286 — Time: 0.29s\n","🕒 Epoch Time: 27.48s | Avg Batch Time: 0.44s\n","📊 Train Loss: 0.2260 | Val Loss: 0.3458 | IoU: 0.7928 | Dice: 0.8826 | Jaccard: 0.7928 | Precision: 0.8741 | Recall: 0.8925 | Accuracy: 0.9715\n","\n","Epoch 13/100\n","  🧪 Batch 1/55 — Loss: 0.1815 — Time: 0.45s\n","  🧪 Batch 2/55 — Loss: 0.1845 — Time: 0.44s\n","  🧪 Batch 3/55 — Loss: 0.2142 — Time: 0.44s\n","  🧪 Batch 4/55 — Loss: 0.1823 — Time: 0.44s\n","  🧪 Batch 5/55 — Loss: 0.1811 — Time: 0.44s\n","  🧪 Batch 6/55 — Loss: 0.1891 — Time: 0.44s\n","  🧪 Batch 7/55 — Loss: 0.2621 — Time: 0.44s\n","  🧪 Batch 8/55 — Loss: 0.1897 — Time: 0.44s\n","  🧪 Batch 9/55 — Loss: 0.2062 — Time: 0.44s\n","  🧪 Batch 10/55 — Loss: 0.1794 — Time: 0.44s\n","  🧪 Batch 11/55 — Loss: 0.1947 — Time: 0.44s\n","  🧪 Batch 12/55 — Loss: 0.2887 — Time: 0.44s\n","  🧪 Batch 13/55 — Loss: 0.1963 — Time: 0.44s\n","  🧪 Batch 14/55 — Loss: 0.2233 — Time: 0.44s\n","  🧪 Batch 15/55 — Loss: 0.1805 — Time: 0.44s\n","  🧪 Batch 16/55 — Loss: 0.1894 — Time: 0.44s\n","  🧪 Batch 17/55 — Loss: 0.1976 — Time: 0.44s\n","  🧪 Batch 18/55 — Loss: 0.2602 — Time: 0.44s\n","  🧪 Batch 19/55 — Loss: 0.2168 — Time: 0.44s\n","  🧪 Batch 20/55 — Loss: 0.3289 — Time: 0.44s\n","  🧪 Batch 21/55 — Loss: 0.1927 — Time: 0.47s\n","  🧪 Batch 22/55 — Loss: 0.1908 — Time: 0.47s\n","  🧪 Batch 23/55 — Loss: 0.2146 — Time: 0.47s\n","  🧪 Batch 24/55 — Loss: 0.2094 — Time: 0.44s\n","  🧪 Batch 25/55 — Loss: 0.1713 — Time: 0.45s\n","  🧪 Batch 26/55 — Loss: 0.2395 — Time: 0.49s\n","  🧪 Batch 27/55 — Loss: 0.2208 — Time: 0.44s\n","  🧪 Batch 28/55 — Loss: 0.2073 — Time: 0.44s\n","  🧪 Batch 29/55 — Loss: 0.2042 — Time: 0.44s\n","  🧪 Batch 30/55 — Loss: 0.2075 — Time: 0.45s\n","  🧪 Batch 31/55 — Loss: 0.2539 — Time: 0.44s\n","  🧪 Batch 32/55 — Loss: 0.1641 — Time: 0.44s\n","  🧪 Batch 33/55 — Loss: 0.2195 — Time: 0.44s\n","  🧪 Batch 34/55 — Loss: 0.2204 — Time: 0.44s\n","  🧪 Batch 35/55 — Loss: 0.2299 — Time: 0.44s\n","  🧪 Batch 36/55 — Loss: 0.2170 — Time: 0.44s\n","  🧪 Batch 37/55 — Loss: 0.2039 — Time: 0.44s\n","  🧪 Batch 38/55 — Loss: 0.2236 — Time: 0.44s\n","  🧪 Batch 39/55 — Loss: 0.2221 — Time: 0.44s\n","  🧪 Batch 40/55 — Loss: 0.1996 — Time: 0.44s\n","  🧪 Batch 41/55 — Loss: 0.1727 — Time: 0.44s\n","  🧪 Batch 42/55 — Loss: 0.1793 — Time: 0.44s\n","  🧪 Batch 43/55 — Loss: 0.2141 — Time: 0.44s\n","  🧪 Batch 44/55 — Loss: 0.2000 — Time: 0.44s\n","  🧪 Batch 45/55 — Loss: 0.2132 — Time: 0.44s\n","  🧪 Batch 46/55 — Loss: 0.1720 — Time: 0.44s\n","  🧪 Batch 47/55 — Loss: 0.3092 — Time: 0.44s\n","  🧪 Batch 48/55 — Loss: 0.1958 — Time: 0.44s\n","  🧪 Batch 49/55 — Loss: 0.1754 — Time: 0.44s\n","  🧪 Batch 50/55 — Loss: 0.2135 — Time: 0.45s\n","  🧪 Batch 51/55 — Loss: 0.1820 — Time: 0.48s\n","  🧪 Batch 52/55 — Loss: 0.1709 — Time: 0.44s\n","  🧪 Batch 53/55 — Loss: 0.2039 — Time: 0.44s\n","  🧪 Batch 54/55 — Loss: 0.1703 — Time: 0.44s\n","  🧪 Batch 55/55 — Loss: 0.2347 — Time: 0.29s\n","🕒 Epoch Time: 27.54s | Avg Batch Time: 0.44s\n","📊 Train Loss: 0.2085 | Val Loss: 0.3149 | IoU: 0.8069 | Dice: 0.8915 | Jaccard: 0.8069 | Precision: 0.9222 | Recall: 0.8642 | Accuracy: 0.9746\n","\n","Epoch 14/100\n","  🧪 Batch 1/55 — Loss: 0.2081 — Time: 0.46s\n","  🧪 Batch 2/55 — Loss: 0.1751 — Time: 0.44s\n","  🧪 Batch 3/55 — Loss: 0.1574 — Time: 0.44s\n","  🧪 Batch 4/55 — Loss: 0.1579 — Time: 0.44s\n","  🧪 Batch 5/55 — Loss: 0.1906 — Time: 0.44s\n","  🧪 Batch 6/55 — Loss: 0.2310 — Time: 0.44s\n","  🧪 Batch 7/55 — Loss: 0.1775 — Time: 0.44s\n","  🧪 Batch 8/55 — Loss: 0.2094 — Time: 0.44s\n","  🧪 Batch 9/55 — Loss: 0.1736 — Time: 0.44s\n","  🧪 Batch 10/55 — Loss: 0.1915 — Time: 0.44s\n","  🧪 Batch 11/55 — Loss: 0.1529 — Time: 0.44s\n","  🧪 Batch 12/55 — Loss: 0.1581 — Time: 0.44s\n","  🧪 Batch 13/55 — Loss: 0.2061 — Time: 0.44s\n","  🧪 Batch 14/55 — Loss: 0.1880 — Time: 0.44s\n","  🧪 Batch 15/55 — Loss: 0.2790 — Time: 0.44s\n","  🧪 Batch 16/55 — Loss: 0.1916 — Time: 0.44s\n","  🧪 Batch 17/55 — Loss: 0.1571 — Time: 0.44s\n","  🧪 Batch 18/55 — Loss: 0.2004 — Time: 0.44s\n","  🧪 Batch 19/55 — Loss: 0.1721 — Time: 0.44s\n","  🧪 Batch 20/55 — Loss: 0.1785 — Time: 0.45s\n","  🧪 Batch 21/55 — Loss: 0.1938 — Time: 0.44s\n","  🧪 Batch 22/55 — Loss: 0.1856 — Time: 0.44s\n","  🧪 Batch 23/55 — Loss: 0.2090 — Time: 0.44s\n","  🧪 Batch 24/55 — Loss: 0.2673 — Time: 0.51s\n","  🧪 Batch 25/55 — Loss: 0.2015 — Time: 0.50s\n","  🧪 Batch 26/55 — Loss: 0.1906 — Time: 0.46s\n","  🧪 Batch 27/55 — Loss: 0.1811 — Time: 0.45s\n","  🧪 Batch 28/55 — Loss: 0.1939 — Time: 0.44s\n","  🧪 Batch 29/55 — Loss: 0.2127 — Time: 0.44s\n","  🧪 Batch 30/55 — Loss: 0.1961 — Time: 0.44s\n","  🧪 Batch 31/55 — Loss: 0.2000 — Time: 0.44s\n","  🧪 Batch 32/55 — Loss: 0.1689 — Time: 0.44s\n","  🧪 Batch 33/55 — Loss: 0.1873 — Time: 0.44s\n","  🧪 Batch 34/55 — Loss: 0.1524 — Time: 0.44s\n","  🧪 Batch 35/55 — Loss: 0.1841 — Time: 0.44s\n","  🧪 Batch 36/55 — Loss: 0.2132 — Time: 0.44s\n","  🧪 Batch 37/55 — Loss: 0.2215 — Time: 0.44s\n","  🧪 Batch 38/55 — Loss: 0.1954 — Time: 0.44s\n","  🧪 Batch 39/55 — Loss: 0.1461 — Time: 0.44s\n","  🧪 Batch 40/55 — Loss: 0.1958 — Time: 0.44s\n","  🧪 Batch 41/55 — Loss: 0.1410 — Time: 0.44s\n","  🧪 Batch 42/55 — Loss: 0.1614 — Time: 0.44s\n","  🧪 Batch 43/55 — Loss: 0.1664 — Time: 0.45s\n","  🧪 Batch 44/55 — Loss: 0.1405 — Time: 0.44s\n","  🧪 Batch 45/55 — Loss: 0.2030 — Time: 0.44s\n","  🧪 Batch 46/55 — Loss: 0.2107 — Time: 0.44s\n","  🧪 Batch 47/55 — Loss: 0.1620 — Time: 0.44s\n","  🧪 Batch 48/55 — Loss: 0.1719 — Time: 0.44s\n","  🧪 Batch 49/55 — Loss: 0.1959 — Time: 0.44s\n","  🧪 Batch 50/55 — Loss: 0.1489 — Time: 0.44s\n","  🧪 Batch 51/55 — Loss: 0.1555 — Time: 0.45s\n","  🧪 Batch 52/55 — Loss: 0.1806 — Time: 0.44s\n","  🧪 Batch 53/55 — Loss: 0.1953 — Time: 0.44s\n","  🧪 Batch 54/55 — Loss: 0.2260 — Time: 0.44s\n","  🧪 Batch 55/55 — Loss: 0.1717 — Time: 0.28s\n","🕒 Epoch Time: 27.48s | Avg Batch Time: 0.44s\n","📊 Train Loss: 0.1870 | Val Loss: 0.3067 | IoU: 0.8051 | Dice: 0.8904 | Jaccard: 0.8051 | Precision: 0.9148 | Recall: 0.8687 | Accuracy: 0.9740\n","\n","Epoch 15/100\n","  🧪 Batch 1/55 — Loss: 0.1862 — Time: 0.46s\n","  🧪 Batch 2/55 — Loss: 0.1961 — Time: 0.44s\n","  🧪 Batch 3/55 — Loss: 0.1625 — Time: 0.44s\n","  🧪 Batch 4/55 — Loss: 0.2191 — Time: 0.44s\n","  🧪 Batch 5/55 — Loss: 0.1640 — Time: 0.46s\n","  🧪 Batch 6/55 — Loss: 0.1984 — Time: 0.44s\n","  🧪 Batch 7/55 — Loss: 0.1642 — Time: 0.44s\n","  🧪 Batch 8/55 — Loss: 0.1664 — Time: 0.44s\n","  🧪 Batch 9/55 — Loss: 0.1690 — Time: 0.44s\n","  🧪 Batch 10/55 — Loss: 0.1550 — Time: 0.44s\n","  🧪 Batch 11/55 — Loss: 0.1568 — Time: 0.44s\n","  🧪 Batch 12/55 — Loss: 0.2157 — Time: 0.44s\n","  🧪 Batch 13/55 — Loss: 0.1443 — Time: 0.44s\n","  🧪 Batch 14/55 — Loss: 0.1931 — Time: 0.44s\n","  🧪 Batch 15/55 — Loss: 0.1459 — Time: 0.44s\n","  🧪 Batch 16/55 — Loss: 0.2149 — Time: 0.44s\n","  🧪 Batch 17/55 — Loss: 0.1962 — Time: 0.44s\n","  🧪 Batch 18/55 — Loss: 0.1927 — Time: 0.44s\n","  🧪 Batch 19/55 — Loss: 0.1603 — Time: 0.45s\n","  🧪 Batch 20/55 — Loss: 0.1499 — Time: 0.44s\n","  🧪 Batch 21/55 — Loss: 0.1579 — Time: 0.47s\n","  🧪 Batch 22/55 — Loss: 0.1424 — Time: 0.45s\n","  🧪 Batch 23/55 — Loss: 0.1549 — Time: 0.49s\n","  🧪 Batch 24/55 — Loss: 0.1587 — Time: 0.47s\n","  🧪 Batch 25/55 — Loss: 0.1588 — Time: 0.49s\n","  🧪 Batch 26/55 — Loss: 0.1549 — Time: 0.46s\n","  🧪 Batch 27/55 — Loss: 0.1711 — Time: 0.44s\n","  🧪 Batch 28/55 — Loss: 0.1423 — Time: 0.44s\n","  🧪 Batch 29/55 — Loss: 0.1784 — Time: 0.44s\n","  🧪 Batch 30/55 — Loss: 0.1912 — Time: 0.44s\n","  🧪 Batch 31/55 — Loss: 0.1920 — Time: 0.45s\n","  🧪 Batch 32/55 — Loss: 0.1687 — Time: 0.44s\n","  🧪 Batch 33/55 — Loss: 0.1915 — Time: 0.44s\n","  🧪 Batch 34/55 — Loss: 0.1443 — Time: 0.44s\n","  🧪 Batch 35/55 — Loss: 0.1380 — Time: 0.44s\n","  🧪 Batch 36/55 — Loss: 0.1331 — Time: 0.44s\n","  🧪 Batch 37/55 — Loss: 0.1601 — Time: 0.44s\n","  🧪 Batch 38/55 — Loss: 0.1487 — Time: 0.44s\n","  🧪 Batch 39/55 — Loss: 0.1448 — Time: 0.44s\n","  🧪 Batch 40/55 — Loss: 0.1574 — Time: 0.44s\n","  🧪 Batch 41/55 — Loss: 0.1629 — Time: 0.44s\n","  🧪 Batch 42/55 — Loss: 0.1928 — Time: 0.44s\n","  🧪 Batch 43/55 — Loss: 0.1499 — Time: 0.44s\n","  🧪 Batch 44/55 — Loss: 0.2100 — Time: 0.44s\n","  🧪 Batch 45/55 — Loss: 0.1266 — Time: 0.44s\n","  🧪 Batch 46/55 — Loss: 0.1985 — Time: 0.44s\n","  🧪 Batch 47/55 — Loss: 0.1784 — Time: 0.44s\n","  🧪 Batch 48/55 — Loss: 0.1283 — Time: 0.44s\n","  🧪 Batch 49/55 — Loss: 0.1923 — Time: 0.44s\n","  🧪 Batch 50/55 — Loss: 0.1388 — Time: 0.45s\n","  🧪 Batch 51/55 — Loss: 0.1382 — Time: 0.45s\n","  🧪 Batch 52/55 — Loss: 0.1715 — Time: 0.45s\n","  🧪 Batch 53/55 — Loss: 0.2012 — Time: 0.44s\n","  🧪 Batch 54/55 — Loss: 0.1687 — Time: 0.44s\n","  🧪 Batch 55/55 — Loss: 0.1380 — Time: 0.29s\n","🕒 Epoch Time: 27.60s | Avg Batch Time: 0.44s\n","📊 Train Loss: 0.1679 | Val Loss: 0.2913 | IoU: 0.8077 | Dice: 0.8918 | Jaccard: 0.8077 | Precision: 0.9223 | Recall: 0.8650 | Accuracy: 0.9744\n","\n","Epoch 16/100\n","  🧪 Batch 1/55 — Loss: 0.1670 — Time: 0.44s\n","  🧪 Batch 2/55 — Loss: 0.1527 — Time: 0.44s\n","  🧪 Batch 3/55 — Loss: 0.1517 — Time: 0.44s\n","  🧪 Batch 4/55 — Loss: 0.1409 — Time: 0.43s\n","  🧪 Batch 5/55 — Loss: 0.1388 — Time: 0.44s\n","  🧪 Batch 6/55 — Loss: 0.1789 — Time: 0.44s\n","  🧪 Batch 7/55 — Loss: 0.2282 — Time: 0.43s\n","  🧪 Batch 8/55 — Loss: 0.1796 — Time: 0.44s\n","  🧪 Batch 9/55 — Loss: 0.1465 — Time: 0.44s\n","  🧪 Batch 10/55 — Loss: 0.1717 — Time: 0.44s\n","  🧪 Batch 11/55 — Loss: 0.1753 — Time: 0.44s\n","  🧪 Batch 12/55 — Loss: 0.1899 — Time: 0.44s\n","  🧪 Batch 13/55 — Loss: 0.1559 — Time: 0.44s\n","  🧪 Batch 14/55 — Loss: 0.1576 — Time: 0.44s\n","  🧪 Batch 15/55 — Loss: 0.1528 — Time: 0.44s\n","  🧪 Batch 16/55 — Loss: 0.1490 — Time: 0.44s\n","  🧪 Batch 17/55 — Loss: 0.1727 — Time: 0.44s\n","  🧪 Batch 18/55 — Loss: 0.1421 — Time: 0.44s\n","  🧪 Batch 19/55 — Loss: 0.1348 — Time: 0.44s\n","  🧪 Batch 20/55 — Loss: 0.1441 — Time: 0.44s\n","  🧪 Batch 21/55 — Loss: 0.1418 — Time: 0.46s\n","  🧪 Batch 22/55 — Loss: 0.1080 — Time: 0.45s\n","  🧪 Batch 23/55 — Loss: 0.1678 — Time: 0.47s\n","  🧪 Batch 24/55 — Loss: 0.1608 — Time: 0.49s\n","  🧪 Batch 25/55 — Loss: 0.1299 — Time: 0.47s\n","  🧪 Batch 26/55 — Loss: 0.1733 — Time: 0.44s\n","  🧪 Batch 27/55 — Loss: 0.1629 — Time: 0.44s\n","  🧪 Batch 28/55 — Loss: 0.1882 — Time: 0.44s\n","  🧪 Batch 29/55 — Loss: 0.2004 — Time: 0.44s\n","  🧪 Batch 30/55 — Loss: 0.1296 — Time: 0.44s\n","  🧪 Batch 31/55 — Loss: 0.1485 — Time: 0.44s\n","  🧪 Batch 32/55 — Loss: 0.1623 — Time: 0.44s\n","  🧪 Batch 33/55 — Loss: 0.1460 — Time: 0.44s\n","  🧪 Batch 34/55 — Loss: 0.1631 — Time: 0.44s\n","  🧪 Batch 35/55 — Loss: 0.1545 — Time: 0.44s\n","  🧪 Batch 36/55 — Loss: 0.1650 — Time: 0.44s\n","  🧪 Batch 37/55 — Loss: 0.1591 — Time: 0.44s\n","  🧪 Batch 38/55 — Loss: 0.1980 — Time: 0.44s\n","  🧪 Batch 39/55 — Loss: 0.1506 — Time: 0.44s\n","  🧪 Batch 40/55 — Loss: 0.1775 — Time: 0.44s\n","  🧪 Batch 41/55 — Loss: 0.1553 — Time: 0.44s\n","  🧪 Batch 42/55 — Loss: 0.1856 — Time: 0.44s\n","  🧪 Batch 43/55 — Loss: 0.1496 — Time: 0.44s\n","  🧪 Batch 44/55 — Loss: 0.1503 — Time: 0.44s\n","  🧪 Batch 45/55 — Loss: 0.1347 — Time: 0.44s\n","  🧪 Batch 46/55 — Loss: 0.1467 — Time: 0.44s\n","  🧪 Batch 47/55 — Loss: 0.1587 — Time: 0.44s\n","  🧪 Batch 48/55 — Loss: 0.1578 — Time: 0.44s\n","  🧪 Batch 49/55 — Loss: 0.1513 — Time: 0.44s\n","  🧪 Batch 50/55 — Loss: 0.1440 — Time: 0.44s\n","  🧪 Batch 51/55 — Loss: 0.1269 — Time: 0.46s\n","  🧪 Batch 52/55 — Loss: 0.1514 — Time: 0.44s\n","  🧪 Batch 53/55 — Loss: 0.1364 — Time: 0.44s\n","  🧪 Batch 54/55 — Loss: 0.2060 — Time: 0.44s\n","  🧪 Batch 55/55 — Loss: 0.1300 — Time: 0.29s\n","🕒 Epoch Time: 27.63s | Avg Batch Time: 0.44s\n","📊 Train Loss: 0.1582 | Val Loss: 0.2757 | IoU: 0.8130 | Dice: 0.8951 | Jaccard: 0.8130 | Precision: 0.9067 | Recall: 0.8854 | Accuracy: 0.9746\n","\n","Epoch 17/100\n","  🧪 Batch 1/55 — Loss: 0.1291 — Time: 0.47s\n","  🧪 Batch 2/55 — Loss: 0.1237 — Time: 0.44s\n","  🧪 Batch 3/55 — Loss: 0.1227 — Time: 0.44s\n","  🧪 Batch 4/55 — Loss: 0.1526 — Time: 0.44s\n","  🧪 Batch 5/55 — Loss: 0.1471 — Time: 0.44s\n","  🧪 Batch 6/55 — Loss: 0.1318 — Time: 0.44s\n","  🧪 Batch 7/55 — Loss: 0.1183 — Time: 0.44s\n","  🧪 Batch 8/55 — Loss: 0.1234 — Time: 0.44s\n","  🧪 Batch 9/55 — Loss: 0.1172 — Time: 0.44s\n","  🧪 Batch 10/55 — Loss: 0.1323 — Time: 0.44s\n","  🧪 Batch 11/55 — Loss: 0.1208 — Time: 0.44s\n","  🧪 Batch 12/55 — Loss: 0.1334 — Time: 0.44s\n","  🧪 Batch 13/55 — Loss: 0.1295 — Time: 0.44s\n","  🧪 Batch 14/55 — Loss: 0.1425 — Time: 0.44s\n","  🧪 Batch 15/55 — Loss: 0.1570 — Time: 0.44s\n","  🧪 Batch 16/55 — Loss: 0.1414 — Time: 0.44s\n","  🧪 Batch 17/55 — Loss: 0.1828 — Time: 0.44s\n","  🧪 Batch 18/55 — Loss: 0.1207 — Time: 0.44s\n","  🧪 Batch 19/55 — Loss: 0.2179 — Time: 0.44s\n","  🧪 Batch 20/55 — Loss: 0.1292 — Time: 0.45s\n","  🧪 Batch 21/55 — Loss: 0.1526 — Time: 0.49s\n","  🧪 Batch 22/55 — Loss: 0.1684 — Time: 0.45s\n","  🧪 Batch 23/55 — Loss: 0.1432 — Time: 0.45s\n","  🧪 Batch 24/55 — Loss: 0.1924 — Time: 0.46s\n","  🧪 Batch 25/55 — Loss: 0.1474 — Time: 0.47s\n","  🧪 Batch 26/55 — Loss: 0.1316 — Time: 0.47s\n","  🧪 Batch 27/55 — Loss: 0.1840 — Time: 0.44s\n","  🧪 Batch 28/55 — Loss: 0.1414 — Time: 0.44s\n","  🧪 Batch 29/55 — Loss: 0.1249 — Time: 0.44s\n","  🧪 Batch 30/55 — Loss: 0.1313 — Time: 0.44s\n","  🧪 Batch 31/55 — Loss: 0.1333 — Time: 0.44s\n","  🧪 Batch 32/55 — Loss: 0.2105 — Time: 0.45s\n","  🧪 Batch 33/55 — Loss: 0.1629 — Time: 0.44s\n","  🧪 Batch 34/55 — Loss: 0.1492 — Time: 0.44s\n","  🧪 Batch 35/55 — Loss: 0.1335 — Time: 0.44s\n","  🧪 Batch 36/55 — Loss: 0.1425 — Time: 0.44s\n","  🧪 Batch 37/55 — Loss: 0.1355 — Time: 0.44s\n","  🧪 Batch 38/55 — Loss: 0.1358 — Time: 0.44s\n","  🧪 Batch 39/55 — Loss: 0.2036 — Time: 0.44s\n","  🧪 Batch 40/55 — Loss: 0.1255 — Time: 0.44s\n","  🧪 Batch 41/55 — Loss: 0.1172 — Time: 0.44s\n","  🧪 Batch 42/55 — Loss: 0.1748 — Time: 0.44s\n","  🧪 Batch 43/55 — Loss: 0.1557 — Time: 0.44s\n","  🧪 Batch 44/55 — Loss: 0.1274 — Time: 0.44s\n","  🧪 Batch 45/55 — Loss: 0.1533 — Time: 0.45s\n","  🧪 Batch 46/55 — Loss: 0.1231 — Time: 0.44s\n","  🧪 Batch 47/55 — Loss: 0.1157 — Time: 0.44s\n","  🧪 Batch 48/55 — Loss: 0.1457 — Time: 0.44s\n","  🧪 Batch 49/55 — Loss: 0.1339 — Time: 0.44s\n","  🧪 Batch 50/55 — Loss: 0.1197 — Time: 0.45s\n","  🧪 Batch 51/55 — Loss: 0.1258 — Time: 0.45s\n","  🧪 Batch 52/55 — Loss: 0.2235 — Time: 0.44s\n","  🧪 Batch 53/55 — Loss: 0.2311 — Time: 0.44s\n","  🧪 Batch 54/55 — Loss: 0.1447 — Time: 0.44s\n","  🧪 Batch 55/55 — Loss: 0.1652 — Time: 0.29s\n","🕒 Epoch Time: 27.62s | Avg Batch Time: 0.44s\n","📊 Train Loss: 0.1469 | Val Loss: 0.2730 | IoU: 0.8102 | Dice: 0.8934 | Jaccard: 0.8102 | Precision: 0.9186 | Recall: 0.8704 | Accuracy: 0.9745\n","\n","Epoch 18/100\n","  🧪 Batch 1/55 — Loss: 0.1598 — Time: 0.44s\n","  🧪 Batch 2/55 — Loss: 0.1539 — Time: 0.44s\n","  🧪 Batch 3/55 — Loss: 0.1566 — Time: 0.44s\n","  🧪 Batch 4/55 — Loss: 0.1454 — Time: 0.44s\n","  🧪 Batch 5/55 — Loss: 0.1730 — Time: 0.44s\n","  🧪 Batch 6/55 — Loss: 0.1218 — Time: 0.44s\n","  🧪 Batch 7/55 — Loss: 0.1229 — Time: 0.44s\n","  🧪 Batch 8/55 — Loss: 0.1392 — Time: 0.44s\n","  🧪 Batch 9/55 — Loss: 0.2328 — Time: 0.44s\n","  🧪 Batch 10/55 — Loss: 0.1214 — Time: 0.44s\n","  🧪 Batch 11/55 — Loss: 0.1285 — Time: 0.44s\n","  🧪 Batch 12/55 — Loss: 0.1360 — Time: 0.44s\n","  🧪 Batch 13/55 — Loss: 0.1879 — Time: 0.44s\n","  🧪 Batch 14/55 — Loss: 0.1408 — Time: 0.44s\n","  🧪 Batch 15/55 — Loss: 0.1490 — Time: 0.44s\n","  🧪 Batch 16/55 — Loss: 0.1204 — Time: 0.44s\n","  🧪 Batch 17/55 — Loss: 0.1316 — Time: 0.44s\n","  🧪 Batch 18/55 — Loss: 0.1641 — Time: 0.45s\n","  🧪 Batch 19/55 — Loss: 0.1276 — Time: 0.44s\n","  🧪 Batch 20/55 — Loss: 0.1309 — Time: 0.48s\n","  🧪 Batch 21/55 — Loss: 0.1276 — Time: 0.48s\n","  🧪 Batch 22/55 — Loss: 0.1265 — Time: 0.46s\n","  🧪 Batch 23/55 — Loss: 0.1554 — Time: 0.44s\n","  🧪 Batch 24/55 — Loss: 0.1120 — Time: 0.47s\n","  🧪 Batch 25/55 — Loss: 0.1167 — Time: 0.49s\n","  🧪 Batch 26/55 — Loss: 0.1295 — Time: 0.44s\n","  🧪 Batch 27/55 — Loss: 0.1271 — Time: 0.44s\n","  🧪 Batch 28/55 — Loss: 0.1248 — Time: 0.45s\n","  🧪 Batch 29/55 — Loss: 0.1565 — Time: 0.46s\n","  🧪 Batch 30/55 — Loss: 0.1144 — Time: 0.44s\n","  🧪 Batch 31/55 — Loss: 0.1088 — Time: 0.44s\n","  🧪 Batch 32/55 — Loss: 0.1175 — Time: 0.44s\n","  🧪 Batch 33/55 — Loss: 0.1041 — Time: 0.44s\n","  🧪 Batch 34/55 — Loss: 0.1021 — Time: 0.44s\n","  🧪 Batch 35/55 — Loss: 0.1162 — Time: 0.44s\n","  🧪 Batch 36/55 — Loss: 0.1078 — Time: 0.44s\n","  🧪 Batch 37/55 — Loss: 0.1268 — Time: 0.44s\n","  🧪 Batch 38/55 — Loss: 0.1495 — Time: 0.44s\n","  🧪 Batch 39/55 — Loss: 0.1197 — Time: 0.44s\n","  🧪 Batch 40/55 — Loss: 0.1216 — Time: 0.44s\n","  🧪 Batch 41/55 — Loss: 0.1205 — Time: 0.44s\n","  🧪 Batch 42/55 — Loss: 0.1022 — Time: 0.44s\n","  🧪 Batch 43/55 — Loss: 0.1418 — Time: 0.44s\n","  🧪 Batch 44/55 — Loss: 0.1201 — Time: 0.44s\n","  🧪 Batch 45/55 — Loss: 0.1156 — Time: 0.44s\n","  🧪 Batch 46/55 — Loss: 0.1202 — Time: 0.44s\n","  🧪 Batch 47/55 — Loss: 0.1303 — Time: 0.44s\n","  🧪 Batch 48/55 — Loss: 0.1347 — Time: 0.44s\n","  🧪 Batch 49/55 — Loss: 0.1161 — Time: 0.44s\n","  🧪 Batch 50/55 — Loss: 0.1455 — Time: 0.44s\n","  🧪 Batch 51/55 — Loss: 0.1227 — Time: 0.47s\n","  🧪 Batch 52/55 — Loss: 0.1655 — Time: 0.47s\n","  🧪 Batch 53/55 — Loss: 0.1275 — Time: 0.44s\n","  🧪 Batch 54/55 — Loss: 0.1312 — Time: 0.44s\n","  🧪 Batch 55/55 — Loss: 0.1032 — Time: 0.28s\n","🕒 Epoch Time: 27.65s | Avg Batch Time: 0.44s\n","📊 Train Loss: 0.1328 | Val Loss: 0.2640 | IoU: 0.8108 | Dice: 0.8939 | Jaccard: 0.8108 | Precision: 0.9119 | Recall: 0.8779 | Accuracy: 0.9748\n","\n","Epoch 19/100\n","  🧪 Batch 1/55 — Loss: 0.1178 — Time: 0.44s\n","  🧪 Batch 2/55 — Loss: 0.1136 — Time: 0.45s\n","  🧪 Batch 3/55 — Loss: 0.1070 — Time: 0.43s\n","  🧪 Batch 4/55 — Loss: 0.1447 — Time: 0.44s\n","  🧪 Batch 5/55 — Loss: 0.1007 — Time: 0.44s\n","  🧪 Batch 6/55 — Loss: 0.1991 — Time: 0.44s\n","  🧪 Batch 7/55 — Loss: 0.1067 — Time: 0.44s\n","  🧪 Batch 8/55 — Loss: 0.1036 — Time: 0.44s\n","  🧪 Batch 9/55 — Loss: 0.1371 — Time: 0.44s\n","  🧪 Batch 10/55 — Loss: 0.1164 — Time: 0.44s\n","  🧪 Batch 11/55 — Loss: 0.1024 — Time: 0.44s\n","  🧪 Batch 12/55 — Loss: 0.1301 — Time: 0.44s\n","  🧪 Batch 13/55 — Loss: 0.1151 — Time: 0.44s\n","  🧪 Batch 14/55 — Loss: 0.1121 — Time: 0.44s\n","  🧪 Batch 15/55 — Loss: 0.1212 — Time: 0.44s\n","  🧪 Batch 16/55 — Loss: 0.1154 — Time: 0.44s\n","  🧪 Batch 17/55 — Loss: 0.1341 — Time: 0.44s\n","  🧪 Batch 18/55 — Loss: 0.1128 — Time: 0.44s\n","  🧪 Batch 19/55 — Loss: 0.1129 — Time: 0.44s\n","  🧪 Batch 20/55 — Loss: 0.1187 — Time: 0.44s\n","  🧪 Batch 21/55 — Loss: 0.1130 — Time: 0.44s\n","  🧪 Batch 22/55 — Loss: 0.1182 — Time: 0.46s\n","  🧪 Batch 23/55 — Loss: 0.1017 — Time: 0.47s\n","  🧪 Batch 24/55 — Loss: 0.1382 — Time: 0.44s\n","  🧪 Batch 25/55 — Loss: 0.1197 — Time: 0.48s\n","  🧪 Batch 26/55 — Loss: 0.1233 — Time: 0.44s\n","  🧪 Batch 27/55 — Loss: 0.1177 — Time: 0.44s\n","  🧪 Batch 28/55 — Loss: 0.1432 — Time: 0.44s\n","  🧪 Batch 29/55 — Loss: 0.1329 — Time: 0.45s\n","  🧪 Batch 30/55 — Loss: 0.1335 — Time: 0.44s\n","  🧪 Batch 31/55 — Loss: 0.1098 — Time: 0.44s\n","  🧪 Batch 32/55 — Loss: 0.1080 — Time: 0.44s\n","  🧪 Batch 33/55 — Loss: 0.1640 — Time: 0.44s\n","  🧪 Batch 34/55 — Loss: 0.1103 — Time: 0.44s\n","  🧪 Batch 35/55 — Loss: 0.1144 — Time: 0.44s\n","  🧪 Batch 36/55 — Loss: 0.1079 — Time: 0.44s\n","  🧪 Batch 37/55 — Loss: 0.1075 — Time: 0.44s\n","  🧪 Batch 38/55 — Loss: 0.1049 — Time: 0.44s\n","  🧪 Batch 39/55 — Loss: 0.1078 — Time: 0.44s\n","  🧪 Batch 40/55 — Loss: 0.1185 — Time: 0.44s\n","  🧪 Batch 41/55 — Loss: 0.1735 — Time: 0.44s\n","  🧪 Batch 42/55 — Loss: 0.1122 — Time: 0.44s\n","  🧪 Batch 43/55 — Loss: 0.2317 — Time: 0.44s\n","  🧪 Batch 44/55 — Loss: 0.1148 — Time: 0.44s\n","  🧪 Batch 45/55 — Loss: 0.1169 — Time: 0.44s\n","  🧪 Batch 46/55 — Loss: 0.1708 — Time: 0.44s\n","  🧪 Batch 47/55 — Loss: 0.1243 — Time: 0.44s\n","  🧪 Batch 48/55 — Loss: 0.1157 — Time: 0.44s\n","  🧪 Batch 49/55 — Loss: 0.1251 — Time: 0.44s\n","  🧪 Batch 50/55 — Loss: 0.1080 — Time: 0.44s\n","  🧪 Batch 51/55 — Loss: 0.1137 — Time: 0.45s\n","  🧪 Batch 52/55 — Loss: 0.1201 — Time: 0.44s\n","  🧪 Batch 53/55 — Loss: 0.1435 — Time: 0.44s\n","  🧪 Batch 54/55 — Loss: 0.1405 — Time: 0.44s\n","  🧪 Batch 55/55 — Loss: 0.1218 — Time: 0.28s\n","🕒 Epoch Time: 27.47s | Avg Batch Time: 0.44s\n","📊 Train Loss: 0.1245 | Val Loss: 0.2692 | IoU: 0.8041 | Dice: 0.8894 | Jaccard: 0.8041 | Precision: 0.9278 | Recall: 0.8562 | Accuracy: 0.9742\n","\n","Epoch 20/100\n","  🧪 Batch 1/55 — Loss: 0.1139 — Time: 0.44s\n","  🧪 Batch 2/55 — Loss: 0.1189 — Time: 0.44s\n","  🧪 Batch 3/55 — Loss: 0.1083 — Time: 0.44s\n","  🧪 Batch 4/55 — Loss: 0.1225 — Time: 0.44s\n","  🧪 Batch 5/55 — Loss: 0.1003 — Time: 0.44s\n","  🧪 Batch 6/55 — Loss: 0.1241 — Time: 0.44s\n","  🧪 Batch 7/55 — Loss: 0.1311 — Time: 0.45s\n","  🧪 Batch 8/55 — Loss: 0.1092 — Time: 0.44s\n","  🧪 Batch 9/55 — Loss: 0.1694 — Time: 0.44s\n","  🧪 Batch 10/55 — Loss: 0.0997 — Time: 0.44s\n","  🧪 Batch 11/55 — Loss: 0.1487 — Time: 0.44s\n","  🧪 Batch 12/55 — Loss: 0.1149 — Time: 0.44s\n","  🧪 Batch 13/55 — Loss: 0.1310 — Time: 0.44s\n","  🧪 Batch 14/55 — Loss: 0.1464 — Time: 0.44s\n","  🧪 Batch 15/55 — Loss: 0.1373 — Time: 0.44s\n","  🧪 Batch 16/55 — Loss: 0.1205 — Time: 0.44s\n","  🧪 Batch 17/55 — Loss: 0.1304 — Time: 0.44s\n","  🧪 Batch 18/55 — Loss: 0.1150 — Time: 0.44s\n","  🧪 Batch 19/55 — Loss: 0.1155 — Time: 0.44s\n","  🧪 Batch 20/55 — Loss: 0.1229 — Time: 0.46s\n","  🧪 Batch 21/55 — Loss: 0.1150 — Time: 0.49s\n","  🧪 Batch 22/55 — Loss: 0.1151 — Time: 0.46s\n","  🧪 Batch 23/55 — Loss: 0.1048 — Time: 0.44s\n","  🧪 Batch 24/55 — Loss: 0.1122 — Time: 0.48s\n","  🧪 Batch 25/55 — Loss: 0.1014 — Time: 0.49s\n","  🧪 Batch 26/55 — Loss: 0.1324 — Time: 0.46s\n","  🧪 Batch 27/55 — Loss: 0.1061 — Time: 0.51s\n","  🧪 Batch 28/55 — Loss: 0.0968 — Time: 0.44s\n","  🧪 Batch 29/55 — Loss: 0.1238 — Time: 0.44s\n","  🧪 Batch 30/55 — Loss: 0.1144 — Time: 0.44s\n","  🧪 Batch 31/55 — Loss: 0.1510 — Time: 0.44s\n","  🧪 Batch 32/55 — Loss: 0.1111 — Time: 0.44s\n","  🧪 Batch 33/55 — Loss: 0.1109 — Time: 0.44s\n","  🧪 Batch 34/55 — Loss: 0.1124 — Time: 0.44s\n","  🧪 Batch 35/55 — Loss: 0.1273 — Time: 0.44s\n","  🧪 Batch 36/55 — Loss: 0.1077 — Time: 0.44s\n","  🧪 Batch 37/55 — Loss: 0.1218 — Time: 0.44s\n","  🧪 Batch 38/55 — Loss: 0.0969 — Time: 0.44s\n","  🧪 Batch 39/55 — Loss: 0.1309 — Time: 0.44s\n","  🧪 Batch 40/55 — Loss: 0.1035 — Time: 0.44s\n","  🧪 Batch 41/55 — Loss: 0.1008 — Time: 0.44s\n","  🧪 Batch 42/55 — Loss: 0.1096 — Time: 0.45s\n","  🧪 Batch 43/55 — Loss: 0.1147 — Time: 0.44s\n","  🧪 Batch 44/55 — Loss: 0.1292 — Time: 0.44s\n","  🧪 Batch 45/55 — Loss: 0.1466 — Time: 0.44s\n","  🧪 Batch 46/55 — Loss: 0.0990 — Time: 0.44s\n","  🧪 Batch 47/55 — Loss: 0.1103 — Time: 0.44s\n","  🧪 Batch 48/55 — Loss: 0.1136 — Time: 0.44s\n","  🧪 Batch 49/55 — Loss: 0.1280 — Time: 0.44s\n","  🧪 Batch 50/55 — Loss: 0.1065 — Time: 0.44s\n","  🧪 Batch 51/55 — Loss: 0.0981 — Time: 0.46s\n","  🧪 Batch 52/55 — Loss: 0.1110 — Time: 0.45s\n","  🧪 Batch 53/55 — Loss: 0.1250 — Time: 0.44s\n","  🧪 Batch 54/55 — Loss: 0.1083 — Time: 0.44s\n","  🧪 Batch 55/55 — Loss: 0.1267 — Time: 0.29s\n","🕒 Epoch Time: 27.74s | Avg Batch Time: 0.44s\n","📊 Train Loss: 0.1182 | Val Loss: 0.2606 | IoU: 0.8099 | Dice: 0.8928 | Jaccard: 0.8099 | Precision: 0.9185 | Recall: 0.8703 | Accuracy: 0.9744\n","\n","Epoch 21/100\n","  🧪 Batch 1/55 — Loss: 0.1275 — Time: 0.44s\n","  🧪 Batch 2/55 — Loss: 0.1051 — Time: 0.44s\n","  🧪 Batch 3/55 — Loss: 0.1095 — Time: 0.44s\n","  🧪 Batch 4/55 — Loss: 0.0996 — Time: 0.44s\n","  🧪 Batch 5/55 — Loss: 0.1146 — Time: 0.44s\n","  🧪 Batch 6/55 — Loss: 0.1062 — Time: 0.44s\n","  🧪 Batch 7/55 — Loss: 0.1017 — Time: 0.44s\n","  🧪 Batch 8/55 — Loss: 0.1124 — Time: 0.44s\n","  🧪 Batch 9/55 — Loss: 0.0977 — Time: 0.44s\n","  🧪 Batch 10/55 — Loss: 0.1215 — Time: 0.44s\n","  🧪 Batch 11/55 — Loss: 0.0884 — Time: 0.44s\n","  🧪 Batch 12/55 — Loss: 0.1215 — Time: 0.44s\n","  🧪 Batch 13/55 — Loss: 0.1048 — Time: 0.44s\n","  🧪 Batch 14/55 — Loss: 0.1037 — Time: 0.44s\n","  🧪 Batch 15/55 — Loss: 0.1022 — Time: 0.44s\n","  🧪 Batch 16/55 — Loss: 0.0984 — Time: 0.44s\n","  🧪 Batch 17/55 — Loss: 0.0940 — Time: 0.44s\n","  🧪 Batch 18/55 — Loss: 0.0994 — Time: 0.44s\n","  🧪 Batch 19/55 — Loss: 0.1009 — Time: 0.44s\n","  🧪 Batch 20/55 — Loss: 0.1043 — Time: 0.44s\n","  🧪 Batch 21/55 — Loss: 0.1049 — Time: 0.45s\n","  🧪 Batch 22/55 — Loss: 0.1039 — Time: 0.46s\n","  🧪 Batch 23/55 — Loss: 0.0985 — Time: 0.44s\n","  🧪 Batch 24/55 — Loss: 0.1056 — Time: 0.48s\n","  🧪 Batch 25/55 — Loss: 0.1044 — Time: 0.48s\n","  🧪 Batch 26/55 — Loss: 0.0958 — Time: 0.48s\n","  🧪 Batch 27/55 — Loss: 0.1027 — Time: 0.45s\n","  🧪 Batch 28/55 — Loss: 0.1416 — Time: 0.45s\n","  🧪 Batch 29/55 — Loss: 0.1039 — Time: 0.45s\n","  🧪 Batch 30/55 — Loss: 0.0882 — Time: 0.44s\n","  🧪 Batch 31/55 — Loss: 0.0931 — Time: 0.44s\n","  🧪 Batch 32/55 — Loss: 0.1048 — Time: 0.44s\n","  🧪 Batch 33/55 — Loss: 0.1363 — Time: 0.44s\n","  🧪 Batch 34/55 — Loss: 0.1031 — Time: 0.44s\n","  🧪 Batch 35/55 — Loss: 0.1140 — Time: 0.44s\n","  🧪 Batch 36/55 — Loss: 0.0989 — Time: 0.44s\n","  🧪 Batch 37/55 — Loss: 0.1178 — Time: 0.44s\n","  🧪 Batch 38/55 — Loss: 0.1056 — Time: 0.44s\n","  🧪 Batch 39/55 — Loss: 0.1495 — Time: 0.44s\n","  🧪 Batch 40/55 — Loss: 0.1059 — Time: 0.44s\n","  🧪 Batch 41/55 — Loss: 0.1202 — Time: 0.44s\n","  🧪 Batch 42/55 — Loss: 0.1126 — Time: 0.44s\n","  🧪 Batch 43/55 — Loss: 0.1462 — Time: 0.44s\n","  🧪 Batch 44/55 — Loss: 0.1038 — Time: 0.44s\n","  🧪 Batch 45/55 — Loss: 0.1167 — Time: 0.44s\n","  🧪 Batch 46/55 — Loss: 0.1413 — Time: 0.44s\n","  🧪 Batch 47/55 — Loss: 0.1224 — Time: 0.44s\n","  🧪 Batch 48/55 — Loss: 0.1167 — Time: 0.44s\n","  🧪 Batch 49/55 — Loss: 0.0968 — Time: 0.44s\n","  🧪 Batch 50/55 — Loss: 0.1029 — Time: 0.44s\n","  🧪 Batch 51/55 — Loss: 0.1048 — Time: 0.44s\n","  🧪 Batch 52/55 — Loss: 0.1127 — Time: 0.44s\n","  🧪 Batch 53/55 — Loss: 0.1046 — Time: 0.44s\n","  🧪 Batch 54/55 — Loss: 0.1008 — Time: 0.44s\n","  🧪 Batch 55/55 — Loss: 0.1105 — Time: 0.29s\n","🕒 Epoch Time: 27.54s | Avg Batch Time: 0.44s\n","📊 Train Loss: 0.1092 | Val Loss: 0.2513 | IoU: 0.8121 | Dice: 0.8947 | Jaccard: 0.8121 | Precision: 0.9047 | Recall: 0.8866 | Accuracy: 0.9747\n","\n","Epoch 22/100\n","  🧪 Batch 1/55 — Loss: 0.0950 — Time: 0.46s\n","  🧪 Batch 2/55 — Loss: 0.0988 — Time: 0.44s\n","  🧪 Batch 3/55 — Loss: 0.1941 — Time: 0.44s\n","  🧪 Batch 4/55 — Loss: 0.0939 — Time: 0.44s\n","  🧪 Batch 5/55 — Loss: 0.0981 — Time: 0.44s\n","  🧪 Batch 6/55 — Loss: 0.0931 — Time: 0.44s\n","  🧪 Batch 7/55 — Loss: 0.1260 — Time: 0.44s\n","  🧪 Batch 8/55 — Loss: 0.1047 — Time: 0.44s\n","  🧪 Batch 9/55 — Loss: 0.0959 — Time: 0.44s\n","  🧪 Batch 10/55 — Loss: 0.0924 — Time: 0.44s\n","  🧪 Batch 11/55 — Loss: 0.1029 — Time: 0.44s\n","  🧪 Batch 12/55 — Loss: 0.1375 — Time: 0.44s\n","  🧪 Batch 13/55 — Loss: 0.1167 — Time: 0.44s\n","  🧪 Batch 14/55 — Loss: 0.1142 — Time: 0.44s\n","  🧪 Batch 15/55 — Loss: 0.1487 — Time: 0.44s\n","  🧪 Batch 16/55 — Loss: 0.0967 — Time: 0.44s\n","  🧪 Batch 17/55 — Loss: 0.1156 — Time: 0.44s\n","  🧪 Batch 18/55 — Loss: 0.0985 — Time: 0.44s\n","  🧪 Batch 19/55 — Loss: 0.1081 — Time: 0.44s\n","  🧪 Batch 20/55 — Loss: 0.1058 — Time: 0.44s\n","  🧪 Batch 21/55 — Loss: 0.1071 — Time: 0.52s\n","  🧪 Batch 22/55 — Loss: 0.1151 — Time: 0.47s\n","  🧪 Batch 23/55 — Loss: 0.1057 — Time: 0.44s\n","  🧪 Batch 24/55 — Loss: 0.0865 — Time: 0.47s\n","  🧪 Batch 25/55 — Loss: 0.1094 — Time: 0.49s\n","  🧪 Batch 26/55 — Loss: 0.0886 — Time: 0.46s\n","  🧪 Batch 27/55 — Loss: 0.1536 — Time: 0.47s\n","  🧪 Batch 28/55 — Loss: 0.0972 — Time: 0.45s\n","  🧪 Batch 29/55 — Loss: 0.1335 — Time: 0.44s\n","  🧪 Batch 30/55 — Loss: 0.1034 — Time: 0.46s\n","  🧪 Batch 31/55 — Loss: 0.0882 — Time: 0.45s\n","  🧪 Batch 32/55 — Loss: 0.1066 — Time: 0.44s\n","  🧪 Batch 33/55 — Loss: 0.0911 — Time: 0.44s\n","  🧪 Batch 34/55 — Loss: 0.0937 — Time: 0.44s\n","  🧪 Batch 35/55 — Loss: 0.0995 — Time: 0.44s\n","  🧪 Batch 36/55 — Loss: 0.1112 — Time: 0.44s\n","  🧪 Batch 37/55 — Loss: 0.0843 — Time: 0.44s\n","  🧪 Batch 38/55 — Loss: 0.1051 — Time: 0.44s\n","  🧪 Batch 39/55 — Loss: 0.1178 — Time: 0.44s\n","  🧪 Batch 40/55 — Loss: 0.1042 — Time: 0.44s\n","  🧪 Batch 41/55 — Loss: 0.0855 — Time: 0.44s\n","  🧪 Batch 42/55 — Loss: 0.0841 — Time: 0.44s\n","  🧪 Batch 43/55 — Loss: 0.0971 — Time: 0.44s\n","  🧪 Batch 44/55 — Loss: 0.0808 — Time: 0.44s\n","  🧪 Batch 45/55 — Loss: 0.0918 — Time: 0.44s\n","  🧪 Batch 46/55 — Loss: 0.1059 — Time: 0.44s\n","  🧪 Batch 47/55 — Loss: 0.1270 — Time: 0.44s\n","  🧪 Batch 48/55 — Loss: 0.0876 — Time: 0.44s\n","  🧪 Batch 49/55 — Loss: 0.0871 — Time: 0.44s\n","  🧪 Batch 50/55 — Loss: 0.1086 — Time: 0.44s\n","  🧪 Batch 51/55 — Loss: 0.1127 — Time: 0.45s\n","  🧪 Batch 52/55 — Loss: 0.0998 — Time: 0.44s\n","  🧪 Batch 53/55 — Loss: 0.0922 — Time: 0.44s\n","  🧪 Batch 54/55 — Loss: 0.0968 — Time: 0.44s\n","  🧪 Batch 55/55 — Loss: 0.0890 — Time: 0.28s\n","🕒 Epoch Time: 27.76s | Avg Batch Time: 0.44s\n","📊 Train Loss: 0.1052 | Val Loss: 0.2487 | IoU: 0.8108 | Dice: 0.8939 | Jaccard: 0.8108 | Precision: 0.9273 | Recall: 0.8645 | Accuracy: 0.9750\n","\n","Epoch 23/100\n","  🧪 Batch 1/55 — Loss: 0.1082 — Time: 0.45s\n","  🧪 Batch 2/55 — Loss: 0.1186 — Time: 0.44s\n","  🧪 Batch 3/55 — Loss: 0.0934 — Time: 0.44s\n","  🧪 Batch 4/55 — Loss: 0.0955 — Time: 0.44s\n","  🧪 Batch 5/55 — Loss: 0.1116 — Time: 0.44s\n","  🧪 Batch 6/55 — Loss: 0.0952 — Time: 0.44s\n","  🧪 Batch 7/55 — Loss: 0.0848 — Time: 0.44s\n","  🧪 Batch 8/55 — Loss: 0.1013 — Time: 0.44s\n","  🧪 Batch 9/55 — Loss: 0.0971 — Time: 0.44s\n","  🧪 Batch 10/55 — Loss: 0.1134 — Time: 0.44s\n","  🧪 Batch 11/55 — Loss: 0.0835 — Time: 0.44s\n","  🧪 Batch 12/55 — Loss: 0.0803 — Time: 0.44s\n","  🧪 Batch 13/55 — Loss: 0.0869 — Time: 0.44s\n","  🧪 Batch 14/55 — Loss: 0.1121 — Time: 0.44s\n","  🧪 Batch 15/55 — Loss: 0.1171 — Time: 0.44s\n","  🧪 Batch 16/55 — Loss: 0.0950 — Time: 0.44s\n","  🧪 Batch 17/55 — Loss: 0.1410 — Time: 0.44s\n","  🧪 Batch 18/55 — Loss: 0.1113 — Time: 0.44s\n","  🧪 Batch 19/55 — Loss: 0.1016 — Time: 0.44s\n","  🧪 Batch 20/55 — Loss: 0.1010 — Time: 0.45s\n","  🧪 Batch 21/55 — Loss: 0.0903 — Time: 0.45s\n","  🧪 Batch 22/55 — Loss: 0.1020 — Time: 0.47s\n","  🧪 Batch 23/55 — Loss: 0.0878 — Time: 0.46s\n","  🧪 Batch 24/55 — Loss: 0.0915 — Time: 0.48s\n","  🧪 Batch 25/55 — Loss: 0.0938 — Time: 0.45s\n","  🧪 Batch 26/55 — Loss: 0.1001 — Time: 0.48s\n","  🧪 Batch 27/55 — Loss: 0.0834 — Time: 0.45s\n","  🧪 Batch 28/55 — Loss: 0.0835 — Time: 0.44s\n","  🧪 Batch 29/55 — Loss: 0.1082 — Time: 0.44s\n","  🧪 Batch 30/55 — Loss: 0.0872 — Time: 0.44s\n","  🧪 Batch 31/55 — Loss: 0.1178 — Time: 0.45s\n","  🧪 Batch 32/55 — Loss: 0.1177 — Time: 0.44s\n","  🧪 Batch 33/55 — Loss: 0.0950 — Time: 0.44s\n","  🧪 Batch 34/55 — Loss: 0.0884 — Time: 0.44s\n","  🧪 Batch 35/55 — Loss: 0.1133 — Time: 0.44s\n","  🧪 Batch 36/55 — Loss: 0.1146 — Time: 0.44s\n","  🧪 Batch 37/55 — Loss: 0.0808 — Time: 0.44s\n","  🧪 Batch 38/55 — Loss: 0.0860 — Time: 0.44s\n","  🧪 Batch 39/55 — Loss: 0.1139 — Time: 0.45s\n","  🧪 Batch 40/55 — Loss: 0.1085 — Time: 0.44s\n","  🧪 Batch 41/55 — Loss: 0.0937 — Time: 0.44s\n","  🧪 Batch 42/55 — Loss: 0.1106 — Time: 0.44s\n","  🧪 Batch 43/55 — Loss: 0.1042 — Time: 0.44s\n","  🧪 Batch 44/55 — Loss: 0.1066 — Time: 0.44s\n","  🧪 Batch 45/55 — Loss: 0.0980 — Time: 0.44s\n","  🧪 Batch 46/55 — Loss: 0.0941 — Time: 0.44s\n","  🧪 Batch 47/55 — Loss: 0.0832 — Time: 0.44s\n","  🧪 Batch 48/55 — Loss: 0.0957 — Time: 0.44s\n","  🧪 Batch 49/55 — Loss: 0.0892 — Time: 0.44s\n","  🧪 Batch 50/55 — Loss: 0.1149 — Time: 0.44s\n","  🧪 Batch 51/55 — Loss: 0.0996 — Time: 0.45s\n","  🧪 Batch 52/55 — Loss: 0.1114 — Time: 0.44s\n","  🧪 Batch 53/55 — Loss: 0.1136 — Time: 0.44s\n","  🧪 Batch 54/55 — Loss: 0.0943 — Time: 0.44s\n","  🧪 Batch 55/55 — Loss: 0.1015 — Time: 0.28s\n","🕒 Epoch Time: 27.66s | Avg Batch Time: 0.44s\n","📊 Train Loss: 0.1004 | Val Loss: 0.2470 | IoU: 0.8113 | Dice: 0.8942 | Jaccard: 0.8113 | Precision: 0.9217 | Recall: 0.8697 | Accuracy: 0.9752\n","\n","Epoch 24/100\n","  🧪 Batch 1/55 — Loss: 0.0858 — Time: 0.44s\n","  🧪 Batch 2/55 — Loss: 0.0875 — Time: 0.44s\n","  🧪 Batch 3/55 — Loss: 0.0849 — Time: 0.44s\n","  🧪 Batch 4/55 — Loss: 0.0901 — Time: 0.44s\n","  🧪 Batch 5/55 — Loss: 0.0906 — Time: 0.44s\n","  🧪 Batch 6/55 — Loss: 0.2342 — Time: 0.44s\n","  🧪 Batch 7/55 — Loss: 0.0794 — Time: 0.45s\n","  🧪 Batch 8/55 — Loss: 0.1384 — Time: 0.44s\n","  🧪 Batch 9/55 — Loss: 0.0827 — Time: 0.44s\n","  🧪 Batch 10/55 — Loss: 0.1623 — Time: 0.44s\n","  🧪 Batch 11/55 — Loss: 0.0916 — Time: 0.44s\n","  🧪 Batch 12/55 — Loss: 0.1683 — Time: 0.44s\n","  🧪 Batch 13/55 — Loss: 0.1015 — Time: 0.44s\n","  🧪 Batch 14/55 — Loss: 0.1090 — Time: 0.44s\n","  🧪 Batch 15/55 — Loss: 0.0910 — Time: 0.44s\n","  🧪 Batch 16/55 — Loss: 0.0990 — Time: 0.44s\n","  🧪 Batch 17/55 — Loss: 0.0987 — Time: 0.44s\n","  🧪 Batch 18/55 — Loss: 0.0965 — Time: 0.44s\n","  🧪 Batch 19/55 — Loss: 0.1243 — Time: 0.44s\n","  🧪 Batch 20/55 — Loss: 0.1066 — Time: 0.46s\n","  🧪 Batch 21/55 — Loss: 0.0932 — Time: 0.44s\n","  🧪 Batch 22/55 — Loss: 0.0958 — Time: 0.47s\n","  🧪 Batch 23/55 — Loss: 0.0963 — Time: 0.49s\n","  🧪 Batch 24/55 — Loss: 0.0960 — Time: 0.45s\n","  🧪 Batch 25/55 — Loss: 0.0958 — Time: 0.46s\n","  🧪 Batch 26/55 — Loss: 0.0879 — Time: 0.44s\n","  🧪 Batch 27/55 — Loss: 0.1018 — Time: 0.45s\n","  🧪 Batch 28/55 — Loss: 0.1241 — Time: 0.44s\n","  🧪 Batch 29/55 — Loss: 0.1045 — Time: 0.44s\n","  🧪 Batch 30/55 — Loss: 0.0945 — Time: 0.44s\n","  🧪 Batch 31/55 — Loss: 0.1089 — Time: 0.44s\n","  🧪 Batch 32/55 — Loss: 0.0974 — Time: 0.44s\n","  🧪 Batch 33/55 — Loss: 0.0812 — Time: 0.44s\n","  🧪 Batch 34/55 — Loss: 0.0845 — Time: 0.44s\n","  🧪 Batch 35/55 — Loss: 0.0846 — Time: 0.44s\n","  🧪 Batch 36/55 — Loss: 0.1082 — Time: 0.44s\n","  🧪 Batch 37/55 — Loss: 0.1247 — Time: 0.44s\n","  🧪 Batch 38/55 — Loss: 0.0904 — Time: 0.44s\n","  🧪 Batch 39/55 — Loss: 0.0880 — Time: 0.44s\n","  🧪 Batch 40/55 — Loss: 0.1169 — Time: 0.44s\n","  🧪 Batch 41/55 — Loss: 0.0970 — Time: 0.44s\n","  🧪 Batch 42/55 — Loss: 0.1148 — Time: 0.44s\n","  🧪 Batch 43/55 — Loss: 0.0991 — Time: 0.44s\n","  🧪 Batch 44/55 — Loss: 0.1091 — Time: 0.44s\n","  🧪 Batch 45/55 — Loss: 0.0904 — Time: 0.44s\n","  🧪 Batch 46/55 — Loss: 0.0846 — Time: 0.44s\n","  🧪 Batch 47/55 — Loss: 0.1074 — Time: 0.44s\n","  🧪 Batch 48/55 — Loss: 0.0827 — Time: 0.44s\n","  🧪 Batch 49/55 — Loss: 0.0900 — Time: 0.44s\n","  🧪 Batch 50/55 — Loss: 0.1103 — Time: 0.44s\n","  🧪 Batch 51/55 — Loss: 0.0902 — Time: 0.44s\n","  🧪 Batch 52/55 — Loss: 0.0822 — Time: 0.44s\n","  🧪 Batch 53/55 — Loss: 0.0880 — Time: 0.44s\n","  🧪 Batch 54/55 — Loss: 0.0833 — Time: 0.44s\n","  🧪 Batch 55/55 — Loss: 0.0827 — Time: 0.29s\n","🕒 Epoch Time: 27.72s | Avg Batch Time: 0.44s\n","📊 Train Loss: 0.1020 | Val Loss: 0.2373 | IoU: 0.8168 | Dice: 0.8979 | Jaccard: 0.8168 | Precision: 0.9127 | Recall: 0.8851 | Accuracy: 0.9756\n","\n","Epoch 25/100\n","  🧪 Batch 1/55 — Loss: 0.0838 — Time: 0.44s\n","  🧪 Batch 2/55 — Loss: 0.0834 — Time: 0.44s\n","  🧪 Batch 3/55 — Loss: 0.1046 — Time: 0.44s\n","  🧪 Batch 4/55 — Loss: 0.0963 — Time: 0.44s\n","  🧪 Batch 5/55 — Loss: 0.1190 — Time: 0.44s\n","  🧪 Batch 6/55 — Loss: 0.0909 — Time: 0.44s\n","  🧪 Batch 7/55 — Loss: 0.0918 — Time: 0.44s\n","  🧪 Batch 8/55 — Loss: 0.0784 — Time: 0.44s\n","  🧪 Batch 9/55 — Loss: 0.0894 — Time: 0.44s\n","  🧪 Batch 10/55 — Loss: 0.0815 — Time: 0.44s\n","  🧪 Batch 11/55 — Loss: 0.0761 — Time: 0.44s\n","  🧪 Batch 12/55 — Loss: 0.0918 — Time: 0.44s\n","  🧪 Batch 13/55 — Loss: 0.0809 — Time: 0.44s\n","  🧪 Batch 14/55 — Loss: 0.0796 — Time: 0.44s\n","  🧪 Batch 15/55 — Loss: 0.0835 — Time: 0.44s\n","  🧪 Batch 16/55 — Loss: 0.0817 — Time: 0.44s\n","  🧪 Batch 17/55 — Loss: 0.0929 — Time: 0.44s\n","  🧪 Batch 18/55 — Loss: 0.1659 — Time: 0.44s\n","  🧪 Batch 19/55 — Loss: 0.0886 — Time: 0.44s\n","  🧪 Batch 20/55 — Loss: 0.1052 — Time: 0.44s\n","  🧪 Batch 21/55 — Loss: 0.0804 — Time: 0.44s\n","  🧪 Batch 22/55 — Loss: 0.0901 — Time: 0.47s\n","  🧪 Batch 23/55 — Loss: 0.0908 — Time: 0.45s\n","  🧪 Batch 24/55 — Loss: 0.0804 — Time: 0.49s\n","  🧪 Batch 25/55 — Loss: 0.1045 — Time: 0.47s\n","  🧪 Batch 26/55 — Loss: 0.0974 — Time: 0.44s\n","  🧪 Batch 27/55 — Loss: 0.0900 — Time: 0.49s\n","  🧪 Batch 28/55 — Loss: 0.0771 — Time: 0.44s\n","  🧪 Batch 29/55 — Loss: 0.1004 — Time: 0.44s\n","  🧪 Batch 30/55 — Loss: 0.0956 — Time: 0.44s\n","  🧪 Batch 31/55 — Loss: 0.0808 — Time: 0.44s\n","  🧪 Batch 32/55 — Loss: 0.0803 — Time: 0.44s\n","  🧪 Batch 33/55 — Loss: 0.0894 — Time: 0.44s\n","  🧪 Batch 34/55 — Loss: 0.0979 — Time: 0.44s\n","  🧪 Batch 35/55 — Loss: 0.0757 — Time: 0.44s\n","  🧪 Batch 36/55 — Loss: 0.0788 — Time: 0.44s\n","  🧪 Batch 37/55 — Loss: 0.1127 — Time: 0.44s\n","  🧪 Batch 38/55 — Loss: 0.0970 — Time: 0.44s\n","  🧪 Batch 39/55 — Loss: 0.0865 — Time: 0.44s\n","  🧪 Batch 40/55 — Loss: 0.0910 — Time: 0.44s\n","  🧪 Batch 41/55 — Loss: 0.1091 — Time: 0.44s\n","  🧪 Batch 42/55 — Loss: 0.1531 — Time: 0.44s\n","  🧪 Batch 43/55 — Loss: 0.0824 — Time: 0.44s\n","  🧪 Batch 44/55 — Loss: 0.0767 — Time: 0.44s\n","  🧪 Batch 45/55 — Loss: 0.0807 — Time: 0.44s\n","  🧪 Batch 46/55 — Loss: 0.0922 — Time: 0.44s\n","  🧪 Batch 47/55 — Loss: 0.1119 — Time: 0.45s\n","  🧪 Batch 48/55 — Loss: 0.0923 — Time: 0.44s\n","  🧪 Batch 49/55 — Loss: 0.0829 — Time: 0.44s\n","  🧪 Batch 50/55 — Loss: 0.0932 — Time: 0.44s\n","  🧪 Batch 51/55 — Loss: 0.0792 — Time: 0.44s\n","  🧪 Batch 52/55 — Loss: 0.0906 — Time: 0.44s\n","  🧪 Batch 53/55 — Loss: 0.0815 — Time: 0.44s\n","  🧪 Batch 54/55 — Loss: 0.1071 — Time: 0.44s\n","  🧪 Batch 55/55 — Loss: 0.0744 — Time: 0.28s\n","🕒 Epoch Time: 27.53s | Avg Batch Time: 0.44s\n","📊 Train Loss: 0.0922 | Val Loss: 0.2372 | IoU: 0.8151 | Dice: 0.8966 | Jaccard: 0.8151 | Precision: 0.9027 | Recall: 0.8923 | Accuracy: 0.9751\n","\n","Epoch 26/100\n","  🧪 Batch 1/55 — Loss: 0.0957 — Time: 0.45s\n","  🧪 Batch 2/55 — Loss: 0.0969 — Time: 0.44s\n","  🧪 Batch 3/55 — Loss: 0.0945 — Time: 0.44s\n","  🧪 Batch 4/55 — Loss: 0.0938 — Time: 0.44s\n","  🧪 Batch 5/55 — Loss: 0.0801 — Time: 0.44s\n","  🧪 Batch 6/55 — Loss: 0.0781 — Time: 0.44s\n","  🧪 Batch 7/55 — Loss: 0.0897 — Time: 0.44s\n","  🧪 Batch 8/55 — Loss: 0.0851 — Time: 0.44s\n","  🧪 Batch 9/55 — Loss: 0.0852 — Time: 0.44s\n","  🧪 Batch 10/55 — Loss: 0.0904 — Time: 0.44s\n","  🧪 Batch 11/55 — Loss: 0.0857 — Time: 0.44s\n","  🧪 Batch 12/55 — Loss: 0.0945 — Time: 0.44s\n","  🧪 Batch 13/55 — Loss: 0.0724 — Time: 0.45s\n","  🧪 Batch 14/55 — Loss: 0.0815 — Time: 0.44s\n","  🧪 Batch 15/55 — Loss: 0.0828 — Time: 0.44s\n","  🧪 Batch 16/55 — Loss: 0.0764 — Time: 0.44s\n","  🧪 Batch 17/55 — Loss: 0.0846 — Time: 0.44s\n","  🧪 Batch 18/55 — Loss: 0.0871 — Time: 0.44s\n","  🧪 Batch 19/55 — Loss: 0.1432 — Time: 0.45s\n","  🧪 Batch 20/55 — Loss: 0.0759 — Time: 0.44s\n","  🧪 Batch 21/55 — Loss: 0.0927 — Time: 0.47s\n","  🧪 Batch 22/55 — Loss: 0.0953 — Time: 0.44s\n","  🧪 Batch 23/55 — Loss: 0.0777 — Time: 0.45s\n","  🧪 Batch 24/55 — Loss: 0.0775 — Time: 0.46s\n","  🧪 Batch 25/55 — Loss: 0.0838 — Time: 0.49s\n","  🧪 Batch 26/55 — Loss: 0.0791 — Time: 0.44s\n","  🧪 Batch 27/55 — Loss: 0.0855 — Time: 0.44s\n","  🧪 Batch 28/55 — Loss: 0.0737 — Time: 0.44s\n","  🧪 Batch 29/55 — Loss: 0.0933 — Time: 0.44s\n","  🧪 Batch 30/55 — Loss: 0.1123 — Time: 0.44s\n","  🧪 Batch 31/55 — Loss: 0.0851 — Time: 0.44s\n","  🧪 Batch 32/55 — Loss: 0.0856 — Time: 0.45s\n","  🧪 Batch 33/55 — Loss: 0.0770 — Time: 0.44s\n","  🧪 Batch 34/55 — Loss: 0.0729 — Time: 0.44s\n","  🧪 Batch 35/55 — Loss: 0.0763 — Time: 0.44s\n","  🧪 Batch 36/55 — Loss: 0.0823 — Time: 0.44s\n","  🧪 Batch 37/55 — Loss: 0.0765 — Time: 0.44s\n","  🧪 Batch 38/55 — Loss: 0.0770 — Time: 0.44s\n","  🧪 Batch 39/55 — Loss: 0.0979 — Time: 0.44s\n","  🧪 Batch 40/55 — Loss: 0.0816 — Time: 0.44s\n","  🧪 Batch 41/55 — Loss: 0.0954 — Time: 0.44s\n","  🧪 Batch 42/55 — Loss: 0.0684 — Time: 0.44s\n","  🧪 Batch 43/55 — Loss: 0.0961 — Time: 0.44s\n","  🧪 Batch 44/55 — Loss: 0.0866 — Time: 0.44s\n","  🧪 Batch 45/55 — Loss: 0.0765 — Time: 0.44s\n","  🧪 Batch 46/55 — Loss: 0.1113 — Time: 0.44s\n","  🧪 Batch 47/55 — Loss: 0.0989 — Time: 0.44s\n","  🧪 Batch 48/55 — Loss: 0.0744 — Time: 0.45s\n","  🧪 Batch 49/55 — Loss: 0.1081 — Time: 0.44s\n","  🧪 Batch 50/55 — Loss: 0.0817 — Time: 0.44s\n","  🧪 Batch 51/55 — Loss: 0.0944 — Time: 0.44s\n","  🧪 Batch 52/55 — Loss: 0.0858 — Time: 0.44s\n","  🧪 Batch 53/55 — Loss: 0.0907 — Time: 0.44s\n","  🧪 Batch 54/55 — Loss: 0.0921 — Time: 0.44s\n","  🧪 Batch 55/55 — Loss: 0.0946 — Time: 0.29s\n","🕒 Epoch Time: 27.61s | Avg Batch Time: 0.44s\n","📊 Train Loss: 0.0875 | Val Loss: 0.2373 | IoU: 0.8156 | Dice: 0.8968 | Jaccard: 0.8156 | Precision: 0.9279 | Recall: 0.8697 | Accuracy: 0.9758\n","\n","Epoch 27/100\n","  🧪 Batch 1/55 — Loss: 0.0868 — Time: 0.45s\n","  🧪 Batch 2/55 — Loss: 0.0706 — Time: 0.44s\n","  🧪 Batch 3/55 — Loss: 0.0928 — Time: 0.44s\n","  🧪 Batch 4/55 — Loss: 0.0852 — Time: 0.44s\n","  🧪 Batch 5/55 — Loss: 0.0702 — Time: 0.45s\n","  🧪 Batch 6/55 — Loss: 0.0671 — Time: 0.44s\n","  🧪 Batch 7/55 — Loss: 0.0917 — Time: 0.44s\n","  🧪 Batch 8/55 — Loss: 0.0785 — Time: 0.45s\n","  🧪 Batch 9/55 — Loss: 0.0708 — Time: 0.44s\n","  🧪 Batch 10/55 — Loss: 0.0873 — Time: 0.44s\n","  🧪 Batch 11/55 — Loss: 0.0752 — Time: 0.44s\n","  🧪 Batch 12/55 — Loss: 0.0734 — Time: 0.44s\n","  🧪 Batch 13/55 — Loss: 0.1106 — Time: 0.44s\n","  🧪 Batch 14/55 — Loss: 0.0746 — Time: 0.44s\n","  🧪 Batch 15/55 — Loss: 0.0843 — Time: 0.44s\n","  🧪 Batch 16/55 — Loss: 0.0745 — Time: 0.44s\n","  🧪 Batch 17/55 — Loss: 0.0749 — Time: 0.44s\n","  🧪 Batch 18/55 — Loss: 0.0734 — Time: 0.44s\n","  🧪 Batch 19/55 — Loss: 0.0793 — Time: 0.44s\n","  🧪 Batch 20/55 — Loss: 0.0800 — Time: 0.45s\n","  🧪 Batch 21/55 — Loss: 0.0674 — Time: 0.46s\n","  🧪 Batch 22/55 — Loss: 0.0982 — Time: 0.47s\n","  🧪 Batch 23/55 — Loss: 0.1021 — Time: 0.44s\n","  🧪 Batch 24/55 — Loss: 0.0777 — Time: 0.47s\n","  🧪 Batch 25/55 — Loss: 0.0813 — Time: 0.43s\n","  🧪 Batch 26/55 — Loss: 0.0801 — Time: 0.45s\n","  🧪 Batch 27/55 — Loss: 0.0745 — Time: 0.44s\n","  🧪 Batch 28/55 — Loss: 0.0717 — Time: 0.44s\n","  🧪 Batch 29/55 — Loss: 0.0655 — Time: 0.44s\n","  🧪 Batch 30/55 — Loss: 0.1022 — Time: 0.44s\n","  🧪 Batch 31/55 — Loss: 0.1027 — Time: 0.45s\n","  🧪 Batch 32/55 — Loss: 0.0819 — Time: 0.44s\n","  🧪 Batch 33/55 — Loss: 0.0785 — Time: 0.44s\n","  🧪 Batch 34/55 — Loss: 0.1061 — Time: 0.44s\n","  🧪 Batch 35/55 — Loss: 0.0677 — Time: 0.44s\n","  🧪 Batch 36/55 — Loss: 0.0646 — Time: 0.44s\n","  🧪 Batch 37/55 — Loss: 0.0870 — Time: 0.44s\n","  🧪 Batch 38/55 — Loss: 0.0795 — Time: 0.44s\n","  🧪 Batch 39/55 — Loss: 0.0760 — Time: 0.44s\n","  🧪 Batch 40/55 — Loss: 0.0972 — Time: 0.44s\n","  🧪 Batch 41/55 — Loss: 0.0923 — Time: 0.44s\n","  🧪 Batch 42/55 — Loss: 0.0812 — Time: 0.44s\n","  🧪 Batch 43/55 — Loss: 0.0926 — Time: 0.44s\n","  🧪 Batch 44/55 — Loss: 0.0970 — Time: 0.44s\n","  🧪 Batch 45/55 — Loss: 0.0874 — Time: 0.44s\n","  🧪 Batch 46/55 — Loss: 0.0863 — Time: 0.44s\n","  🧪 Batch 47/55 — Loss: 0.0840 — Time: 0.44s\n","  🧪 Batch 48/55 — Loss: 0.0683 — Time: 0.44s\n","  🧪 Batch 49/55 — Loss: 0.0891 — Time: 0.44s\n","  🧪 Batch 50/55 — Loss: 0.0906 — Time: 0.44s\n","  🧪 Batch 51/55 — Loss: 0.0717 — Time: 0.45s\n","  🧪 Batch 52/55 — Loss: 0.0926 — Time: 0.44s\n","  🧪 Batch 53/55 — Loss: 0.0879 — Time: 0.44s\n","  🧪 Batch 54/55 — Loss: 0.0681 — Time: 0.44s\n","  🧪 Batch 55/55 — Loss: 0.0743 — Time: 0.29s\n","🕒 Epoch Time: 27.64s | Avg Batch Time: 0.44s\n","📊 Train Loss: 0.0823 | Val Loss: 0.2366 | IoU: 0.8129 | Dice: 0.8955 | Jaccard: 0.8129 | Precision: 0.9363 | Recall: 0.8599 | Accuracy: 0.9758\n","\n","Epoch 28/100\n","  🧪 Batch 1/55 — Loss: 0.0920 — Time: 0.46s\n","  🧪 Batch 2/55 — Loss: 0.0751 — Time: 0.44s\n","  🧪 Batch 3/55 — Loss: 0.0759 — Time: 0.44s\n","  🧪 Batch 4/55 — Loss: 0.0735 — Time: 0.44s\n","  🧪 Batch 5/55 — Loss: 0.0748 — Time: 0.44s\n","  🧪 Batch 6/55 — Loss: 0.0811 — Time: 0.44s\n","  🧪 Batch 7/55 — Loss: 0.0770 — Time: 0.44s\n","  🧪 Batch 8/55 — Loss: 0.0860 — Time: 0.44s\n","  🧪 Batch 9/55 — Loss: 0.0773 — Time: 0.44s\n","  🧪 Batch 10/55 — Loss: 0.1014 — Time: 0.44s\n","  🧪 Batch 11/55 — Loss: 0.0687 — Time: 0.44s\n","  🧪 Batch 12/55 — Loss: 0.0806 — Time: 0.44s\n","  🧪 Batch 13/55 — Loss: 0.0775 — Time: 0.44s\n","  🧪 Batch 14/55 — Loss: 0.0752 — Time: 0.44s\n","  🧪 Batch 15/55 — Loss: 0.0840 — Time: 0.44s\n","  🧪 Batch 16/55 — Loss: 0.0741 — Time: 0.44s\n","  🧪 Batch 17/55 — Loss: 0.0764 — Time: 0.44s\n","  🧪 Batch 18/55 — Loss: 0.0736 — Time: 0.44s\n","  🧪 Batch 19/55 — Loss: 0.0659 — Time: 0.44s\n","  🧪 Batch 20/55 — Loss: 0.0598 — Time: 0.44s\n","  🧪 Batch 21/55 — Loss: 0.0687 — Time: 0.49s\n","  🧪 Batch 22/55 — Loss: 0.0682 — Time: 0.44s\n","  🧪 Batch 23/55 — Loss: 0.0782 — Time: 0.44s\n","  🧪 Batch 24/55 — Loss: 0.0628 — Time: 0.44s\n","  🧪 Batch 25/55 — Loss: 0.1040 — Time: 0.47s\n","  🧪 Batch 26/55 — Loss: 0.1064 — Time: 0.44s\n","  🧪 Batch 27/55 — Loss: 0.0691 — Time: 0.46s\n","  🧪 Batch 28/55 — Loss: 0.0779 — Time: 0.44s\n","  🧪 Batch 29/55 — Loss: 0.0844 — Time: 0.44s\n","  🧪 Batch 30/55 — Loss: 0.0676 — Time: 0.44s\n","  🧪 Batch 31/55 — Loss: 0.0835 — Time: 0.44s\n","  🧪 Batch 32/55 — Loss: 0.0763 — Time: 0.44s\n","  🧪 Batch 33/55 — Loss: 0.0706 — Time: 0.45s\n","  🧪 Batch 34/55 — Loss: 0.0675 — Time: 0.44s\n","  🧪 Batch 35/55 — Loss: 0.0803 — Time: 0.44s\n","  🧪 Batch 36/55 — Loss: 0.0661 — Time: 0.44s\n","  🧪 Batch 37/55 — Loss: 0.0721 — Time: 0.44s\n","  🧪 Batch 38/55 — Loss: 0.0732 — Time: 0.44s\n","  🧪 Batch 39/55 — Loss: 0.0790 — Time: 0.44s\n","  🧪 Batch 40/55 — Loss: 0.0684 — Time: 0.44s\n","  🧪 Batch 41/55 — Loss: 0.0739 — Time: 0.44s\n","  🧪 Batch 42/55 — Loss: 0.0875 — Time: 0.44s\n","  🧪 Batch 43/55 — Loss: 0.0828 — Time: 0.44s\n","  🧪 Batch 44/55 — Loss: 0.0943 — Time: 0.44s\n","  🧪 Batch 45/55 — Loss: 0.0712 — Time: 0.44s\n","  🧪 Batch 46/55 — Loss: 0.0857 — Time: 0.44s\n","  🧪 Batch 47/55 — Loss: 0.0817 — Time: 0.44s\n","  🧪 Batch 48/55 — Loss: 0.0832 — Time: 0.44s\n","  🧪 Batch 49/55 — Loss: 0.0698 — Time: 0.44s\n","  🧪 Batch 50/55 — Loss: 0.0681 — Time: 0.44s\n","  🧪 Batch 51/55 — Loss: 0.0770 — Time: 0.44s\n","  🧪 Batch 52/55 — Loss: 0.0928 — Time: 0.44s\n","  🧪 Batch 53/55 — Loss: 0.0770 — Time: 0.44s\n","  🧪 Batch 54/55 — Loss: 0.0794 — Time: 0.44s\n","  🧪 Batch 55/55 — Loss: 0.1439 — Time: 0.29s\n","🕒 Epoch Time: 27.51s | Avg Batch Time: 0.44s\n","📊 Train Loss: 0.0790 | Val Loss: 0.2260 | IoU: 0.8234 | Dice: 0.9018 | Jaccard: 0.8234 | Precision: 0.9249 | Recall: 0.8812 | Accuracy: 0.9767\n","\n","Epoch 29/100\n","  🧪 Batch 1/55 — Loss: 0.0658 — Time: 0.46s\n","  🧪 Batch 2/55 — Loss: 0.0705 — Time: 0.44s\n","  🧪 Batch 3/55 — Loss: 0.0688 — Time: 0.44s\n","  🧪 Batch 4/55 — Loss: 0.0827 — Time: 0.44s\n","  🧪 Batch 5/55 — Loss: 0.0657 — Time: 0.44s\n","  🧪 Batch 6/55 — Loss: 0.0962 — Time: 0.44s\n","  🧪 Batch 7/55 — Loss: 0.0692 — Time: 0.44s\n","  🧪 Batch 8/55 — Loss: 0.0644 — Time: 0.44s\n","  🧪 Batch 9/55 — Loss: 0.0755 — Time: 0.44s\n","  🧪 Batch 10/55 — Loss: 0.0837 — Time: 0.44s\n","  🧪 Batch 11/55 — Loss: 0.0875 — Time: 0.44s\n","  🧪 Batch 12/55 — Loss: 0.0833 — Time: 0.44s\n","  🧪 Batch 13/55 — Loss: 0.0764 — Time: 0.44s\n","  🧪 Batch 14/55 — Loss: 0.0707 — Time: 0.44s\n","  🧪 Batch 15/55 — Loss: 0.0874 — Time: 0.44s\n","  🧪 Batch 16/55 — Loss: 0.0831 — Time: 0.44s\n","  🧪 Batch 17/55 — Loss: 0.0948 — Time: 0.44s\n","  🧪 Batch 18/55 — Loss: 0.0855 — Time: 0.44s\n","  🧪 Batch 19/55 — Loss: 0.0713 — Time: 0.44s\n","  🧪 Batch 20/55 — Loss: 0.0711 — Time: 0.45s\n","  🧪 Batch 21/55 — Loss: 0.0727 — Time: 0.45s\n","  🧪 Batch 22/55 — Loss: 0.0765 — Time: 0.44s\n","  🧪 Batch 23/55 — Loss: 0.0835 — Time: 0.44s\n","  🧪 Batch 24/55 — Loss: 0.0749 — Time: 0.45s\n","  🧪 Batch 25/55 — Loss: 0.0810 — Time: 0.47s\n","  🧪 Batch 26/55 — Loss: 0.0764 — Time: 0.44s\n","  🧪 Batch 27/55 — Loss: 0.0710 — Time: 0.44s\n","  🧪 Batch 28/55 — Loss: 0.0785 — Time: 0.44s\n","  🧪 Batch 29/55 — Loss: 0.0647 — Time: 0.44s\n","  🧪 Batch 30/55 — Loss: 0.0887 — Time: 0.44s\n","  🧪 Batch 31/55 — Loss: 0.0664 — Time: 0.44s\n","  🧪 Batch 32/55 — Loss: 0.0792 — Time: 0.44s\n","  🧪 Batch 33/55 — Loss: 0.0833 — Time: 0.44s\n","  🧪 Batch 34/55 — Loss: 0.0903 — Time: 0.44s\n","  🧪 Batch 35/55 — Loss: 0.0705 — Time: 0.44s\n","  🧪 Batch 36/55 — Loss: 0.0768 — Time: 0.44s\n","  🧪 Batch 37/55 — Loss: 0.0791 — Time: 0.44s\n","  🧪 Batch 38/55 — Loss: 0.0713 — Time: 0.44s\n","  🧪 Batch 39/55 — Loss: 0.0818 — Time: 0.44s\n","  🧪 Batch 40/55 — Loss: 0.0801 — Time: 0.44s\n","  🧪 Batch 41/55 — Loss: 0.0709 — Time: 0.44s\n","  🧪 Batch 42/55 — Loss: 0.0751 — Time: 0.44s\n","  🧪 Batch 43/55 — Loss: 0.0873 — Time: 0.44s\n","  🧪 Batch 44/55 — Loss: 0.0813 — Time: 0.44s\n","  🧪 Batch 45/55 — Loss: 0.0808 — Time: 0.44s\n","  🧪 Batch 46/55 — Loss: 0.0736 — Time: 0.44s\n","  🧪 Batch 47/55 — Loss: 0.0793 — Time: 0.44s\n","  🧪 Batch 48/55 — Loss: 0.0691 — Time: 0.44s\n","  🧪 Batch 49/55 — Loss: 0.0633 — Time: 0.44s\n","  🧪 Batch 50/55 — Loss: 0.0709 — Time: 0.44s\n","  🧪 Batch 51/55 — Loss: 0.0645 — Time: 0.45s\n","  🧪 Batch 52/55 — Loss: 0.0741 — Time: 0.44s\n","  🧪 Batch 53/55 — Loss: 0.0721 — Time: 0.44s\n","  🧪 Batch 54/55 — Loss: 0.0612 — Time: 0.44s\n","  🧪 Batch 55/55 — Loss: 0.0834 — Time: 0.28s\n","🕒 Epoch Time: 27.44s | Avg Batch Time: 0.44s\n","📊 Train Loss: 0.0765 | Val Loss: 0.2289 | IoU: 0.8197 | Dice: 0.8994 | Jaccard: 0.8197 | Precision: 0.9166 | Recall: 0.8846 | Accuracy: 0.9760\n","\n","Epoch 30/100\n","  🧪 Batch 1/55 — Loss: 0.0694 — Time: 0.44s\n","  🧪 Batch 2/55 — Loss: 0.0914 — Time: 0.44s\n","  🧪 Batch 3/55 — Loss: 0.0739 — Time: 0.44s\n","  🧪 Batch 4/55 — Loss: 0.0606 — Time: 0.44s\n","  🧪 Batch 5/55 — Loss: 0.0663 — Time: 0.45s\n","  🧪 Batch 6/55 — Loss: 0.0937 — Time: 0.44s\n","  🧪 Batch 7/55 — Loss: 0.0846 — Time: 0.44s\n","  🧪 Batch 8/55 — Loss: 0.0804 — Time: 0.44s\n","  🧪 Batch 9/55 — Loss: 0.0728 — Time: 0.44s\n","  🧪 Batch 10/55 — Loss: 0.0858 — Time: 0.45s\n","  🧪 Batch 11/55 — Loss: 0.0844 — Time: 0.44s\n","  🧪 Batch 12/55 — Loss: 0.0665 — Time: 0.44s\n","  🧪 Batch 13/55 — Loss: 0.0924 — Time: 0.44s\n","  🧪 Batch 14/55 — Loss: 0.0717 — Time: 0.44s\n","  🧪 Batch 15/55 — Loss: 0.0726 — Time: 0.44s\n","  🧪 Batch 16/55 — Loss: 0.0894 — Time: 0.44s\n","  🧪 Batch 17/55 — Loss: 0.0685 — Time: 0.44s\n","  🧪 Batch 18/55 — Loss: 0.0714 — Time: 0.47s\n","  🧪 Batch 19/55 — Loss: 0.0718 — Time: 0.44s\n","  🧪 Batch 20/55 — Loss: 0.0879 — Time: 0.44s\n","  🧪 Batch 21/55 — Loss: 0.0601 — Time: 0.49s\n","  🧪 Batch 22/55 — Loss: 0.0908 — Time: 0.44s\n","  🧪 Batch 23/55 — Loss: 0.0782 — Time: 0.44s\n","  🧪 Batch 24/55 — Loss: 0.0642 — Time: 0.47s\n","  🧪 Batch 25/55 — Loss: 0.0749 — Time: 0.50s\n","  🧪 Batch 26/55 — Loss: 0.0705 — Time: 0.44s\n","  🧪 Batch 27/55 — Loss: 0.0750 — Time: 0.44s\n","  🧪 Batch 28/55 — Loss: 0.0715 — Time: 0.44s\n","  🧪 Batch 29/55 — Loss: 0.0628 — Time: 0.45s\n","  🧪 Batch 30/55 — Loss: 0.0677 — Time: 0.44s\n","  🧪 Batch 31/55 — Loss: 0.0879 — Time: 0.44s\n","  🧪 Batch 32/55 — Loss: 0.0870 — Time: 0.44s\n","  🧪 Batch 33/55 — Loss: 0.0702 — Time: 0.44s\n","  🧪 Batch 34/55 — Loss: 0.0601 — Time: 0.44s\n","  🧪 Batch 35/55 — Loss: 0.0687 — Time: 0.44s\n","  🧪 Batch 36/55 — Loss: 0.0760 — Time: 0.44s\n","  🧪 Batch 37/55 — Loss: 0.0703 — Time: 0.44s\n","  🧪 Batch 38/55 — Loss: 0.0783 — Time: 0.44s\n","  🧪 Batch 39/55 — Loss: 0.0747 — Time: 0.44s\n","  🧪 Batch 40/55 — Loss: 0.0760 — Time: 0.44s\n","  🧪 Batch 41/55 — Loss: 0.0728 — Time: 0.44s\n","  🧪 Batch 42/55 — Loss: 0.0687 — Time: 0.44s\n","  🧪 Batch 43/55 — Loss: 0.0879 — Time: 0.44s\n","  🧪 Batch 44/55 — Loss: 0.0682 — Time: 0.44s\n","  🧪 Batch 45/55 — Loss: 0.0704 — Time: 0.44s\n","  🧪 Batch 46/55 — Loss: 0.0677 — Time: 0.44s\n","  🧪 Batch 47/55 — Loss: 0.0769 — Time: 0.44s\n","  🧪 Batch 48/55 — Loss: 0.0677 — Time: 0.44s\n","  🧪 Batch 49/55 — Loss: 0.0800 — Time: 0.44s\n","  🧪 Batch 50/55 — Loss: 0.0645 — Time: 0.44s\n","  🧪 Batch 51/55 — Loss: 0.0653 — Time: 0.44s\n","  🧪 Batch 52/55 — Loss: 0.0658 — Time: 0.44s\n","  🧪 Batch 53/55 — Loss: 0.0649 — Time: 0.44s\n","  🧪 Batch 54/55 — Loss: 0.0636 — Time: 0.44s\n","  🧪 Batch 55/55 — Loss: 0.0736 — Time: 0.29s\n","🕒 Epoch Time: 27.54s | Avg Batch Time: 0.44s\n","📊 Train Loss: 0.0742 | Val Loss: 0.2255 | IoU: 0.8228 | Dice: 0.9011 | Jaccard: 0.8228 | Precision: 0.9224 | Recall: 0.8829 | Accuracy: 0.9764\n","\n","Epoch 31/100\n","  🧪 Batch 1/55 — Loss: 0.0689 — Time: 0.44s\n","  🧪 Batch 2/55 — Loss: 0.0606 — Time: 0.44s\n","  🧪 Batch 3/55 — Loss: 0.0742 — Time: 0.44s\n","  🧪 Batch 4/55 — Loss: 0.1167 — Time: 0.44s\n","  🧪 Batch 5/55 — Loss: 0.0613 — Time: 0.44s\n","  🧪 Batch 6/55 — Loss: 0.0801 — Time: 0.44s\n","  🧪 Batch 7/55 — Loss: 0.0599 — Time: 0.44s\n","  🧪 Batch 8/55 — Loss: 0.0725 — Time: 0.44s\n","  🧪 Batch 9/55 — Loss: 0.0649 — Time: 0.44s\n","  🧪 Batch 10/55 — Loss: 0.0694 — Time: 0.44s\n","  🧪 Batch 11/55 — Loss: 0.0711 — Time: 0.44s\n","  🧪 Batch 12/55 — Loss: 0.0704 — Time: 0.44s\n","  🧪 Batch 13/55 — Loss: 0.0649 — Time: 0.44s\n","  🧪 Batch 14/55 — Loss: 0.0592 — Time: 0.44s\n","  🧪 Batch 15/55 — Loss: 0.0699 — Time: 0.44s\n","  🧪 Batch 16/55 — Loss: 0.0656 — Time: 0.44s\n","  🧪 Batch 17/55 — Loss: 0.0615 — Time: 0.44s\n","  🧪 Batch 18/55 — Loss: 0.0744 — Time: 0.44s\n","  🧪 Batch 19/55 — Loss: 0.0687 — Time: 0.50s\n","  🧪 Batch 20/55 — Loss: 0.0728 — Time: 0.55s\n","  🧪 Batch 21/55 — Loss: 0.0690 — Time: 0.53s\n","  🧪 Batch 22/55 — Loss: 0.0726 — Time: 0.59s\n","  🧪 Batch 23/55 — Loss: 0.0633 — Time: 0.56s\n","  🧪 Batch 24/55 — Loss: 0.0664 — Time: 0.51s\n","  🧪 Batch 25/55 — Loss: 0.0696 — Time: 0.60s\n","  🧪 Batch 26/55 — Loss: 0.0823 — Time: 0.65s\n","  🧪 Batch 27/55 — Loss: 0.0651 — Time: 0.51s\n","  🧪 Batch 28/55 — Loss: 0.0783 — Time: 0.50s\n","  🧪 Batch 29/55 — Loss: 0.0822 — Time: 0.44s\n","  🧪 Batch 30/55 — Loss: 0.0627 — Time: 0.45s\n","  🧪 Batch 31/55 — Loss: 0.0745 — Time: 0.44s\n","  🧪 Batch 32/55 — Loss: 0.0675 — Time: 0.44s\n","  🧪 Batch 33/55 — Loss: 0.0726 — Time: 0.44s\n","  🧪 Batch 34/55 — Loss: 0.0785 — Time: 0.44s\n","  🧪 Batch 35/55 — Loss: 0.0571 — Time: 0.44s\n","  🧪 Batch 36/55 — Loss: 0.0637 — Time: 0.44s\n","  🧪 Batch 37/55 — Loss: 0.0948 — Time: 0.44s\n","  🧪 Batch 38/55 — Loss: 0.0623 — Time: 0.44s\n","  🧪 Batch 39/55 — Loss: 0.0665 — Time: 0.44s\n","  🧪 Batch 40/55 — Loss: 0.0709 — Time: 0.44s\n","  🧪 Batch 41/55 — Loss: 0.0582 — Time: 0.44s\n","  🧪 Batch 42/55 — Loss: 0.0796 — Time: 0.44s\n","  🧪 Batch 43/55 — Loss: 0.0848 — Time: 0.44s\n","  🧪 Batch 44/55 — Loss: 0.0730 — Time: 0.44s\n","  🧪 Batch 45/55 — Loss: 0.0710 — Time: 0.44s\n","  🧪 Batch 46/55 — Loss: 0.0929 — Time: 0.44s\n","  🧪 Batch 47/55 — Loss: 0.0715 — Time: 0.44s\n","  🧪 Batch 48/55 — Loss: 0.0739 — Time: 0.44s\n","  🧪 Batch 49/55 — Loss: 0.0735 — Time: 0.44s\n","  🧪 Batch 50/55 — Loss: 0.0766 — Time: 0.49s\n","  🧪 Batch 51/55 — Loss: 0.0736 — Time: 0.44s\n","  🧪 Batch 52/55 — Loss: 0.0720 — Time: 0.45s\n","  🧪 Batch 53/55 — Loss: 0.0741 — Time: 0.44s\n","  🧪 Batch 54/55 — Loss: 0.0715 — Time: 0.44s\n","  🧪 Batch 55/55 — Loss: 0.0686 — Time: 0.29s\n","🕒 Epoch Time: 28.33s | Avg Batch Time: 0.46s\n","📊 Train Loss: 0.0717 | Val Loss: 0.2299 | IoU: 0.8185 | Dice: 0.8987 | Jaccard: 0.8185 | Precision: 0.9112 | Recall: 0.8884 | Accuracy: 0.9757\n","\n","Epoch 32/100\n","  🧪 Batch 1/55 — Loss: 0.0682 — Time: 0.44s\n","  🧪 Batch 2/55 — Loss: 0.0881 — Time: 0.44s\n","  🧪 Batch 3/55 — Loss: 0.0689 — Time: 0.44s\n","  🧪 Batch 4/55 — Loss: 0.0670 — Time: 0.44s\n","  🧪 Batch 5/55 — Loss: 0.0682 — Time: 0.44s\n","  🧪 Batch 6/55 — Loss: 0.0614 — Time: 0.44s\n","  🧪 Batch 7/55 — Loss: 0.0665 — Time: 0.44s\n","  🧪 Batch 8/55 — Loss: 0.0656 — Time: 0.44s\n","  🧪 Batch 9/55 — Loss: 0.0702 — Time: 0.44s\n","  🧪 Batch 10/55 — Loss: 0.0619 — Time: 0.44s\n","  🧪 Batch 11/55 — Loss: 0.0588 — Time: 0.44s\n","  🧪 Batch 12/55 — Loss: 0.0687 — Time: 0.44s\n","  🧪 Batch 13/55 — Loss: 0.0755 — Time: 0.44s\n","  🧪 Batch 14/55 — Loss: 0.0584 — Time: 0.44s\n","  🧪 Batch 15/55 — Loss: 0.0791 — Time: 0.44s\n","  🧪 Batch 16/55 — Loss: 0.0650 — Time: 0.44s\n","  🧪 Batch 17/55 — Loss: 0.0643 — Time: 0.44s\n","  🧪 Batch 18/55 — Loss: 0.0624 — Time: 0.44s\n","  🧪 Batch 19/55 — Loss: 0.0652 — Time: 0.44s\n","  🧪 Batch 20/55 — Loss: 0.0724 — Time: 0.44s\n","  🧪 Batch 21/55 — Loss: 0.0710 — Time: 0.44s\n","  🧪 Batch 22/55 — Loss: 0.0657 — Time: 0.44s\n","  🧪 Batch 23/55 — Loss: 0.0622 — Time: 0.49s\n","  🧪 Batch 24/55 — Loss: 0.0606 — Time: 0.49s\n","  🧪 Batch 25/55 — Loss: 0.0725 — Time: 0.44s\n","  🧪 Batch 26/55 — Loss: 0.0739 — Time: 0.44s\n","  🧪 Batch 27/55 — Loss: 0.0679 — Time: 0.44s\n","  🧪 Batch 28/55 — Loss: 0.0613 — Time: 0.44s\n","  🧪 Batch 29/55 — Loss: 0.0684 — Time: 0.44s\n","  🧪 Batch 30/55 — Loss: 0.0673 — Time: 0.44s\n","  🧪 Batch 31/55 — Loss: 0.0631 — Time: 0.45s\n","  🧪 Batch 32/55 — Loss: 0.0615 — Time: 0.44s\n","  🧪 Batch 33/55 — Loss: 0.0718 — Time: 0.44s\n","  🧪 Batch 34/55 — Loss: 0.0813 — Time: 0.44s\n","  🧪 Batch 35/55 — Loss: 0.0612 — Time: 0.44s\n","  🧪 Batch 36/55 — Loss: 0.0589 — Time: 0.44s\n","  🧪 Batch 37/55 — Loss: 0.0677 — Time: 0.44s\n","  🧪 Batch 38/55 — Loss: 0.0732 — Time: 0.44s\n","  🧪 Batch 39/55 — Loss: 0.0590 — Time: 0.44s\n","  🧪 Batch 40/55 — Loss: 0.0634 — Time: 0.44s\n","  🧪 Batch 41/55 — Loss: 0.0647 — Time: 0.44s\n","  🧪 Batch 42/55 — Loss: 0.0683 — Time: 0.44s\n","  🧪 Batch 43/55 — Loss: 0.0725 — Time: 0.44s\n","  🧪 Batch 44/55 — Loss: 0.0674 — Time: 0.44s\n","  🧪 Batch 45/55 — Loss: 0.0691 — Time: 0.44s\n","  🧪 Batch 46/55 — Loss: 0.0656 — Time: 0.44s\n","  🧪 Batch 47/55 — Loss: 0.0698 — Time: 0.44s\n","  🧪 Batch 48/55 — Loss: 0.0755 — Time: 0.44s\n","  🧪 Batch 49/55 — Loss: 0.0675 — Time: 0.44s\n","  🧪 Batch 50/55 — Loss: 0.0709 — Time: 0.46s\n","  🧪 Batch 51/55 — Loss: 0.0649 — Time: 0.45s\n","  🧪 Batch 52/55 — Loss: 0.0662 — Time: 0.44s\n","  🧪 Batch 53/55 — Loss: 0.0626 — Time: 0.44s\n","  🧪 Batch 54/55 — Loss: 0.0634 — Time: 0.44s\n","  🧪 Batch 55/55 — Loss: 0.0578 — Time: 0.29s\n","🕒 Epoch Time: 27.39s | Avg Batch Time: 0.44s\n","📊 Train Loss: 0.0672 | Val Loss: 0.2291 | IoU: 0.8180 | Dice: 0.8984 | Jaccard: 0.8180 | Precision: 0.9151 | Recall: 0.8838 | Accuracy: 0.9757\n","\n","Epoch 33/100\n","  🧪 Batch 1/55 — Loss: 0.0605 — Time: 0.44s\n","  🧪 Batch 2/55 — Loss: 0.0770 — Time: 0.43s\n","  🧪 Batch 3/55 — Loss: 0.0582 — Time: 0.44s\n","  🧪 Batch 4/55 — Loss: 0.0527 — Time: 0.44s\n","  🧪 Batch 5/55 — Loss: 0.0636 — Time: 0.44s\n","  🧪 Batch 6/55 — Loss: 0.0713 — Time: 0.44s\n","  🧪 Batch 7/55 — Loss: 0.0583 — Time: 0.44s\n","  🧪 Batch 8/55 — Loss: 0.0555 — Time: 0.44s\n","  🧪 Batch 9/55 — Loss: 0.0677 — Time: 0.44s\n","  🧪 Batch 10/55 — Loss: 0.0538 — Time: 0.44s\n","  🧪 Batch 11/55 — Loss: 0.0663 — Time: 0.44s\n","  🧪 Batch 12/55 — Loss: 0.0584 — Time: 0.44s\n","  🧪 Batch 13/55 — Loss: 0.0681 — Time: 0.44s\n","  🧪 Batch 14/55 — Loss: 0.0847 — Time: 0.44s\n","  🧪 Batch 15/55 — Loss: 0.0581 — Time: 0.44s\n","  🧪 Batch 16/55 — Loss: 0.0707 — Time: 0.44s\n","  🧪 Batch 17/55 — Loss: 0.0687 — Time: 0.44s\n","  🧪 Batch 18/55 — Loss: 0.0659 — Time: 0.44s\n","  🧪 Batch 19/55 — Loss: 0.0595 — Time: 0.45s\n","  🧪 Batch 20/55 — Loss: 0.0713 — Time: 0.45s\n","  🧪 Batch 21/55 — Loss: 0.0667 — Time: 0.46s\n","  🧪 Batch 22/55 — Loss: 0.0668 — Time: 0.48s\n","  🧪 Batch 23/55 — Loss: 0.0594 — Time: 0.47s\n","  🧪 Batch 24/55 — Loss: 0.0723 — Time: 0.51s\n","  🧪 Batch 25/55 — Loss: 0.0687 — Time: 0.48s\n","  🧪 Batch 26/55 — Loss: 0.0634 — Time: 0.47s\n","  🧪 Batch 27/55 — Loss: 0.0696 — Time: 0.46s\n","  🧪 Batch 28/55 — Loss: 0.0706 — Time: 0.45s\n","  🧪 Batch 29/55 — Loss: 0.0635 — Time: 0.44s\n","  🧪 Batch 30/55 — Loss: 0.0665 — Time: 0.44s\n","  🧪 Batch 31/55 — Loss: 0.0579 — Time: 0.43s\n","  🧪 Batch 32/55 — Loss: 0.0571 — Time: 0.44s\n","  🧪 Batch 33/55 — Loss: 0.0685 — Time: 0.44s\n","  🧪 Batch 34/55 — Loss: 0.0682 — Time: 0.44s\n","  🧪 Batch 35/55 — Loss: 0.0598 — Time: 0.44s\n","  🧪 Batch 36/55 — Loss: 0.0631 — Time: 0.44s\n","  🧪 Batch 37/55 — Loss: 0.0659 — Time: 0.44s\n","  🧪 Batch 38/55 — Loss: 0.0595 — Time: 0.44s\n","  🧪 Batch 39/55 — Loss: 0.0734 — Time: 0.44s\n","  🧪 Batch 40/55 — Loss: 0.0762 — Time: 0.44s\n","  🧪 Batch 41/55 — Loss: 0.0607 — Time: 0.44s\n","  🧪 Batch 42/55 — Loss: 0.0706 — Time: 0.44s\n","  🧪 Batch 43/55 — Loss: 0.0653 — Time: 0.44s\n","  🧪 Batch 44/55 — Loss: 0.0625 — Time: 0.44s\n","  🧪 Batch 45/55 — Loss: 0.0650 — Time: 0.44s\n","  🧪 Batch 46/55 — Loss: 0.0612 — Time: 0.44s\n","  🧪 Batch 47/55 — Loss: 0.0590 — Time: 0.44s\n","  🧪 Batch 48/55 — Loss: 0.0642 — Time: 0.44s\n","  🧪 Batch 49/55 — Loss: 0.0558 — Time: 0.44s\n","  🧪 Batch 50/55 — Loss: 0.0610 — Time: 0.45s\n","  🧪 Batch 51/55 — Loss: 0.1009 — Time: 0.44s\n","  🧪 Batch 52/55 — Loss: 0.0806 — Time: 0.44s\n","  🧪 Batch 53/55 — Loss: 0.0609 — Time: 0.44s\n","  🧪 Batch 54/55 — Loss: 0.0605 — Time: 0.44s\n","  🧪 Batch 55/55 — Loss: 0.0712 — Time: 0.29s\n","🕒 Epoch Time: 27.56s | Avg Batch Time: 0.44s\n","📊 Train Loss: 0.0656 | Val Loss: 0.2326 | IoU: 0.8161 | Dice: 0.8972 | Jaccard: 0.8161 | Precision: 0.9272 | Recall: 0.8711 | Accuracy: 0.9757\n","\n","Epoch 34/100\n","  🧪 Batch 1/55 — Loss: 0.0626 — Time: 0.45s\n","  🧪 Batch 2/55 — Loss: 0.0562 — Time: 0.44s\n","  🧪 Batch 3/55 — Loss: 0.0616 — Time: 0.44s\n","  🧪 Batch 4/55 — Loss: 0.0555 — Time: 0.44s\n","  🧪 Batch 5/55 — Loss: 0.0612 — Time: 0.44s\n","  🧪 Batch 6/55 — Loss: 0.0692 — Time: 0.44s\n","  🧪 Batch 7/55 — Loss: 0.0796 — Time: 0.44s\n","  🧪 Batch 8/55 — Loss: 0.0715 — Time: 0.44s\n","  🧪 Batch 9/55 — Loss: 0.0574 — Time: 0.44s\n","  🧪 Batch 10/55 — Loss: 0.0537 — Time: 0.44s\n","  🧪 Batch 11/55 — Loss: 0.0701 — Time: 0.43s\n","  🧪 Batch 12/55 — Loss: 0.0581 — Time: 0.44s\n","  🧪 Batch 13/55 — Loss: 0.0629 — Time: 0.44s\n","  🧪 Batch 14/55 — Loss: 0.0540 — Time: 0.44s\n","  🧪 Batch 15/55 — Loss: 0.0707 — Time: 0.44s\n","  🧪 Batch 16/55 — Loss: 0.0585 — Time: 0.44s\n","  🧪 Batch 17/55 — Loss: 0.0663 — Time: 0.44s\n","  🧪 Batch 18/55 — Loss: 0.0541 — Time: 0.44s\n","  🧪 Batch 19/55 — Loss: 0.0699 — Time: 0.44s\n","  🧪 Batch 20/55 — Loss: 0.0605 — Time: 0.44s\n","  🧪 Batch 21/55 — Loss: 0.0653 — Time: 0.44s\n","  🧪 Batch 22/55 — Loss: 0.0558 — Time: 0.44s\n","  🧪 Batch 23/55 — Loss: 0.0654 — Time: 0.47s\n","  🧪 Batch 24/55 — Loss: 0.0594 — Time: 0.45s\n","  🧪 Batch 25/55 — Loss: 0.0612 — Time: 0.50s\n","  🧪 Batch 26/55 — Loss: 0.0633 — Time: 0.51s\n","  🧪 Batch 27/55 — Loss: 0.0617 — Time: 0.44s\n","  🧪 Batch 28/55 — Loss: 0.0620 — Time: 0.44s\n","  🧪 Batch 29/55 — Loss: 0.0676 — Time: 0.44s\n","  🧪 Batch 30/55 — Loss: 0.0701 — Time: 0.45s\n","  🧪 Batch 31/55 — Loss: 0.0566 — Time: 0.44s\n","  🧪 Batch 32/55 — Loss: 0.0676 — Time: 0.45s\n","  🧪 Batch 33/55 — Loss: 0.0646 — Time: 0.44s\n","  🧪 Batch 34/55 — Loss: 0.0570 — Time: 0.44s\n","  🧪 Batch 35/55 — Loss: 0.0642 — Time: 0.44s\n","  🧪 Batch 36/55 — Loss: 0.0609 — Time: 0.44s\n","  🧪 Batch 37/55 — Loss: 0.0728 — Time: 0.44s\n","  🧪 Batch 38/55 — Loss: 0.0697 — Time: 0.44s\n","  🧪 Batch 39/55 — Loss: 0.0863 — Time: 0.44s\n","  🧪 Batch 40/55 — Loss: 0.0620 — Time: 0.44s\n","  🧪 Batch 41/55 — Loss: 0.0662 — Time: 0.44s\n","  🧪 Batch 42/55 — Loss: 0.0655 — Time: 0.44s\n","  🧪 Batch 43/55 — Loss: 0.0719 — Time: 0.44s\n","  🧪 Batch 44/55 — Loss: 0.0558 — Time: 0.44s\n","  🧪 Batch 45/55 — Loss: 0.0646 — Time: 0.44s\n","  🧪 Batch 46/55 — Loss: 0.0640 — Time: 0.44s\n","  🧪 Batch 47/55 — Loss: 0.0659 — Time: 0.44s\n","  🧪 Batch 48/55 — Loss: 0.0621 — Time: 0.44s\n","  🧪 Batch 49/55 — Loss: 0.0563 — Time: 0.44s\n","  🧪 Batch 50/55 — Loss: 0.0682 — Time: 0.44s\n","  🧪 Batch 51/55 — Loss: 0.0597 — Time: 0.44s\n","  🧪 Batch 52/55 — Loss: 0.0883 — Time: 0.44s\n","  🧪 Batch 53/55 — Loss: 0.0685 — Time: 0.44s\n","  🧪 Batch 54/55 — Loss: 0.0828 — Time: 0.44s\n","  🧪 Batch 55/55 — Loss: 0.0611 — Time: 0.29s\n","🕒 Epoch Time: 27.54s | Avg Batch Time: 0.44s\n","📊 Train Loss: 0.0645 | Val Loss: 0.2258 | IoU: 0.8212 | Dice: 0.9004 | Jaccard: 0.8212 | Precision: 0.9303 | Recall: 0.8746 | Accuracy: 0.9765\n","\n","Epoch 35/100\n","  🧪 Batch 1/55 — Loss: 0.0660 — Time: 0.54s\n","  🧪 Batch 2/55 — Loss: 0.0674 — Time: 0.47s\n","  🧪 Batch 3/55 — Loss: 0.0648 — Time: 0.52s\n","  🧪 Batch 4/55 — Loss: 0.0613 — Time: 0.44s\n","  🧪 Batch 5/55 — Loss: 0.0656 — Time: 0.53s\n","  🧪 Batch 6/55 — Loss: 0.0608 — Time: 0.52s\n","  🧪 Batch 7/55 — Loss: 0.0636 — Time: 0.43s\n","  🧪 Batch 8/55 — Loss: 0.0638 — Time: 0.44s\n","  🧪 Batch 9/55 — Loss: 0.0570 — Time: 0.44s\n","  🧪 Batch 10/55 — Loss: 0.0737 — Time: 0.44s\n","  🧪 Batch 11/55 — Loss: 0.0731 — Time: 0.44s\n","  🧪 Batch 12/55 — Loss: 0.0586 — Time: 0.44s\n","  🧪 Batch 13/55 — Loss: 0.0604 — Time: 0.44s\n","  🧪 Batch 14/55 — Loss: 0.0564 — Time: 0.44s\n","  🧪 Batch 15/55 — Loss: 0.0556 — Time: 0.44s\n","  🧪 Batch 16/55 — Loss: 0.0597 — Time: 0.44s\n","  🧪 Batch 17/55 — Loss: 0.0653 — Time: 0.44s\n","  🧪 Batch 18/55 — Loss: 0.0593 — Time: 0.45s\n","  🧪 Batch 19/55 — Loss: 0.0771 — Time: 0.47s\n","  🧪 Batch 20/55 — Loss: 0.0591 — Time: 0.44s\n","  🧪 Batch 21/55 — Loss: 0.0613 — Time: 0.44s\n","  🧪 Batch 22/55 — Loss: 0.0587 — Time: 0.46s\n","  🧪 Batch 23/55 — Loss: 0.0566 — Time: 0.47s\n","  🧪 Batch 24/55 — Loss: 0.0661 — Time: 0.44s\n","  🧪 Batch 25/55 — Loss: 0.0795 — Time: 0.44s\n","  🧪 Batch 26/55 — Loss: 0.0561 — Time: 0.44s\n","  🧪 Batch 27/55 — Loss: 0.0584 — Time: 0.45s\n","  🧪 Batch 28/55 — Loss: 0.0665 — Time: 0.44s\n","  🧪 Batch 29/55 — Loss: 0.0568 — Time: 0.44s\n","  🧪 Batch 30/55 — Loss: 0.0601 — Time: 0.44s\n","  🧪 Batch 31/55 — Loss: 0.0566 — Time: 0.44s\n","  🧪 Batch 32/55 — Loss: 0.0549 — Time: 0.44s\n","  🧪 Batch 33/55 — Loss: 0.0740 — Time: 0.44s\n","  🧪 Batch 34/55 — Loss: 0.0618 — Time: 0.44s\n","  🧪 Batch 35/55 — Loss: 0.0601 — Time: 0.44s\n","  🧪 Batch 36/55 — Loss: 0.0614 — Time: 0.44s\n","  🧪 Batch 37/55 — Loss: 0.0605 — Time: 0.44s\n","  🧪 Batch 38/55 — Loss: 0.0629 — Time: 0.44s\n","  🧪 Batch 39/55 — Loss: 0.0653 — Time: 0.44s\n","  🧪 Batch 40/55 — Loss: 0.0628 — Time: 0.44s\n","  🧪 Batch 41/55 — Loss: 0.0640 — Time: 0.44s\n","  🧪 Batch 42/55 — Loss: 0.0612 — Time: 0.44s\n","  🧪 Batch 43/55 — Loss: 0.0618 — Time: 0.44s\n","  🧪 Batch 44/55 — Loss: 0.0641 — Time: 0.44s\n","  🧪 Batch 45/55 — Loss: 0.0831 — Time: 0.44s\n","  🧪 Batch 46/55 — Loss: 0.0742 — Time: 0.44s\n","  🧪 Batch 47/55 — Loss: 0.0552 — Time: 0.44s\n","  🧪 Batch 48/55 — Loss: 0.0637 — Time: 0.44s\n","  🧪 Batch 49/55 — Loss: 0.0592 — Time: 0.45s\n","  🧪 Batch 50/55 — Loss: 0.0664 — Time: 0.49s\n","  🧪 Batch 51/55 — Loss: 0.0698 — Time: 0.46s\n","  🧪 Batch 52/55 — Loss: 0.0591 — Time: 0.44s\n","  🧪 Batch 53/55 — Loss: 0.0594 — Time: 0.44s\n","  🧪 Batch 54/55 — Loss: 0.0598 — Time: 0.44s\n","  🧪 Batch 55/55 — Loss: 0.0587 — Time: 0.29s\n","🕒 Epoch Time: 28.09s | Avg Batch Time: 0.45s\n","📊 Train Loss: 0.0631 | Val Loss: 0.2292 | IoU: 0.8167 | Dice: 0.8976 | Jaccard: 0.8167 | Precision: 0.9302 | Recall: 0.8687 | Accuracy: 0.9759\n","⏹️ Early stopping triggered after 35 epochs.\n","\n","🧪 Testing model trained on CombinedDataset on all datasets...\n","| Trained On      | Tested On       |     Loss |      IoU |     Dice |   Jaccard |   Precision |   Recall |   Accuracy |\n","|:----------------|:----------------|---------:|---------:|---------:|----------:|------------:|---------:|-----------:|\n","| CombinedDataset | CombinedDataset | 0.185574 | 0.841047 | 0.913258 |  0.841047 |    0.939811 | 0.888972 |   0.980911 |\n","\n","⏱️ Total Time for Training + Testing: 979.14 seconds\n","\n","📋 Final Cross-Dataset Testing Summary:\n","| Trained On      | Tested On       |     Loss |      IoU |     Dice |   Jaccard |   Precision |   Recall |   Accuracy |\n","|:----------------|:----------------|---------:|---------:|---------:|----------:|------------:|---------:|-----------:|\n","| CombinedDataset | CombinedDataset | 0.185574 | 0.841047 | 0.913258 |  0.841047 |    0.939811 | 0.888972 |   0.980911 |\n"]}]},{"cell_type":"code","source":["evaluation_df = evaluate_model_on_datasets(trained_model, separate_loaders)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"nh2VRy5px8l1","executionInfo":{"status":"ok","timestamp":1745354753664,"user_tz":240,"elapsed":3087,"user":{"displayName":"Ashok Kamath","userId":"01269285531323045015"}},"outputId":"e9b01812-a418-40c6-dd7d-08d594d16c77"},"execution_count":29,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","🧪 Evaluating trained model on all test sets...\n","\n","🔍 Testing on: CVC-ClinicDB\n","🔍 Testing on: CVC-ColonDB\n","🔍 Testing on: ETIS-LaribPolypDB\n","🔍 Testing on: Kvasir-SEG\n","\n","⏱️ Total Evaluation Time: 3.05 seconds\n","\n","📋 Test Results Across Datasets (with Average):\n","| Tested On         |     Loss |      IoU |     Dice |   Jaccard |   Precision |   Recall |   Accuracy |\n","|:------------------|---------:|---------:|---------:|----------:|------------:|---------:|-----------:|\n","| CVC-ClinicDB      | 0.173286 | 0.846071 | 0.915336 |  0.846071 |    0.944182 | 0.892699 |   0.984647 |\n","| CVC-ColonDB       | 0.138843 | 0.85811  | 0.923507 |  0.85811  |    0.93774  | 0.909704 |   0.992436 |\n","| ETIS-LaribPolypDB | 0.464115 | 0.51178  | 0.638788 |  0.51178  |    0.938224 | 0.540285 |   0.984364 |\n","| Kvasir-SEG        | 0.198417 | 0.848067 | 0.9172   |  0.848067 |    0.933992 | 0.901806 |   0.975417 |\n","| Average           | 0.243665 | 0.766007 | 0.848708 |  0.766007 |    0.938534 | 0.811123 |   0.984216 |\n"]}]},{"cell_type":"markdown","source":["# U-Net++ (no pretrained weights)\n"],"metadata":{"id":"KrTaZDMkA6ZR"}},{"cell_type":"code","source":["def unet_no_PT_weights_model_factory():\n","    return smp.UnetPlusPlus(\n","        encoder_name=\"resnet34\",     # Same encoder as your U-Net\n","        encoder_weights=None,  # pretrained on ImageNet\n","        in_channels=3,               # rgb input\n","        classes=1,                   # Binary segmentation (e.g., polyps)\n","    )"],"metadata":{"id":"auu5otDqA_bc","executionInfo":{"status":"ok","timestamp":1745351391759,"user_tz":240,"elapsed":4,"user":{"displayName":"Ashok Kamath","userId":"01269285531323045015"}}},"execution_count":25,"outputs":[]},{"cell_type":"code","source":["u_net_results, trained_model = train_and_cross_test(model_class=unet_no_PT_weights_model_factory, all_data_loaders=[combined_loaders], num_epochs=1)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ygPmgsg-BJb-","executionInfo":{"status":"ok","timestamp":1745352335537,"user_tz":240,"elapsed":942525,"user":{"displayName":"Ashok Kamath","userId":"01269285531323045015"}},"outputId":"64b3ab0e-5689-452d-90ba-658f82f7dbef"},"execution_count":26,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","🚀 Training on: CombinedDataset\n","\n","Epoch 1/1\n","  🧪 Batch 1/55 — Loss: 1.5803 — Time: 1.50s\n","  🧪 Batch 2/55 — Loss: 1.5544 — Time: 0.81s\n","  🧪 Batch 3/55 — Loss: 1.5796 — Time: 0.85s\n","  🧪 Batch 4/55 — Loss: 1.5413 — Time: 0.81s\n","  🧪 Batch 5/55 — Loss: 1.3381 — Time: 0.87s\n","  🧪 Batch 6/55 — Loss: 1.4447 — Time: 0.81s\n","  🧪 Batch 7/55 — Loss: 1.4720 — Time: 0.87s\n","  🧪 Batch 8/55 — Loss: 1.3836 — Time: 0.81s\n","  🧪 Batch 9/55 — Loss: 1.4174 — Time: 0.86s\n","  🧪 Batch 10/55 — Loss: 1.4023 — Time: 0.81s\n","  🧪 Batch 11/55 — Loss: 1.4084 — Time: 0.86s\n","  🧪 Batch 12/55 — Loss: 1.4311 — Time: 0.81s\n","  🧪 Batch 13/55 — Loss: 1.4196 — Time: 0.85s\n","  🧪 Batch 14/55 — Loss: 1.3889 — Time: 0.81s\n","  🧪 Batch 15/55 — Loss: 1.3947 — Time: 0.86s\n","  🧪 Batch 16/55 — Loss: 1.3385 — Time: 0.81s\n","  🧪 Batch 17/55 — Loss: 1.2801 — Time: 0.86s\n","  🧪 Batch 18/55 — Loss: 1.3026 — Time: 0.81s\n","  🧪 Batch 19/55 — Loss: 1.3147 — Time: 0.86s\n","  🧪 Batch 20/55 — Loss: 1.3848 — Time: 0.81s\n","  🧪 Batch 21/55 — Loss: 1.3280 — Time: 0.85s\n","  🧪 Batch 22/55 — Loss: 1.2660 — Time: 0.81s\n","  🧪 Batch 23/55 — Loss: 1.3048 — Time: 0.87s\n","  🧪 Batch 24/55 — Loss: 1.2565 — Time: 0.81s\n","  🧪 Batch 25/55 — Loss: 1.2840 — Time: 0.86s\n","  🧪 Batch 26/55 — Loss: 1.2480 — Time: 0.81s\n","  🧪 Batch 27/55 — Loss: 1.2328 — Time: 0.86s\n","  🧪 Batch 28/55 — Loss: 1.2507 — Time: 0.81s\n","  🧪 Batch 29/55 — Loss: 1.2301 — Time: 0.87s\n","  🧪 Batch 30/55 — Loss: 1.2163 — Time: 0.81s\n","  🧪 Batch 31/55 — Loss: 1.2627 — Time: 0.87s\n","  🧪 Batch 32/55 — Loss: 1.1651 — Time: 0.81s\n","  🧪 Batch 33/55 — Loss: 1.2276 — Time: 0.86s\n","  🧪 Batch 34/55 — Loss: 1.2249 — Time: 0.81s\n","  🧪 Batch 35/55 — Loss: 1.2243 — Time: 0.87s\n","  🧪 Batch 36/55 — Loss: 1.1073 — Time: 0.81s\n","  🧪 Batch 37/55 — Loss: 1.1914 — Time: 0.86s\n","  🧪 Batch 38/55 — Loss: 1.1872 — Time: 0.81s\n","  🧪 Batch 39/55 — Loss: 1.1546 — Time: 0.85s\n","  🧪 Batch 40/55 — Loss: 1.1994 — Time: 0.81s\n","  🧪 Batch 41/55 — Loss: 1.1606 — Time: 0.87s\n","  🧪 Batch 42/55 — Loss: 1.2128 — Time: 0.81s\n","  🧪 Batch 43/55 — Loss: 1.1585 — Time: 0.86s\n","  🧪 Batch 44/55 — Loss: 1.1507 — Time: 0.81s\n","  🧪 Batch 45/55 — Loss: 1.2612 — Time: 0.87s\n","  🧪 Batch 46/55 — Loss: 1.1644 — Time: 0.81s\n","  🧪 Batch 47/55 — Loss: 1.0804 — Time: 0.85s\n","  🧪 Batch 48/55 — Loss: 1.1080 — Time: 0.81s\n","  🧪 Batch 49/55 — Loss: 1.1066 — Time: 0.85s\n","  🧪 Batch 50/55 — Loss: 1.1869 — Time: 0.81s\n","  🧪 Batch 51/55 — Loss: 1.0564 — Time: 0.86s\n","  🧪 Batch 52/55 — Loss: 1.0895 — Time: 0.82s\n","  🧪 Batch 53/55 — Loss: 1.1339 — Time: 0.86s\n","  🧪 Batch 54/55 — Loss: 1.1496 — Time: 0.82s\n","  🧪 Batch 55/55 — Loss: 1.1784 — Time: 0.66s\n","🕒 Epoch Time: 816.18s | Avg Batch Time: 0.84s\n","📊 Train Loss: 1.2752 | Val Loss: 1.1272 | IoU: 0.2016 | Dice: 0.3245 | Jaccard: 0.2016 | Precision: 0.4498 | Recall: 0.2655 | Accuracy: 0.8880\n","\n","🧪 Testing model trained on CombinedDataset on all datasets...\n","| Trained On      | Tested On       |    Loss |      IoU |     Dice |   Jaccard |   Precision |   Recall |   Accuracy |\n","|:----------------|:----------------|--------:|---------:|---------:|----------:|------------:|---------:|-----------:|\n","| CombinedDataset | CombinedDataset | 1.12169 | 0.209385 | 0.333648 |  0.209385 |    0.465571 | 0.273435 |   0.890515 |\n","\n","⏱️ Total Time for Training + Testing: 942.43 seconds\n","\n","📋 Final Cross-Dataset Testing Summary:\n","| Trained On      | Tested On       |    Loss |      IoU |     Dice |   Jaccard |   Precision |   Recall |   Accuracy |\n","|:----------------|:----------------|--------:|---------:|---------:|----------:|------------:|---------:|-----------:|\n","| CombinedDataset | CombinedDataset | 1.12169 | 0.209385 | 0.333648 |  0.209385 |    0.465571 | 0.273435 |   0.890515 |\n"]}]},{"cell_type":"code","source":["evaluation_df = evaluate_model_on_datasets(trained_model, separate_loaders)"],"metadata":{"id":"tkhBjs3VBtuu"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"KcFJ3kp07J0Q"},"source":["# U-Net++"]},{"cell_type":"code","source":["# This crashes the runtime, which restarts the whole Colab environment\n","import os\n","os.kill(os.getpid(), 9)\n"],"metadata":{"id":"DJ7jifFq839k"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!nvidia-smi\n","\n","torch.cuda.empty_cache()\n","torch.cuda.reset_peak_memory_stats()\n","\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"yWJrfG-Y7u5e","executionInfo":{"status":"ok","timestamp":1745265770201,"user_tz":240,"elapsed":215,"user":{"displayName":"Ashok Kamath","userId":"01269285531323045015"}},"outputId":"df1af7e9-06e3-4334-c1e9-d06e36d3a9e2"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Mon Apr 21 20:02:49 2025       \n","+-----------------------------------------------------------------------------------------+\n","| NVIDIA-SMI 550.54.15              Driver Version: 550.54.15      CUDA Version: 12.4     |\n","|-----------------------------------------+------------------------+----------------------+\n","| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n","| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n","|                                         |                        |               MIG M. |\n","|=========================================+========================+======================|\n","|   0  Tesla T4                       Off |   00000000:00:04.0 Off |                    0 |\n","| N/A   75C    P0             33W /   70W |   15092MiB /  15360MiB |      0%      Default |\n","|                                         |                        |                  N/A |\n","+-----------------------------------------+------------------------+----------------------+\n","                                                                                         \n","+-----------------------------------------------------------------------------------------+\n","| Processes:                                                                              |\n","|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n","|        ID   ID                                                               Usage      |\n","|=========================================================================================|\n","+-----------------------------------------------------------------------------------------+\n"]}]},{"cell_type":"code","execution_count":null,"metadata":{"id":"AfCQLIBz7GMp"},"outputs":[],"source":["def unetpp_model_factory():\n","    return smp.UnetPlusPlus(\n","        encoder_name=\"resnet34\",     # Same encoder as your U-Net\n","        encoder_weights=\"imagenet\",  # pretrained on ImageNet\n","        in_channels=3,               # rgb input\n","        classes=1,                   # Binary segmentation (e.g., polyps)\n","    )\n"]},{"cell_type":"code","source":["u_net_pp_results, trained_model = train_and_cross_test(model_class=unetpp_model_factory, all_data_loaders=[combined_loaders], num_epochs=100)\n","#u_net_pp_results = train_and_cross_test(model_class=unetpp_model_factory, all_data_loaders=all_data_loaders, num_epochs=10)\n","print('U-Net Plus Plus Results')\n","print(u_net_pp_results)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"yhYi5dDtQgdm","outputId":"943e5b93-f9d4-4902-e330-6720470c3613","executionInfo":{"status":"ok","timestamp":1745270771162,"user_tz":240,"elapsed":699580,"user":{"displayName":"Ashok Kamath","userId":"01269285531323045015"}}},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","🚀 Training on: CombinedDataset\n","\n","Epoch 1/100\n","  🧪 Batch 1/55 — Loss: 1.4446 — Time: 0.91s\n","  🧪 Batch 2/55 — Loss: 1.3799 — Time: 0.89s\n","  🧪 Batch 3/55 — Loss: 1.3521 — Time: 0.88s\n","  🧪 Batch 4/55 — Loss: 1.3426 — Time: 0.87s\n","  🧪 Batch 5/55 — Loss: 1.3372 — Time: 0.89s\n","  🧪 Batch 6/55 — Loss: 1.2106 — Time: 0.88s\n","  🧪 Batch 7/55 — Loss: 1.2402 — Time: 0.88s\n","  🧪 Batch 8/55 — Loss: 1.2324 — Time: 0.89s\n","  🧪 Batch 9/55 — Loss: 1.2138 — Time: 0.89s\n","  🧪 Batch 10/55 — Loss: 1.1926 — Time: 0.89s\n","  🧪 Batch 11/55 — Loss: 1.2037 — Time: 0.89s\n","  🧪 Batch 12/55 — Loss: 1.1893 — Time: 0.89s\n","  🧪 Batch 13/55 — Loss: 1.1141 — Time: 0.89s\n","  🧪 Batch 14/55 — Loss: 1.1224 — Time: 0.91s\n","  🧪 Batch 15/55 — Loss: 1.1830 — Time: 0.90s\n","  🧪 Batch 16/55 — Loss: 1.1139 — Time: 0.91s\n","  🧪 Batch 17/55 — Loss: 1.1111 — Time: 0.91s\n","  🧪 Batch 18/55 — Loss: 1.0969 — Time: 0.91s\n","  🧪 Batch 19/55 — Loss: 1.0349 — Time: 0.91s\n","  🧪 Batch 20/55 — Loss: 1.0250 — Time: 0.90s\n","  🧪 Batch 21/55 — Loss: 1.0195 — Time: 0.91s\n","  🧪 Batch 22/55 — Loss: 0.9356 — Time: 0.91s\n","  🧪 Batch 23/55 — Loss: 1.0427 — Time: 0.93s\n","  🧪 Batch 24/55 — Loss: 1.0025 — Time: 0.93s\n","  🧪 Batch 25/55 — Loss: 0.9272 — Time: 0.92s\n","  🧪 Batch 26/55 — Loss: 1.0002 — Time: 0.93s\n","  🧪 Batch 27/55 — Loss: 0.9768 — Time: 0.93s\n","  🧪 Batch 28/55 — Loss: 0.8515 — Time: 0.92s\n","  🧪 Batch 29/55 — Loss: 1.0292 — Time: 0.92s\n","  🧪 Batch 30/55 — Loss: 0.9903 — Time: 0.92s\n","  🧪 Batch 31/55 — Loss: 0.9652 — Time: 0.93s\n","  🧪 Batch 32/55 — Loss: 0.8807 — Time: 0.92s\n","  🧪 Batch 33/55 — Loss: 0.9332 — Time: 0.92s\n","  🧪 Batch 34/55 — Loss: 0.9192 — Time: 0.92s\n","  🧪 Batch 35/55 — Loss: 0.8777 — Time: 0.91s\n","  🧪 Batch 36/55 — Loss: 0.8445 — Time: 0.90s\n","  🧪 Batch 37/55 — Loss: 0.9198 — Time: 0.91s\n","  🧪 Batch 38/55 — Loss: 0.8735 — Time: 0.91s\n","  🧪 Batch 39/55 — Loss: 0.8682 — Time: 0.91s\n","  🧪 Batch 40/55 — Loss: 0.7862 — Time: 0.90s\n","  🧪 Batch 41/55 — Loss: 0.9072 — Time: 0.90s\n","  🧪 Batch 42/55 — Loss: 0.8093 — Time: 0.89s\n","  🧪 Batch 43/55 — Loss: 0.8385 — Time: 0.89s\n","  🧪 Batch 44/55 — Loss: 0.9494 — Time: 0.89s\n","  🧪 Batch 45/55 — Loss: 0.9224 — Time: 0.90s\n","  🧪 Batch 46/55 — Loss: 0.8758 — Time: 0.89s\n","  🧪 Batch 47/55 — Loss: 0.8554 — Time: 0.89s\n","  🧪 Batch 48/55 — Loss: 0.7354 — Time: 0.89s\n","  🧪 Batch 49/55 — Loss: 0.8602 — Time: 0.89s\n","  🧪 Batch 50/55 — Loss: 0.8361 — Time: 0.89s\n","  🧪 Batch 51/55 — Loss: 0.8184 — Time: 0.89s\n","  🧪 Batch 52/55 — Loss: 0.8514 — Time: 0.89s\n","  🧪 Batch 53/55 — Loss: 0.8647 — Time: 0.89s\n","  🧪 Batch 54/55 — Loss: 0.8640 — Time: 0.89s\n","  🧪 Batch 55/55 — Loss: 0.8111 — Time: 0.65s\n","🕒 Epoch Time: 52.73s | Avg Batch Time: 0.90s\n","📊 Train Loss: 1.0106 | Val Loss: 0.8043 | IoU: 0.6302 | Dice: 0.8016 | Jaccard: 0.6694 | Precision: 0.7483 | Recall: 0.8665 | Accuracy: 0.9527\n","\n","Epoch 2/100\n","  🧪 Batch 1/55 — Loss: 0.7568 — Time: 0.87s\n","  🧪 Batch 2/55 — Loss: 0.7902 — Time: 0.87s\n","  🧪 Batch 3/55 — Loss: 0.7798 — Time: 0.89s\n","  🧪 Batch 4/55 — Loss: 0.7251 — Time: 0.88s\n","  🧪 Batch 5/55 — Loss: 0.7547 — Time: 0.88s\n","  🧪 Batch 6/55 — Loss: 0.7334 — Time: 0.88s\n","  🧪 Batch 7/55 — Loss: 0.7188 — Time: 0.88s\n","  🧪 Batch 8/55 — Loss: 0.7514 — Time: 0.87s\n","  🧪 Batch 9/55 — Loss: 0.6973 — Time: 0.87s\n","  🧪 Batch 10/55 — Loss: 0.6889 — Time: 0.89s\n","  🧪 Batch 11/55 — Loss: 0.7555 — Time: 0.88s\n","  🧪 Batch 12/55 — Loss: 0.6748 — Time: 0.87s\n","  🧪 Batch 13/55 — Loss: 0.7298 — Time: 0.87s\n","  🧪 Batch 14/55 — Loss: 0.8099 — Time: 0.87s\n","  🧪 Batch 15/55 — Loss: 0.7140 — Time: 0.88s\n","  🧪 Batch 16/55 — Loss: 0.7000 — Time: 0.88s\n","  🧪 Batch 17/55 — Loss: 0.6423 — Time: 0.87s\n","  🧪 Batch 18/55 — Loss: 0.6630 — Time: 0.87s\n","  🧪 Batch 19/55 — Loss: 0.6707 — Time: 0.87s\n","  🧪 Batch 20/55 — Loss: 0.6222 — Time: 0.88s\n","  🧪 Batch 21/55 — Loss: 0.6637 — Time: 0.88s\n","  🧪 Batch 22/55 — Loss: 0.7461 — Time: 0.87s\n","  🧪 Batch 23/55 — Loss: 0.6332 — Time: 0.87s\n","  🧪 Batch 24/55 — Loss: 0.6507 — Time: 0.89s\n","  🧪 Batch 25/55 — Loss: 0.6169 — Time: 0.88s\n","  🧪 Batch 26/55 — Loss: 0.6668 — Time: 0.88s\n","  🧪 Batch 27/55 — Loss: 0.6087 — Time: 0.87s\n","  🧪 Batch 28/55 — Loss: 0.6422 — Time: 0.89s\n","  🧪 Batch 29/55 — Loss: 0.7939 — Time: 0.88s\n","  🧪 Batch 30/55 — Loss: 0.6169 — Time: 0.88s\n","  🧪 Batch 31/55 — Loss: 0.6376 — Time: 0.87s\n","  🧪 Batch 32/55 — Loss: 0.5779 — Time: 0.89s\n","  🧪 Batch 33/55 — Loss: 0.6426 — Time: 0.89s\n","  🧪 Batch 34/55 — Loss: 0.6750 — Time: 0.89s\n","  🧪 Batch 35/55 — Loss: 0.6103 — Time: 0.88s\n","  🧪 Batch 36/55 — Loss: 0.6301 — Time: 0.89s\n","  🧪 Batch 37/55 — Loss: 0.5704 — Time: 0.89s\n","  🧪 Batch 38/55 — Loss: 0.7090 — Time: 0.89s\n","  🧪 Batch 39/55 — Loss: 0.7223 — Time: 0.88s\n","  🧪 Batch 40/55 — Loss: 0.6148 — Time: 0.89s\n","  🧪 Batch 41/55 — Loss: 0.6479 — Time: 0.89s\n","  🧪 Batch 42/55 — Loss: 0.6180 — Time: 0.89s\n","  🧪 Batch 43/55 — Loss: 0.5152 — Time: 0.89s\n","  🧪 Batch 44/55 — Loss: 0.6457 — Time: 0.89s\n","  🧪 Batch 45/55 — Loss: 0.5762 — Time: 0.89s\n","  🧪 Batch 46/55 — Loss: 0.5511 — Time: 0.89s\n","  🧪 Batch 47/55 — Loss: 0.6398 — Time: 0.89s\n","  🧪 Batch 48/55 — Loss: 0.6141 — Time: 0.89s\n","  🧪 Batch 49/55 — Loss: 0.5251 — Time: 0.89s\n","  🧪 Batch 50/55 — Loss: 0.5660 — Time: 0.89s\n","  🧪 Batch 51/55 — Loss: 0.5574 — Time: 0.89s\n","  🧪 Batch 52/55 — Loss: 0.5700 — Time: 0.89s\n","  🧪 Batch 53/55 — Loss: 0.5184 — Time: 0.89s\n","  🧪 Batch 54/55 — Loss: 0.6288 — Time: 0.89s\n","  🧪 Batch 55/55 — Loss: 0.5971 — Time: 0.65s\n","🕒 Epoch Time: 51.66s | Avg Batch Time: 0.88s\n","📊 Train Loss: 0.6578 | Val Loss: 0.6254 | IoU: 0.6776 | Dice: 0.8220 | Jaccard: 0.6999 | Precision: 0.7430 | Recall: 0.9236 | Accuracy: 0.9523\n","\n","Epoch 3/100\n","  🧪 Batch 1/55 — Loss: 0.5532 — Time: 0.88s\n","  🧪 Batch 2/55 — Loss: 0.5425 — Time: 0.89s\n","  🧪 Batch 3/55 — Loss: 0.5355 — Time: 0.90s\n","  🧪 Batch 4/55 — Loss: 0.4667 — Time: 0.89s\n","  🧪 Batch 5/55 — Loss: 0.6031 — Time: 0.89s\n","  🧪 Batch 6/55 — Loss: 0.6132 — Time: 0.89s\n","  🧪 Batch 7/55 — Loss: 0.4801 — Time: 0.89s\n","  🧪 Batch 8/55 — Loss: 0.4950 — Time: 0.88s\n","  🧪 Batch 9/55 — Loss: 0.5375 — Time: 0.89s\n","  🧪 Batch 10/55 — Loss: 0.5614 — Time: 0.89s\n","  🧪 Batch 11/55 — Loss: 0.5461 — Time: 0.89s\n","  🧪 Batch 12/55 — Loss: 0.5039 — Time: 0.89s\n","  🧪 Batch 13/55 — Loss: 0.5729 — Time: 0.89s\n","  🧪 Batch 14/55 — Loss: 0.4732 — Time: 0.89s\n","  🧪 Batch 15/55 — Loss: 0.5268 — Time: 0.89s\n","  🧪 Batch 16/55 — Loss: 0.4450 — Time: 0.89s\n","  🧪 Batch 17/55 — Loss: 0.5279 — Time: 0.89s\n","  🧪 Batch 18/55 — Loss: 0.5990 — Time: 0.89s\n","  🧪 Batch 19/55 — Loss: 0.5299 — Time: 0.89s\n","  🧪 Batch 20/55 — Loss: 0.6013 — Time: 0.89s\n","  🧪 Batch 21/55 — Loss: 0.4755 — Time: 0.89s\n","  🧪 Batch 22/55 — Loss: 0.5116 — Time: 0.89s\n","  🧪 Batch 23/55 — Loss: 0.5135 — Time: 0.89s\n","  🧪 Batch 24/55 — Loss: 0.4581 — Time: 0.89s\n","  🧪 Batch 25/55 — Loss: 0.4960 — Time: 0.89s\n","  🧪 Batch 26/55 — Loss: 0.5715 — Time: 0.89s\n","  🧪 Batch 27/55 — Loss: 0.4927 — Time: 0.89s\n","  🧪 Batch 28/55 — Loss: 0.4584 — Time: 0.89s\n","  🧪 Batch 29/55 — Loss: 0.4570 — Time: 0.89s\n","  🧪 Batch 30/55 — Loss: 0.5422 — Time: 0.89s\n","  🧪 Batch 31/55 — Loss: 0.6626 — Time: 0.88s\n","  🧪 Batch 32/55 — Loss: 0.5263 — Time: 0.88s\n","  🧪 Batch 33/55 — Loss: 0.4172 — Time: 0.89s\n","  🧪 Batch 34/55 — Loss: 0.4439 — Time: 0.89s\n","  🧪 Batch 35/55 — Loss: 0.4985 — Time: 0.89s\n","  🧪 Batch 36/55 — Loss: 0.5458 — Time: 0.89s\n","  🧪 Batch 37/55 — Loss: 0.4807 — Time: 0.88s\n","  🧪 Batch 38/55 — Loss: 0.4310 — Time: 0.89s\n","  🧪 Batch 39/55 — Loss: 0.5200 — Time: 0.88s\n","  🧪 Batch 40/55 — Loss: 0.4532 — Time: 0.88s\n","  🧪 Batch 41/55 — Loss: 0.4341 — Time: 0.88s\n","  🧪 Batch 42/55 — Loss: 0.4146 — Time: 0.89s\n","  🧪 Batch 43/55 — Loss: 0.4993 — Time: 0.87s\n","  🧪 Batch 44/55 — Loss: 0.3888 — Time: 0.89s\n","  🧪 Batch 45/55 — Loss: 0.4065 — Time: 0.89s\n","  🧪 Batch 46/55 — Loss: 0.4873 — Time: 0.89s\n","  🧪 Batch 47/55 — Loss: 0.4711 — Time: 0.87s\n","  🧪 Batch 48/55 — Loss: 0.5391 — Time: 0.89s\n","  🧪 Batch 49/55 — Loss: 0.3521 — Time: 0.89s\n","  🧪 Batch 50/55 — Loss: 0.4329 — Time: 0.88s\n","  🧪 Batch 51/55 — Loss: 0.4775 — Time: 0.87s\n","  🧪 Batch 52/55 — Loss: 0.3988 — Time: 0.87s\n","  🧪 Batch 53/55 — Loss: 0.4418 — Time: 0.89s\n","  🧪 Batch 54/55 — Loss: 0.4267 — Time: 0.89s\n","  🧪 Batch 55/55 — Loss: 0.4100 — Time: 0.65s\n","🕒 Epoch Time: 51.91s | Avg Batch Time: 0.88s\n","📊 Train Loss: 0.4955 | Val Loss: 0.5081 | IoU: 0.7352 | Dice: 0.8674 | Jaccard: 0.7674 | Precision: 0.8556 | Recall: 0.8817 | Accuracy: 0.9685\n","\n","Epoch 4/100\n","  🧪 Batch 1/55 — Loss: 0.3634 — Time: 0.87s\n","  🧪 Batch 2/55 — Loss: 0.4153 — Time: 0.88s\n","  🧪 Batch 3/55 — Loss: 0.4217 — Time: 0.88s\n","  🧪 Batch 4/55 — Loss: 0.4349 — Time: 0.89s\n","  🧪 Batch 5/55 — Loss: 0.4996 — Time: 0.89s\n","  🧪 Batch 6/55 — Loss: 0.4861 — Time: 0.88s\n","  🧪 Batch 7/55 — Loss: 0.4341 — Time: 0.89s\n","  🧪 Batch 8/55 — Loss: 0.4191 — Time: 0.88s\n","  🧪 Batch 9/55 — Loss: 0.3747 — Time: 0.88s\n","  🧪 Batch 10/55 — Loss: 0.4058 — Time: 0.89s\n","  🧪 Batch 11/55 — Loss: 0.3350 — Time: 0.87s\n","  🧪 Batch 12/55 — Loss: 0.3966 — Time: 0.89s\n","  🧪 Batch 13/55 — Loss: 0.3977 — Time: 0.89s\n","  🧪 Batch 14/55 — Loss: 0.3428 — Time: 0.89s\n","  🧪 Batch 15/55 — Loss: 0.3692 — Time: 0.89s\n","  🧪 Batch 16/55 — Loss: 0.4596 — Time: 0.88s\n","  🧪 Batch 17/55 — Loss: 0.4211 — Time: 0.89s\n","  🧪 Batch 18/55 — Loss: 0.3873 — Time: 0.89s\n","  🧪 Batch 19/55 — Loss: 0.4423 — Time: 0.88s\n","  🧪 Batch 20/55 — Loss: 0.3362 — Time: 0.88s\n","  🧪 Batch 21/55 — Loss: 0.3824 — Time: 0.89s\n","  🧪 Batch 22/55 — Loss: 0.4101 — Time: 0.89s\n","  🧪 Batch 23/55 — Loss: 0.4543 — Time: 0.89s\n","  🧪 Batch 24/55 — Loss: 0.3566 — Time: 0.89s\n","  🧪 Batch 25/55 — Loss: 0.4133 — Time: 0.89s\n","  🧪 Batch 26/55 — Loss: 0.3206 — Time: 0.89s\n","  🧪 Batch 27/55 — Loss: 0.3503 — Time: 0.89s\n","  🧪 Batch 28/55 — Loss: 0.3634 — Time: 0.89s\n","  🧪 Batch 29/55 — Loss: 0.4518 — Time: 0.89s\n","  🧪 Batch 30/55 — Loss: 0.3271 — Time: 0.89s\n","  🧪 Batch 31/55 — Loss: 0.3590 — Time: 0.89s\n","  🧪 Batch 32/55 — Loss: 0.3435 — Time: 0.89s\n","  🧪 Batch 33/55 — Loss: 0.3064 — Time: 0.89s\n","  🧪 Batch 34/55 — Loss: 0.3956 — Time: 0.89s\n","  🧪 Batch 35/55 — Loss: 0.3535 — Time: 0.89s\n","  🧪 Batch 36/55 — Loss: 0.4143 — Time: 0.89s\n","  🧪 Batch 37/55 — Loss: 0.3306 — Time: 0.89s\n","  🧪 Batch 38/55 — Loss: 0.3925 — Time: 0.89s\n","  🧪 Batch 39/55 — Loss: 0.3564 — Time: 0.89s\n","  🧪 Batch 40/55 — Loss: 0.3045 — Time: 0.88s\n","  🧪 Batch 41/55 — Loss: 0.3522 — Time: 0.88s\n","  🧪 Batch 42/55 — Loss: 0.3362 — Time: 0.89s\n","  🧪 Batch 43/55 — Loss: 0.4041 — Time: 0.89s\n","  🧪 Batch 44/55 — Loss: 0.3459 — Time: 0.89s\n","  🧪 Batch 45/55 — Loss: 0.3392 — Time: 0.89s\n","  🧪 Batch 46/55 — Loss: 0.4103 — Time: 0.89s\n","  🧪 Batch 47/55 — Loss: 0.3224 — Time: 0.89s\n","  🧪 Batch 48/55 — Loss: 0.3581 — Time: 0.89s\n","  🧪 Batch 49/55 — Loss: 0.3141 — Time: 0.89s\n","  🧪 Batch 50/55 — Loss: 0.3775 — Time: 0.89s\n","  🧪 Batch 51/55 — Loss: 0.3195 — Time: 0.89s\n","  🧪 Batch 52/55 — Loss: 0.3361 — Time: 0.89s\n","  🧪 Batch 53/55 — Loss: 0.3133 — Time: 0.89s\n","  🧪 Batch 54/55 — Loss: 0.3149 — Time: 0.89s\n","  🧪 Batch 55/55 — Loss: 0.3496 — Time: 0.65s\n","🕒 Epoch Time: 52.24s | Avg Batch Time: 0.88s\n","📊 Train Loss: 0.3768 | Val Loss: 0.4424 | IoU: 0.7504 | Dice: 0.8763 | Jaccard: 0.7821 | Precision: 0.8676 | Recall: 0.8875 | Accuracy: 0.9696\n","\n","Epoch 5/100\n","  🧪 Batch 1/55 — Loss: 0.3267 — Time: 0.88s\n","  🧪 Batch 2/55 — Loss: 0.3133 — Time: 0.88s\n","  🧪 Batch 3/55 — Loss: 0.3154 — Time: 0.89s\n","  🧪 Batch 4/55 — Loss: 0.3373 — Time: 0.89s\n","  🧪 Batch 5/55 — Loss: 0.2769 — Time: 0.89s\n","  🧪 Batch 6/55 — Loss: 0.4590 — Time: 0.88s\n","  🧪 Batch 7/55 — Loss: 0.2731 — Time: 0.88s\n","  🧪 Batch 8/55 — Loss: 0.3225 — Time: 0.89s\n","  🧪 Batch 9/55 — Loss: 0.2858 — Time: 0.89s\n","  🧪 Batch 10/55 — Loss: 0.2975 — Time: 0.89s\n","  🧪 Batch 11/55 — Loss: 0.3498 — Time: 0.88s\n","  🧪 Batch 12/55 — Loss: 0.2878 — Time: 0.87s\n","  🧪 Batch 13/55 — Loss: 0.3298 — Time: 0.89s\n","  🧪 Batch 14/55 — Loss: 0.2702 — Time: 0.89s\n","  🧪 Batch 15/55 — Loss: 0.2645 — Time: 0.89s\n","  🧪 Batch 16/55 — Loss: 0.2862 — Time: 0.88s\n","  🧪 Batch 17/55 — Loss: 0.3088 — Time: 0.88s\n","  🧪 Batch 18/55 — Loss: 0.3493 — Time: 0.89s\n","  🧪 Batch 19/55 — Loss: 0.2437 — Time: 0.89s\n","  🧪 Batch 20/55 — Loss: 0.3505 — Time: 0.89s\n","  🧪 Batch 21/55 — Loss: 0.2924 — Time: 0.89s\n","  🧪 Batch 22/55 — Loss: 0.2763 — Time: 0.89s\n","  🧪 Batch 23/55 — Loss: 0.4108 — Time: 0.88s\n","  🧪 Batch 24/55 — Loss: 0.2824 — Time: 0.89s\n","  🧪 Batch 25/55 — Loss: 0.3058 — Time: 0.89s\n","  🧪 Batch 26/55 — Loss: 0.3665 — Time: 0.89s\n","  🧪 Batch 27/55 — Loss: 0.2859 — Time: 0.88s\n","  🧪 Batch 28/55 — Loss: 0.2772 — Time: 0.89s\n","  🧪 Batch 29/55 — Loss: 0.3265 — Time: 0.89s\n","  🧪 Batch 30/55 — Loss: 0.3474 — Time: 0.88s\n","  🧪 Batch 31/55 — Loss: 0.3172 — Time: 0.88s\n","  🧪 Batch 32/55 — Loss: 0.3833 — Time: 0.88s\n","  🧪 Batch 33/55 — Loss: 0.3145 — Time: 0.87s\n","  🧪 Batch 34/55 — Loss: 0.2666 — Time: 0.89s\n","  🧪 Batch 35/55 — Loss: 0.3014 — Time: 0.89s\n","  🧪 Batch 36/55 — Loss: 0.2599 — Time: 0.88s\n","  🧪 Batch 37/55 — Loss: 0.3629 — Time: 0.88s\n","  🧪 Batch 38/55 — Loss: 0.2431 — Time: 0.89s\n","  🧪 Batch 39/55 — Loss: 0.2676 — Time: 0.89s\n","  🧪 Batch 40/55 — Loss: 0.2584 — Time: 0.89s\n","  🧪 Batch 41/55 — Loss: 0.2855 — Time: 0.89s\n","  🧪 Batch 42/55 — Loss: 0.3029 — Time: 0.89s\n","  🧪 Batch 43/55 — Loss: 0.3179 — Time: 0.89s\n","  🧪 Batch 44/55 — Loss: 0.4525 — Time: 0.87s\n","  🧪 Batch 45/55 — Loss: 0.3311 — Time: 0.88s\n","  🧪 Batch 46/55 — Loss: 0.3331 — Time: 0.89s\n","  🧪 Batch 47/55 — Loss: 0.2805 — Time: 0.88s\n","  🧪 Batch 48/55 — Loss: 0.3330 — Time: 0.88s\n","  🧪 Batch 49/55 — Loss: 0.2553 — Time: 0.89s\n","  🧪 Batch 50/55 — Loss: 0.2708 — Time: 0.89s\n","  🧪 Batch 51/55 — Loss: 0.2673 — Time: 0.89s\n","  🧪 Batch 52/55 — Loss: 0.2569 — Time: 0.87s\n","  🧪 Batch 53/55 — Loss: 0.3227 — Time: 0.89s\n","  🧪 Batch 54/55 — Loss: 0.2951 — Time: 0.89s\n","  🧪 Batch 55/55 — Loss: 0.2526 — Time: 0.65s\n","🕒 Epoch Time: 52.65s | Avg Batch Time: 0.88s\n","📊 Train Loss: 0.3082 | Val Loss: 0.3991 | IoU: 0.7326 | Dice: 0.8677 | Jaccard: 0.7681 | Precision: 0.8421 | Recall: 0.8960 | Accuracy: 0.9676\n","\n","Epoch 6/100\n","  🧪 Batch 1/55 — Loss: 0.2804 — Time: 0.86s\n","  🧪 Batch 2/55 — Loss: 0.2326 — Time: 0.89s\n","  🧪 Batch 3/55 — Loss: 0.2575 — Time: 0.88s\n","  🧪 Batch 4/55 — Loss: 0.2430 — Time: 0.89s\n","  🧪 Batch 5/55 — Loss: 0.2640 — Time: 0.88s\n","  🧪 Batch 6/55 — Loss: 0.2328 — Time: 0.88s\n","  🧪 Batch 7/55 — Loss: 0.3017 — Time: 0.88s\n","  🧪 Batch 8/55 — Loss: 0.2182 — Time: 0.88s\n","  🧪 Batch 9/55 — Loss: 0.2874 — Time: 0.89s\n","  🧪 Batch 10/55 — Loss: 0.2888 — Time: 0.89s\n","  🧪 Batch 11/55 — Loss: 0.2199 — Time: 0.88s\n","  🧪 Batch 12/55 — Loss: 0.2434 — Time: 0.89s\n","  🧪 Batch 13/55 — Loss: 0.2645 — Time: 0.88s\n","  🧪 Batch 14/55 — Loss: 0.2905 — Time: 0.88s\n","  🧪 Batch 15/55 — Loss: 0.2668 — Time: 0.89s\n","  🧪 Batch 16/55 — Loss: 0.2630 — Time: 0.88s\n","  🧪 Batch 17/55 — Loss: 0.2208 — Time: 0.89s\n","  🧪 Batch 18/55 — Loss: 0.2337 — Time: 0.89s\n","  🧪 Batch 19/55 — Loss: 0.3003 — Time: 0.88s\n","  🧪 Batch 20/55 — Loss: 0.2354 — Time: 0.89s\n","  🧪 Batch 21/55 — Loss: 0.2313 — Time: 0.89s\n","  🧪 Batch 22/55 — Loss: 0.2485 — Time: 0.89s\n","  🧪 Batch 23/55 — Loss: 0.2659 — Time: 0.89s\n","  🧪 Batch 24/55 — Loss: 0.2558 — Time: 0.89s\n","  🧪 Batch 25/55 — Loss: 0.2464 — Time: 0.88s\n","  🧪 Batch 26/55 — Loss: 0.2286 — Time: 0.88s\n","  🧪 Batch 27/55 — Loss: 0.2232 — Time: 0.89s\n","  🧪 Batch 28/55 — Loss: 0.2269 — Time: 0.88s\n","  🧪 Batch 29/55 — Loss: 0.2702 — Time: 0.89s\n","  🧪 Batch 30/55 — Loss: 0.3002 — Time: 0.89s\n","  🧪 Batch 31/55 — Loss: 0.2951 — Time: 0.89s\n","  🧪 Batch 32/55 — Loss: 0.2187 — Time: 0.89s\n","  🧪 Batch 33/55 — Loss: 0.2159 — Time: 0.89s\n","  🧪 Batch 34/55 — Loss: 0.2351 — Time: 0.89s\n","  🧪 Batch 35/55 — Loss: 0.2295 — Time: 0.89s\n","  🧪 Batch 36/55 — Loss: 0.3610 — Time: 0.89s\n","  🧪 Batch 37/55 — Loss: 0.1987 — Time: 0.89s\n","  🧪 Batch 38/55 — Loss: 0.3064 — Time: 0.89s\n","  🧪 Batch 39/55 — Loss: 0.2492 — Time: 0.89s\n","  🧪 Batch 40/55 — Loss: 0.2556 — Time: 0.89s\n","  🧪 Batch 41/55 — Loss: 0.2824 — Time: 0.90s\n","  🧪 Batch 42/55 — Loss: 0.2393 — Time: 0.88s\n","  🧪 Batch 43/55 — Loss: 0.2226 — Time: 0.89s\n","  🧪 Batch 44/55 — Loss: 0.2721 — Time: 0.89s\n","  🧪 Batch 45/55 — Loss: 0.2306 — Time: 0.89s\n","  🧪 Batch 46/55 — Loss: 0.2894 — Time: 0.89s\n","  🧪 Batch 47/55 — Loss: 0.2696 — Time: 0.89s\n","  🧪 Batch 48/55 — Loss: 0.2497 — Time: 0.89s\n","  🧪 Batch 49/55 — Loss: 0.2736 — Time: 0.89s\n","  🧪 Batch 50/55 — Loss: 0.2587 — Time: 0.89s\n","  🧪 Batch 51/55 — Loss: 0.2343 — Time: 0.89s\n","  🧪 Batch 52/55 — Loss: 0.2260 — Time: 0.89s\n","  🧪 Batch 53/55 — Loss: 0.2508 — Time: 0.89s\n","  🧪 Batch 54/55 — Loss: 0.3162 — Time: 0.89s\n","  🧪 Batch 55/55 — Loss: 0.2124 — Time: 0.65s\n","🕒 Epoch Time: 53.16s | Avg Batch Time: 0.88s\n","📊 Train Loss: 0.2552 | Val Loss: 0.3693 | IoU: 0.7593 | Dice: 0.8746 | Jaccard: 0.7807 | Precision: 0.9054 | Recall: 0.8479 | Accuracy: 0.9703\n","\n","Epoch 7/100\n","  🧪 Batch 1/55 — Loss: 0.2173 — Time: 0.87s\n","  🧪 Batch 2/55 — Loss: 0.2207 — Time: 0.89s\n","  🧪 Batch 3/55 — Loss: 0.2338 — Time: 0.89s\n","  🧪 Batch 4/55 — Loss: 0.2117 — Time: 0.89s\n","  🧪 Batch 5/55 — Loss: 0.1798 — Time: 0.89s\n","  🧪 Batch 6/55 — Loss: 0.2792 — Time: 0.88s\n","  🧪 Batch 7/55 — Loss: 0.2235 — Time: 0.89s\n","  🧪 Batch 8/55 — Loss: 0.2510 — Time: 0.89s\n","  🧪 Batch 9/55 — Loss: 0.2602 — Time: 0.89s\n","  🧪 Batch 10/55 — Loss: 0.2767 — Time: 0.87s\n","  🧪 Batch 11/55 — Loss: 0.2106 — Time: 0.88s\n","  🧪 Batch 12/55 — Loss: 0.2153 — Time: 0.89s\n","  🧪 Batch 13/55 — Loss: 0.2292 — Time: 0.88s\n","  🧪 Batch 14/55 — Loss: 0.2034 — Time: 0.87s\n","  🧪 Batch 15/55 — Loss: 0.2241 — Time: 0.89s\n","  🧪 Batch 16/55 — Loss: 0.2211 — Time: 0.89s\n","  🧪 Batch 17/55 — Loss: 0.1736 — Time: 0.88s\n","  🧪 Batch 18/55 — Loss: 0.2168 — Time: 0.88s\n","  🧪 Batch 19/55 — Loss: 0.2140 — Time: 0.87s\n","  🧪 Batch 20/55 — Loss: 0.2082 — Time: 0.89s\n","  🧪 Batch 21/55 — Loss: 0.1737 — Time: 0.89s\n","  🧪 Batch 22/55 — Loss: 0.2279 — Time: 0.89s\n","  🧪 Batch 23/55 — Loss: 0.2203 — Time: 0.88s\n","  🧪 Batch 24/55 — Loss: 0.1859 — Time: 0.88s\n","  🧪 Batch 25/55 — Loss: 0.2606 — Time: 0.88s\n","  🧪 Batch 26/55 — Loss: 0.2012 — Time: 0.88s\n","  🧪 Batch 27/55 — Loss: 0.2441 — Time: 0.89s\n","  🧪 Batch 28/55 — Loss: 0.2156 — Time: 0.88s\n","  🧪 Batch 29/55 — Loss: 0.2066 — Time: 0.89s\n","  🧪 Batch 30/55 — Loss: 0.2320 — Time: 0.88s\n","  🧪 Batch 31/55 — Loss: 0.1929 — Time: 0.88s\n","  🧪 Batch 32/55 — Loss: 0.2097 — Time: 0.88s\n","  🧪 Batch 33/55 — Loss: 0.2022 — Time: 0.89s\n","  🧪 Batch 34/55 — Loss: 0.2125 — Time: 0.88s\n","  🧪 Batch 35/55 — Loss: 0.1988 — Time: 0.88s\n","  🧪 Batch 36/55 — Loss: 0.1811 — Time: 0.88s\n","  🧪 Batch 37/55 — Loss: 0.2490 — Time: 0.88s\n","  🧪 Batch 38/55 — Loss: 0.2606 — Time: 0.88s\n","  🧪 Batch 39/55 — Loss: 0.2898 — Time: 0.88s\n","  🧪 Batch 40/55 — Loss: 0.1978 — Time: 0.89s\n","  🧪 Batch 41/55 — Loss: 0.2240 — Time: 0.89s\n","  🧪 Batch 42/55 — Loss: 0.1980 — Time: 0.88s\n","  🧪 Batch 43/55 — Loss: 0.1820 — Time: 0.88s\n","  🧪 Batch 44/55 — Loss: 0.2200 — Time: 0.88s\n","  🧪 Batch 45/55 — Loss: 0.1980 — Time: 0.88s\n","  🧪 Batch 46/55 — Loss: 0.1823 — Time: 0.89s\n","  🧪 Batch 47/55 — Loss: 0.1810 — Time: 0.89s\n","  🧪 Batch 48/55 — Loss: 0.2013 — Time: 0.88s\n","  🧪 Batch 49/55 — Loss: 0.2096 — Time: 0.88s\n","  🧪 Batch 50/55 — Loss: 0.1833 — Time: 0.88s\n","  🧪 Batch 51/55 — Loss: 0.1802 — Time: 0.88s\n","  🧪 Batch 52/55 — Loss: 0.1837 — Time: 0.88s\n","  🧪 Batch 53/55 — Loss: 0.2225 — Time: 0.88s\n","  🧪 Batch 54/55 — Loss: 0.1806 — Time: 0.88s\n","  🧪 Batch 55/55 — Loss: 0.1648 — Time: 0.65s\n","🕒 Epoch Time: 52.35s | Avg Batch Time: 0.88s\n","📊 Train Loss: 0.2135 | Val Loss: 0.3095 | IoU: 0.7646 | Dice: 0.8949 | Jaccard: 0.8115 | Precision: 0.9122 | Recall: 0.8796 | Accuracy: 0.9748\n","\n","Epoch 8/100\n","  🧪 Batch 1/55 — Loss: 0.1613 — Time: 0.87s\n","  🧪 Batch 2/55 — Loss: 0.1921 — Time: 0.87s\n","  🧪 Batch 3/55 — Loss: 0.2489 — Time: 0.89s\n","  🧪 Batch 4/55 — Loss: 0.1602 — Time: 0.89s\n","  🧪 Batch 5/55 — Loss: 0.1666 — Time: 0.87s\n","  🧪 Batch 6/55 — Loss: 0.2384 — Time: 0.89s\n","  🧪 Batch 7/55 — Loss: 0.1710 — Time: 0.88s\n","  🧪 Batch 8/55 — Loss: 0.1671 — Time: 0.89s\n","  🧪 Batch 9/55 — Loss: 0.2851 — Time: 0.89s\n","  🧪 Batch 10/55 — Loss: 0.1652 — Time: 0.89s\n","  🧪 Batch 11/55 — Loss: 0.1697 — Time: 0.88s\n","  🧪 Batch 12/55 — Loss: 0.1979 — Time: 0.89s\n","  🧪 Batch 13/55 — Loss: 0.1802 — Time: 0.89s\n","  🧪 Batch 14/55 — Loss: 0.1702 — Time: 0.88s\n","  🧪 Batch 15/55 — Loss: 0.1661 — Time: 0.89s\n","  🧪 Batch 16/55 — Loss: 0.1797 — Time: 0.88s\n","  🧪 Batch 17/55 — Loss: 0.1585 — Time: 0.88s\n","  🧪 Batch 18/55 — Loss: 0.1481 — Time: 0.89s\n","  🧪 Batch 19/55 — Loss: 0.2163 — Time: 0.89s\n","  🧪 Batch 20/55 — Loss: 0.1405 — Time: 0.89s\n","  🧪 Batch 21/55 — Loss: 0.1681 — Time: 0.89s\n","  🧪 Batch 22/55 — Loss: 0.1957 — Time: 0.88s\n","  🧪 Batch 23/55 — Loss: 0.1696 — Time: 0.89s\n","  🧪 Batch 24/55 — Loss: 0.1843 — Time: 0.89s\n","  🧪 Batch 25/55 — Loss: 0.1590 — Time: 0.89s\n","  🧪 Batch 26/55 — Loss: 0.1726 — Time: 0.89s\n","  🧪 Batch 27/55 — Loss: 0.2257 — Time: 0.88s\n","  🧪 Batch 28/55 — Loss: 0.1600 — Time: 0.89s\n","  🧪 Batch 29/55 — Loss: 0.1872 — Time: 0.88s\n","  🧪 Batch 30/55 — Loss: 0.1716 — Time: 0.89s\n","  🧪 Batch 31/55 — Loss: 0.1926 — Time: 0.89s\n","  🧪 Batch 32/55 — Loss: 0.1881 — Time: 0.89s\n","  🧪 Batch 33/55 — Loss: 0.1607 — Time: 0.87s\n","  🧪 Batch 34/55 — Loss: 0.2427 — Time: 0.88s\n","  🧪 Batch 35/55 — Loss: 0.1657 — Time: 0.88s\n","  🧪 Batch 36/55 — Loss: 0.1770 — Time: 0.89s\n","  🧪 Batch 37/55 — Loss: 0.1763 — Time: 0.89s\n","  🧪 Batch 38/55 — Loss: 0.1752 — Time: 0.88s\n","  🧪 Batch 39/55 — Loss: 0.1810 — Time: 0.89s\n","  🧪 Batch 40/55 — Loss: 0.2054 — Time: 0.89s\n","  🧪 Batch 41/55 — Loss: 0.2151 — Time: 0.89s\n","  🧪 Batch 42/55 — Loss: 0.1897 — Time: 0.88s\n","  🧪 Batch 43/55 — Loss: 0.1414 — Time: 0.88s\n","  🧪 Batch 44/55 — Loss: 0.2342 — Time: 0.89s\n","  🧪 Batch 45/55 — Loss: 0.1919 — Time: 0.89s\n","  🧪 Batch 46/55 — Loss: 0.2495 — Time: 0.89s\n","  🧪 Batch 47/55 — Loss: 0.1684 — Time: 0.89s\n","  🧪 Batch 48/55 — Loss: 0.1782 — Time: 0.89s\n","  🧪 Batch 49/55 — Loss: 0.1693 — Time: 0.89s\n","  🧪 Batch 50/55 — Loss: 0.1824 — Time: 0.88s\n","  🧪 Batch 51/55 — Loss: 0.1640 — Time: 0.88s\n","  🧪 Batch 52/55 — Loss: 0.2044 — Time: 0.89s\n","  🧪 Batch 53/55 — Loss: 0.1483 — Time: 0.89s\n","  🧪 Batch 54/55 — Loss: 0.2114 — Time: 0.89s\n","  🧪 Batch 55/55 — Loss: 0.1610 — Time: 0.65s\n","🕒 Epoch Time: 52.21s | Avg Batch Time: 0.88s\n","📊 Train Loss: 0.1846 | Val Loss: 0.3167 | IoU: 0.7610 | Dice: 0.8782 | Jaccard: 0.7856 | Precision: 0.8594 | Recall: 0.9002 | Accuracy: 0.9696\n","\n","Epoch 9/100\n","  🧪 Batch 1/55 — Loss: 0.1581 — Time: 0.89s\n","  🧪 Batch 2/55 — Loss: 0.1468 — Time: 0.88s\n","  🧪 Batch 3/55 — Loss: 0.2539 — Time: 0.89s\n","  🧪 Batch 4/55 — Loss: 0.1578 — Time: 0.89s\n","  🧪 Batch 5/55 — Loss: 0.1890 — Time: 0.89s\n","  🧪 Batch 6/55 — Loss: 0.1715 — Time: 0.89s\n","  🧪 Batch 7/55 — Loss: 0.2030 — Time: 0.89s\n","  🧪 Batch 8/55 — Loss: 0.1515 — Time: 0.89s\n","  🧪 Batch 9/55 — Loss: 0.1620 — Time: 0.89s\n","  🧪 Batch 10/55 — Loss: 0.1617 — Time: 0.89s\n","  🧪 Batch 11/55 — Loss: 0.1574 — Time: 0.89s\n","  🧪 Batch 12/55 — Loss: 0.1655 — Time: 0.89s\n","  🧪 Batch 13/55 — Loss: 0.1801 — Time: 0.89s\n","  🧪 Batch 14/55 — Loss: 0.1829 — Time: 0.89s\n","  🧪 Batch 15/55 — Loss: 0.1868 — Time: 0.89s\n","  🧪 Batch 16/55 — Loss: 0.1486 — Time: 0.89s\n","  🧪 Batch 17/55 — Loss: 0.2383 — Time: 0.89s\n","  🧪 Batch 18/55 — Loss: 0.1929 — Time: 0.89s\n","  🧪 Batch 19/55 — Loss: 0.1446 — Time: 0.89s\n","  🧪 Batch 20/55 — Loss: 0.1684 — Time: 0.89s\n","  🧪 Batch 21/55 — Loss: 0.1404 — Time: 0.89s\n","  🧪 Batch 22/55 — Loss: 0.1779 — Time: 0.89s\n","  🧪 Batch 23/55 — Loss: 0.2399 — Time: 0.89s\n","  🧪 Batch 24/55 — Loss: 0.1680 — Time: 0.89s\n","  🧪 Batch 25/55 — Loss: 0.1651 — Time: 0.89s\n","  🧪 Batch 26/55 — Loss: 0.1990 — Time: 0.89s\n","  🧪 Batch 27/55 — Loss: 0.1695 — Time: 0.89s\n","  🧪 Batch 28/55 — Loss: 0.1617 — Time: 0.89s\n","  🧪 Batch 29/55 — Loss: 0.1681 — Time: 0.89s\n","  🧪 Batch 30/55 — Loss: 0.1688 — Time: 0.89s\n","  🧪 Batch 31/55 — Loss: 0.1633 — Time: 0.89s\n","  🧪 Batch 32/55 — Loss: 0.1566 — Time: 0.89s\n","  🧪 Batch 33/55 — Loss: 0.1528 — Time: 0.88s\n","  🧪 Batch 34/55 — Loss: 0.1629 — Time: 0.89s\n","  🧪 Batch 35/55 — Loss: 0.1863 — Time: 0.89s\n","  🧪 Batch 36/55 — Loss: 0.1489 — Time: 0.89s\n","  🧪 Batch 37/55 — Loss: 0.1642 — Time: 0.89s\n","  🧪 Batch 38/55 — Loss: 0.1376 — Time: 0.89s\n","  🧪 Batch 39/55 — Loss: 0.1700 — Time: 0.89s\n","  🧪 Batch 40/55 — Loss: 0.1630 — Time: 0.89s\n","  🧪 Batch 41/55 — Loss: 0.1502 — Time: 0.89s\n","  🧪 Batch 42/55 — Loss: 0.1422 — Time: 0.89s\n","  🧪 Batch 43/55 — Loss: 0.1296 — Time: 0.89s\n","  🧪 Batch 44/55 — Loss: 0.1585 — Time: 0.89s\n","  🧪 Batch 45/55 — Loss: 0.1608 — Time: 0.89s\n","  🧪 Batch 46/55 — Loss: 0.1737 — Time: 0.90s\n","  🧪 Batch 47/55 — Loss: 0.1498 — Time: 0.88s\n","  🧪 Batch 48/55 — Loss: 0.1326 — Time: 0.89s\n","  🧪 Batch 49/55 — Loss: 0.1314 — Time: 0.89s\n","  🧪 Batch 50/55 — Loss: 0.1529 — Time: 0.89s\n","  🧪 Batch 51/55 — Loss: 0.1437 — Time: 0.89s\n","  🧪 Batch 52/55 — Loss: 0.2005 — Time: 0.88s\n","  🧪 Batch 53/55 — Loss: 0.1328 — Time: 0.89s\n","  🧪 Batch 54/55 — Loss: 0.1795 — Time: 0.89s\n","  🧪 Batch 55/55 — Loss: 0.1614 — Time: 0.65s\n","🕒 Epoch Time: 52.48s | Avg Batch Time: 0.88s\n","📊 Train Loss: 0.1670 | Val Loss: 0.2764 | IoU: 0.7866 | Dice: 0.8941 | Jaccard: 0.8110 | Precision: 0.9324 | Recall: 0.8615 | Accuracy: 0.9754\n","\n","Epoch 10/100\n","  🧪 Batch 1/55 — Loss: 0.1525 — Time: 0.87s\n","  🧪 Batch 2/55 — Loss: 0.1243 — Time: 0.88s\n","  🧪 Batch 3/55 — Loss: 0.1651 — Time: 0.89s\n","  🧪 Batch 4/55 — Loss: 0.1387 — Time: 0.89s\n","  🧪 Batch 5/55 — Loss: 0.1401 — Time: 0.89s\n","  🧪 Batch 6/55 — Loss: 0.1782 — Time: 0.88s\n","  🧪 Batch 7/55 — Loss: 0.1348 — Time: 0.89s\n","  🧪 Batch 8/55 — Loss: 0.1759 — Time: 0.88s\n","  🧪 Batch 9/55 — Loss: 0.1330 — Time: 0.89s\n","  🧪 Batch 10/55 — Loss: 0.1913 — Time: 0.89s\n","  🧪 Batch 11/55 — Loss: 0.1282 — Time: 0.89s\n","  🧪 Batch 12/55 — Loss: 0.1372 — Time: 0.88s\n","  🧪 Batch 13/55 — Loss: 0.1574 — Time: 0.89s\n","  🧪 Batch 14/55 — Loss: 0.1281 — Time: 0.89s\n","  🧪 Batch 15/55 — Loss: 0.1392 — Time: 0.89s\n","  🧪 Batch 16/55 — Loss: 0.1253 — Time: 0.89s\n","  🧪 Batch 17/55 — Loss: 0.1378 — Time: 0.89s\n","  🧪 Batch 18/55 — Loss: 0.1789 — Time: 0.89s\n","  🧪 Batch 19/55 — Loss: 0.1891 — Time: 0.88s\n","  🧪 Batch 20/55 — Loss: 0.1759 — Time: 0.89s\n","  🧪 Batch 21/55 — Loss: 0.1494 — Time: 0.88s\n","  🧪 Batch 22/55 — Loss: 0.1332 — Time: 0.89s\n","  🧪 Batch 23/55 — Loss: 0.1253 — Time: 0.89s\n","  🧪 Batch 24/55 — Loss: 0.1392 — Time: 0.89s\n","  🧪 Batch 25/55 — Loss: 0.1239 — Time: 0.89s\n","  🧪 Batch 26/55 — Loss: 0.1716 — Time: 0.88s\n","  🧪 Batch 27/55 — Loss: 0.1273 — Time: 0.88s\n","  🧪 Batch 28/55 — Loss: 0.1862 — Time: 0.89s\n","  🧪 Batch 29/55 — Loss: 0.1615 — Time: 0.88s\n","  🧪 Batch 30/55 — Loss: 0.1420 — Time: 0.88s\n","  🧪 Batch 31/55 — Loss: 0.1166 — Time: 0.89s\n","  🧪 Batch 32/55 — Loss: 0.1315 — Time: 0.89s\n","  🧪 Batch 33/55 — Loss: 0.1104 — Time: 0.89s\n","  🧪 Batch 34/55 — Loss: 0.1154 — Time: 0.89s\n","  🧪 Batch 35/55 — Loss: 0.1987 — Time: 0.89s\n","  🧪 Batch 36/55 — Loss: 0.1287 — Time: 0.89s\n","  🧪 Batch 37/55 — Loss: 0.1138 — Time: 0.88s\n","  🧪 Batch 38/55 — Loss: 0.1226 — Time: 0.88s\n","  🧪 Batch 39/55 — Loss: 0.1131 — Time: 0.89s\n","  🧪 Batch 40/55 — Loss: 0.1720 — Time: 0.89s\n","  🧪 Batch 41/55 — Loss: 0.1230 — Time: 0.89s\n","  🧪 Batch 42/55 — Loss: 0.2333 — Time: 0.89s\n","  🧪 Batch 43/55 — Loss: 0.1280 — Time: 0.89s\n","  🧪 Batch 44/55 — Loss: 0.1060 — Time: 0.89s\n","  🧪 Batch 45/55 — Loss: 0.1315 — Time: 0.89s\n","  🧪 Batch 46/55 — Loss: 0.1430 — Time: 0.89s\n","  🧪 Batch 47/55 — Loss: 0.1286 — Time: 0.89s\n","  🧪 Batch 48/55 — Loss: 0.1609 — Time: 0.90s\n","  🧪 Batch 49/55 — Loss: 0.1292 — Time: 0.89s\n","  🧪 Batch 50/55 — Loss: 0.1226 — Time: 0.89s\n","  🧪 Batch 51/55 — Loss: 0.1242 — Time: 0.89s\n","  🧪 Batch 52/55 — Loss: 0.1298 — Time: 0.89s\n","  🧪 Batch 53/55 — Loss: 0.1256 — Time: 0.89s\n","  🧪 Batch 54/55 — Loss: 0.1204 — Time: 0.89s\n","  🧪 Batch 55/55 — Loss: 0.1365 — Time: 0.65s\n","🕒 Epoch Time: 51.97s | Avg Batch Time: 0.88s\n","📊 Train Loss: 0.1428 | Val Loss: 0.2690 | IoU: 0.7906 | Dice: 0.8923 | Jaccard: 0.8086 | Precision: 0.9359 | Recall: 0.8559 | Accuracy: 0.9751\n","\n","Epoch 11/100\n","  🧪 Batch 1/55 — Loss: 0.1905 — Time: 0.87s\n","  🧪 Batch 2/55 — Loss: 0.1153 — Time: 0.88s\n","  🧪 Batch 3/55 — Loss: 0.1364 — Time: 0.89s\n","  🧪 Batch 4/55 — Loss: 0.1269 — Time: 0.88s\n","  🧪 Batch 5/55 — Loss: 0.1145 — Time: 0.89s\n","  🧪 Batch 6/55 — Loss: 0.1248 — Time: 0.90s\n","  🧪 Batch 7/55 — Loss: 0.1301 — Time: 0.89s\n","  🧪 Batch 8/55 — Loss: 0.1057 — Time: 0.89s\n","  🧪 Batch 9/55 — Loss: 0.1018 — Time: 0.89s\n","  🧪 Batch 10/55 — Loss: 0.1174 — Time: 0.89s\n","  🧪 Batch 11/55 — Loss: 0.1275 — Time: 0.88s\n","  🧪 Batch 12/55 — Loss: 0.1506 — Time: 0.88s\n","  🧪 Batch 13/55 — Loss: 0.1155 — Time: 0.89s\n","  🧪 Batch 14/55 — Loss: 0.1111 — Time: 0.89s\n","  🧪 Batch 15/55 — Loss: 0.1239 — Time: 0.88s\n","  🧪 Batch 16/55 — Loss: 0.1310 — Time: 0.89s\n","  🧪 Batch 17/55 — Loss: 0.1412 — Time: 0.89s\n","  🧪 Batch 18/55 — Loss: 0.1140 — Time: 0.89s\n","  🧪 Batch 19/55 — Loss: 0.1294 — Time: 0.89s\n","  🧪 Batch 20/55 — Loss: 0.1191 — Time: 0.89s\n","  🧪 Batch 21/55 — Loss: 0.1146 — Time: 0.89s\n","  🧪 Batch 22/55 — Loss: 0.1028 — Time: 0.89s\n","  🧪 Batch 23/55 — Loss: 0.1256 — Time: 0.89s\n","  🧪 Batch 24/55 — Loss: 0.2256 — Time: 0.89s\n","  🧪 Batch 25/55 — Loss: 0.1036 — Time: 0.88s\n","  🧪 Batch 26/55 — Loss: 0.1369 — Time: 0.89s\n","  🧪 Batch 27/55 — Loss: 0.1030 — Time: 0.89s\n","  🧪 Batch 28/55 — Loss: 0.1212 — Time: 0.89s\n","  🧪 Batch 29/55 — Loss: 0.1175 — Time: 0.89s\n","  🧪 Batch 30/55 — Loss: 0.1150 — Time: 0.89s\n","  🧪 Batch 31/55 — Loss: 0.1574 — Time: 0.89s\n","  🧪 Batch 32/55 — Loss: 0.1322 — Time: 0.89s\n","  🧪 Batch 33/55 — Loss: 0.1071 — Time: 0.89s\n","  🧪 Batch 34/55 — Loss: 0.1220 — Time: 0.89s\n","  🧪 Batch 35/55 — Loss: 0.1790 — Time: 0.89s\n","  🧪 Batch 36/55 — Loss: 0.1158 — Time: 0.88s\n","  🧪 Batch 37/55 — Loss: 0.1088 — Time: 0.89s\n","  🧪 Batch 38/55 — Loss: 0.1181 — Time: 0.89s\n","  🧪 Batch 39/55 — Loss: 0.1071 — Time: 0.89s\n","  🧪 Batch 40/55 — Loss: 0.1141 — Time: 0.89s\n","  🧪 Batch 41/55 — Loss: 0.1218 — Time: 0.89s\n","  🧪 Batch 42/55 — Loss: 0.1186 — Time: 0.89s\n","  🧪 Batch 43/55 — Loss: 0.1166 — Time: 0.89s\n","  🧪 Batch 44/55 — Loss: 0.1482 — Time: 0.89s\n","  🧪 Batch 45/55 — Loss: 0.1119 — Time: 0.89s\n","  🧪 Batch 46/55 — Loss: 0.1341 — Time: 0.89s\n","  🧪 Batch 47/55 — Loss: 0.0981 — Time: 0.89s\n","  🧪 Batch 48/55 — Loss: 0.2138 — Time: 0.89s\n","  🧪 Batch 49/55 — Loss: 0.1191 — Time: 0.89s\n","  🧪 Batch 50/55 — Loss: 0.1336 — Time: 0.89s\n","  🧪 Batch 51/55 — Loss: 0.1184 — Time: 0.89s\n","  🧪 Batch 52/55 — Loss: 0.1244 — Time: 0.89s\n","  🧪 Batch 53/55 — Loss: 0.1282 — Time: 0.89s\n","  🧪 Batch 54/55 — Loss: 0.1314 — Time: 0.89s\n","  🧪 Batch 55/55 — Loss: 0.1533 — Time: 0.65s\n","🕒 Epoch Time: 51.96s | Avg Batch Time: 0.88s\n","📊 Train Loss: 0.1277 | Val Loss: 0.3033 | IoU: 0.7694 | Dice: 0.8720 | Jaccard: 0.7775 | Precision: 0.9217 | Recall: 0.8341 | Accuracy: 0.9707\n","\n","Epoch 12/100\n","  🧪 Batch 1/55 — Loss: 0.1096 — Time: 0.87s\n","  🧪 Batch 2/55 — Loss: 0.1335 — Time: 0.89s\n","  🧪 Batch 3/55 — Loss: 0.0995 — Time: 0.89s\n","  🧪 Batch 4/55 — Loss: 0.1528 — Time: 0.90s\n","  🧪 Batch 5/55 — Loss: 0.1125 — Time: 0.87s\n","  🧪 Batch 6/55 — Loss: 0.1202 — Time: 0.89s\n","  🧪 Batch 7/55 — Loss: 0.1386 — Time: 0.89s\n","  🧪 Batch 8/55 — Loss: 0.1025 — Time: 0.89s\n","  🧪 Batch 9/55 — Loss: 0.2838 — Time: 0.88s\n","  🧪 Batch 10/55 — Loss: 0.1012 — Time: 0.88s\n","  🧪 Batch 11/55 — Loss: 0.1290 — Time: 0.89s\n","  🧪 Batch 12/55 — Loss: 0.1209 — Time: 0.89s\n","  🧪 Batch 13/55 — Loss: 0.1083 — Time: 0.89s\n","  🧪 Batch 14/55 — Loss: 0.1039 — Time: 0.88s\n","  🧪 Batch 15/55 — Loss: 0.1220 — Time: 0.88s\n","  🧪 Batch 16/55 — Loss: 0.1382 — Time: 0.89s\n","  🧪 Batch 17/55 — Loss: 0.1158 — Time: 0.87s\n","  🧪 Batch 18/55 — Loss: 0.1067 — Time: 0.89s\n","  🧪 Batch 19/55 — Loss: 0.1243 — Time: 0.89s\n","  🧪 Batch 20/55 — Loss: 0.1302 — Time: 0.88s\n","  🧪 Batch 21/55 — Loss: 0.1481 — Time: 0.88s\n","  🧪 Batch 22/55 — Loss: 0.1033 — Time: 0.88s\n","  🧪 Batch 23/55 — Loss: 0.0981 — Time: 0.88s\n","  🧪 Batch 24/55 — Loss: 0.1032 — Time: 0.88s\n","  🧪 Batch 25/55 — Loss: 0.1019 — Time: 0.89s\n","  🧪 Batch 26/55 — Loss: 0.1220 — Time: 0.89s\n","  🧪 Batch 27/55 — Loss: 0.1067 — Time: 0.88s\n","  🧪 Batch 28/55 — Loss: 0.2015 — Time: 0.88s\n","  🧪 Batch 29/55 — Loss: 0.1036 — Time: 0.89s\n","  🧪 Batch 30/55 — Loss: 0.1441 — Time: 0.89s\n","  🧪 Batch 31/55 — Loss: 0.0993 — Time: 0.87s\n","  🧪 Batch 32/55 — Loss: 0.1145 — Time: 0.89s\n","  🧪 Batch 33/55 — Loss: 0.1041 — Time: 0.89s\n","  🧪 Batch 34/55 — Loss: 0.1249 — Time: 0.88s\n","  🧪 Batch 35/55 — Loss: 0.1096 — Time: 0.89s\n","  🧪 Batch 36/55 — Loss: 0.1713 — Time: 0.88s\n","  🧪 Batch 37/55 — Loss: 0.1142 — Time: 0.88s\n","  🧪 Batch 38/55 — Loss: 0.1258 — Time: 0.88s\n","  🧪 Batch 39/55 — Loss: 0.1366 — Time: 0.89s\n","  🧪 Batch 40/55 — Loss: 0.1097 — Time: 0.88s\n","  🧪 Batch 41/55 — Loss: 0.1109 — Time: 0.89s\n","  🧪 Batch 42/55 — Loss: 0.0918 — Time: 0.88s\n","  🧪 Batch 43/55 — Loss: 0.0963 — Time: 0.88s\n","  🧪 Batch 44/55 — Loss: 0.1074 — Time: 0.88s\n","  🧪 Batch 45/55 — Loss: 0.1366 — Time: 0.88s\n","  🧪 Batch 46/55 — Loss: 0.1105 — Time: 0.89s\n","  🧪 Batch 47/55 — Loss: 0.1067 — Time: 0.87s\n","  🧪 Batch 48/55 — Loss: 0.1009 — Time: 0.89s\n","  🧪 Batch 49/55 — Loss: 0.1255 — Time: 0.89s\n","  🧪 Batch 50/55 — Loss: 0.2482 — Time: 0.87s\n","  🧪 Batch 51/55 — Loss: 0.1106 — Time: 0.88s\n","  🧪 Batch 52/55 — Loss: 0.1333 — Time: 0.89s\n","  🧪 Batch 53/55 — Loss: 0.1217 — Time: 0.88s\n","  🧪 Batch 54/55 — Loss: 0.1474 — Time: 0.88s\n","  🧪 Batch 55/55 — Loss: 0.1306 — Time: 0.65s\n","🕒 Epoch Time: 51.80s | Avg Batch Time: 0.88s\n","📊 Train Loss: 0.1250 | Val Loss: 0.3037 | IoU: 0.7691 | Dice: 0.8699 | Jaccard: 0.7732 | Precision: 0.9279 | Recall: 0.8237 | Accuracy: 0.9708\n","\n","Epoch 13/100\n","  🧪 Batch 1/55 — Loss: 0.1120 — Time: 0.87s\n","  🧪 Batch 2/55 — Loss: 0.1140 — Time: 0.88s\n","  🧪 Batch 3/55 — Loss: 0.1104 — Time: 0.89s\n","  🧪 Batch 4/55 — Loss: 0.1061 — Time: 0.88s\n","  🧪 Batch 5/55 — Loss: 0.1370 — Time: 0.89s\n","  🧪 Batch 6/55 — Loss: 0.1057 — Time: 0.89s\n","  🧪 Batch 7/55 — Loss: 0.1163 — Time: 0.88s\n","  🧪 Batch 8/55 — Loss: 0.1042 — Time: 0.88s\n","  🧪 Batch 9/55 — Loss: 0.1058 — Time: 0.87s\n","  🧪 Batch 10/55 — Loss: 0.1138 — Time: 0.89s\n","  🧪 Batch 11/55 — Loss: 0.1215 — Time: 0.89s\n","  🧪 Batch 12/55 — Loss: 0.1542 — Time: 0.89s\n","  🧪 Batch 13/55 — Loss: 0.0892 — Time: 0.87s\n","  🧪 Batch 14/55 — Loss: 0.0920 — Time: 0.89s\n","  🧪 Batch 15/55 — Loss: 0.1053 — Time: 0.88s\n","  🧪 Batch 16/55 — Loss: 0.1102 — Time: 0.88s\n","  🧪 Batch 17/55 — Loss: 0.1356 — Time: 0.89s\n","  🧪 Batch 18/55 — Loss: 0.0952 — Time: 0.89s\n","  🧪 Batch 19/55 — Loss: 0.1108 — Time: 0.88s\n","  🧪 Batch 20/55 — Loss: 0.1056 — Time: 0.88s\n","  🧪 Batch 21/55 — Loss: 0.1169 — Time: 0.89s\n","  🧪 Batch 22/55 — Loss: 0.1070 — Time: 0.89s\n","  🧪 Batch 23/55 — Loss: 0.0976 — Time: 0.88s\n","  🧪 Batch 24/55 — Loss: 0.1105 — Time: 0.88s\n","  🧪 Batch 25/55 — Loss: 0.0921 — Time: 0.88s\n","  🧪 Batch 26/55 — Loss: 0.1906 — Time: 0.89s\n","  🧪 Batch 27/55 — Loss: 0.0800 — Time: 0.89s\n","  🧪 Batch 28/55 — Loss: 0.0853 — Time: 0.89s\n","  🧪 Batch 29/55 — Loss: 0.0972 — Time: 0.89s\n","  🧪 Batch 30/55 — Loss: 0.1034 — Time: 0.89s\n","  🧪 Batch 31/55 — Loss: 0.0947 — Time: 0.88s\n","  🧪 Batch 32/55 — Loss: 0.1093 — Time: 0.88s\n","  🧪 Batch 33/55 — Loss: 0.1127 — Time: 0.89s\n","  🧪 Batch 34/55 — Loss: 0.0901 — Time: 0.89s\n","  🧪 Batch 35/55 — Loss: 0.1113 — Time: 0.89s\n","  🧪 Batch 36/55 — Loss: 0.1412 — Time: 0.89s\n","  🧪 Batch 37/55 — Loss: 0.0884 — Time: 0.89s\n","  🧪 Batch 38/55 — Loss: 0.1037 — Time: 0.89s\n","  🧪 Batch 39/55 — Loss: 0.1095 — Time: 0.89s\n","  🧪 Batch 40/55 — Loss: 0.1012 — Time: 0.89s\n","  🧪 Batch 41/55 — Loss: 0.0875 — Time: 0.88s\n","  🧪 Batch 42/55 — Loss: 0.0903 — Time: 0.89s\n","  🧪 Batch 43/55 — Loss: 0.0976 — Time: 0.89s\n","  🧪 Batch 44/55 — Loss: 0.1252 — Time: 0.89s\n","  🧪 Batch 45/55 — Loss: 0.0959 — Time: 0.89s\n","  🧪 Batch 46/55 — Loss: 0.1241 — Time: 0.88s\n","  🧪 Batch 47/55 — Loss: 0.1025 — Time: 0.89s\n","  🧪 Batch 48/55 — Loss: 0.1138 — Time: 0.89s\n","  🧪 Batch 49/55 — Loss: 0.0971 — Time: 0.89s\n","  🧪 Batch 50/55 — Loss: 0.1015 — Time: 0.89s\n","  🧪 Batch 51/55 — Loss: 0.1186 — Time: 0.89s\n","  🧪 Batch 52/55 — Loss: 0.1264 — Time: 0.89s\n","  🧪 Batch 53/55 — Loss: 0.1003 — Time: 0.89s\n","  🧪 Batch 54/55 — Loss: 0.0934 — Time: 0.88s\n","  🧪 Batch 55/55 — Loss: 0.0856 — Time: 0.64s\n","🕒 Epoch Time: 52.22s | Avg Batch Time: 0.88s\n","📊 Train Loss: 0.1081 | Val Loss: 0.2594 | IoU: 0.7923 | Dice: 0.8906 | Jaccard: 0.8055 | Precision: 0.9201 | Recall: 0.8665 | Accuracy: 0.9743\n","\n","Epoch 14/100\n","  🧪 Batch 1/55 — Loss: 0.1181 — Time: 0.87s\n","  🧪 Batch 2/55 — Loss: 0.0920 — Time: 0.87s\n","  🧪 Batch 3/55 — Loss: 0.0935 — Time: 0.89s\n","  🧪 Batch 4/55 — Loss: 0.0947 — Time: 0.89s\n","  🧪 Batch 5/55 — Loss: 0.1877 — Time: 0.89s\n","  🧪 Batch 6/55 — Loss: 0.1435 — Time: 0.89s\n","  🧪 Batch 7/55 — Loss: 0.1033 — Time: 0.88s\n","  🧪 Batch 8/55 — Loss: 0.1016 — Time: 0.88s\n","  🧪 Batch 9/55 — Loss: 0.1137 — Time: 0.89s\n","  🧪 Batch 10/55 — Loss: 0.1095 — Time: 0.89s\n","  🧪 Batch 11/55 — Loss: 0.1086 — Time: 0.89s\n","  🧪 Batch 12/55 — Loss: 0.0945 — Time: 0.89s\n","  🧪 Batch 13/55 — Loss: 0.0985 — Time: 0.89s\n","  🧪 Batch 14/55 — Loss: 0.0883 — Time: 0.89s\n","  🧪 Batch 15/55 — Loss: 0.1212 — Time: 0.89s\n","  🧪 Batch 16/55 — Loss: 0.0933 — Time: 0.89s\n","  🧪 Batch 17/55 — Loss: 0.1136 — Time: 0.89s\n","  🧪 Batch 18/55 — Loss: 0.0909 — Time: 0.89s\n","  🧪 Batch 19/55 — Loss: 0.0827 — Time: 0.89s\n","  🧪 Batch 20/55 — Loss: 0.0901 — Time: 0.89s\n","  🧪 Batch 21/55 — Loss: 0.0871 — Time: 0.89s\n","  🧪 Batch 22/55 — Loss: 0.0867 — Time: 0.89s\n","  🧪 Batch 23/55 — Loss: 0.0988 — Time: 0.89s\n","  🧪 Batch 24/55 — Loss: 0.0831 — Time: 0.88s\n","  🧪 Batch 25/55 — Loss: 0.0828 — Time: 0.88s\n","  🧪 Batch 26/55 — Loss: 0.0847 — Time: 0.88s\n","  🧪 Batch 27/55 — Loss: 0.0928 — Time: 0.89s\n","  🧪 Batch 28/55 — Loss: 0.0841 — Time: 0.89s\n","  🧪 Batch 29/55 — Loss: 0.1232 — Time: 0.90s\n","  🧪 Batch 30/55 — Loss: 0.0974 — Time: 0.88s\n","  🧪 Batch 31/55 — Loss: 0.0997 — Time: 0.90s\n","  🧪 Batch 32/55 — Loss: 0.0824 — Time: 0.89s\n","  🧪 Batch 33/55 — Loss: 0.0887 — Time: 0.89s\n","  🧪 Batch 34/55 — Loss: 0.0895 — Time: 0.89s\n","  🧪 Batch 35/55 — Loss: 0.0858 — Time: 0.89s\n","  🧪 Batch 36/55 — Loss: 0.0884 — Time: 0.89s\n","  🧪 Batch 37/55 — Loss: 0.0825 — Time: 0.89s\n","  🧪 Batch 38/55 — Loss: 0.0789 — Time: 0.89s\n","  🧪 Batch 39/55 — Loss: 0.0986 — Time: 0.89s\n","  🧪 Batch 40/55 — Loss: 0.0750 — Time: 0.89s\n","  🧪 Batch 41/55 — Loss: 0.0948 — Time: 0.89s\n","  🧪 Batch 42/55 — Loss: 0.0954 — Time: 0.89s\n","  🧪 Batch 43/55 — Loss: 0.0795 — Time: 0.89s\n","  🧪 Batch 44/55 — Loss: 0.0970 — Time: 0.89s\n","  🧪 Batch 45/55 — Loss: 0.0812 — Time: 0.89s\n","  🧪 Batch 46/55 — Loss: 0.0810 — Time: 0.89s\n","  🧪 Batch 47/55 — Loss: 0.0729 — Time: 0.89s\n","  🧪 Batch 48/55 — Loss: 0.0894 — Time: 0.89s\n","  🧪 Batch 49/55 — Loss: 0.0798 — Time: 0.89s\n","  🧪 Batch 50/55 — Loss: 0.0817 — Time: 0.89s\n","  🧪 Batch 51/55 — Loss: 0.0936 — Time: 0.89s\n","  🧪 Batch 52/55 — Loss: 0.1058 — Time: 0.89s\n","  🧪 Batch 53/55 — Loss: 0.0989 — Time: 0.89s\n","  🧪 Batch 54/55 — Loss: 0.0816 — Time: 0.89s\n","  🧪 Batch 55/55 — Loss: 0.1024 — Time: 0.65s\n","🕒 Epoch Time: 52.87s | Avg Batch Time: 0.88s\n","📊 Train Loss: 0.0957 | Val Loss: 0.2478 | IoU: 0.7952 | Dice: 0.8959 | Jaccard: 0.8143 | Precision: 0.9317 | Recall: 0.8655 | Accuracy: 0.9753\n","\n","Epoch 15/100\n","  🧪 Batch 1/55 — Loss: 0.0880 — Time: 0.87s\n","  🧪 Batch 2/55 — Loss: 0.0972 — Time: 0.87s\n","  🧪 Batch 3/55 — Loss: 0.0865 — Time: 0.89s\n","  🧪 Batch 4/55 — Loss: 0.0844 — Time: 0.89s\n","  🧪 Batch 5/55 — Loss: 0.0781 — Time: 0.89s\n","  🧪 Batch 6/55 — Loss: 0.0910 — Time: 0.88s\n","  🧪 Batch 7/55 — Loss: 0.0748 — Time: 0.89s\n","  🧪 Batch 8/55 — Loss: 0.0739 — Time: 0.89s\n","  🧪 Batch 9/55 — Loss: 0.1158 — Time: 0.89s\n","  🧪 Batch 10/55 — Loss: 0.0708 — Time: 0.89s\n","  🧪 Batch 11/55 — Loss: 0.0864 — Time: 0.88s\n","  🧪 Batch 12/55 — Loss: 0.0828 — Time: 0.88s\n","  🧪 Batch 13/55 — Loss: 0.0789 — Time: 0.89s\n","  🧪 Batch 14/55 — Loss: 0.0886 — Time: 0.89s\n","  🧪 Batch 15/55 — Loss: 0.0957 — Time: 0.87s\n","  🧪 Batch 16/55 — Loss: 0.0763 — Time: 0.88s\n","  🧪 Batch 17/55 — Loss: 0.0821 — Time: 0.88s\n","  🧪 Batch 18/55 — Loss: 0.0701 — Time: 0.89s\n","  🧪 Batch 19/55 — Loss: 0.1650 — Time: 0.89s\n","  🧪 Batch 20/55 — Loss: 0.0773 — Time: 0.89s\n","  🧪 Batch 21/55 — Loss: 0.0996 — Time: 0.88s\n","  🧪 Batch 22/55 — Loss: 0.0885 — Time: 0.88s\n","  🧪 Batch 23/55 — Loss: 0.0802 — Time: 0.87s\n","  🧪 Batch 24/55 — Loss: 0.0860 — Time: 0.89s\n","  🧪 Batch 25/55 — Loss: 0.0716 — Time: 0.89s\n","  🧪 Batch 26/55 — Loss: 0.0872 — Time: 0.89s\n","  🧪 Batch 27/55 — Loss: 0.0784 — Time: 0.89s\n","  🧪 Batch 28/55 — Loss: 0.0710 — Time: 0.89s\n","  🧪 Batch 29/55 — Loss: 0.0942 — Time: 0.89s\n","  🧪 Batch 30/55 — Loss: 0.0919 — Time: 0.89s\n","  🧪 Batch 31/55 — Loss: 0.0977 — Time: 0.89s\n","  🧪 Batch 32/55 — Loss: 0.0748 — Time: 0.89s\n","  🧪 Batch 33/55 — Loss: 0.0701 — Time: 0.88s\n","  🧪 Batch 34/55 — Loss: 0.0664 — Time: 0.88s\n","  🧪 Batch 35/55 — Loss: 0.1018 — Time: 0.89s\n","  🧪 Batch 36/55 — Loss: 0.0878 — Time: 0.89s\n","  🧪 Batch 37/55 — Loss: 0.1198 — Time: 0.89s\n","  🧪 Batch 38/55 — Loss: 0.0750 — Time: 0.88s\n","  🧪 Batch 39/55 — Loss: 0.0685 — Time: 0.89s\n","  🧪 Batch 40/55 — Loss: 0.0912 — Time: 0.89s\n","  🧪 Batch 41/55 — Loss: 0.0890 — Time: 0.89s\n","  🧪 Batch 42/55 — Loss: 0.0735 — Time: 0.89s\n","  🧪 Batch 43/55 — Loss: 0.1018 — Time: 0.89s\n","  🧪 Batch 44/55 — Loss: 0.0666 — Time: 0.89s\n","  🧪 Batch 45/55 — Loss: 0.0720 — Time: 0.88s\n","  🧪 Batch 46/55 — Loss: 0.0814 — Time: 0.89s\n","  🧪 Batch 47/55 — Loss: 0.0972 — Time: 0.89s\n","  🧪 Batch 48/55 — Loss: 0.0896 — Time: 0.89s\n","  🧪 Batch 49/55 — Loss: 0.0696 — Time: 0.89s\n","  🧪 Batch 50/55 — Loss: 0.0737 — Time: 0.89s\n","  🧪 Batch 51/55 — Loss: 0.0965 — Time: 0.89s\n","  🧪 Batch 52/55 — Loss: 0.0864 — Time: 0.89s\n","  🧪 Batch 53/55 — Loss: 0.0836 — Time: 0.89s\n","  🧪 Batch 54/55 — Loss: 0.0834 — Time: 0.87s\n","  🧪 Batch 55/55 — Loss: 0.0857 — Time: 0.65s\n","🕒 Epoch Time: 52.79s | Avg Batch Time: 0.88s\n","📊 Train Loss: 0.0857 | Val Loss: 0.2460 | IoU: 0.7942 | Dice: 0.8952 | Jaccard: 0.8133 | Precision: 0.9368 | Recall: 0.8600 | Accuracy: 0.9753\n","\n","Epoch 16/100\n","  🧪 Batch 1/55 — Loss: 0.0820 — Time: 0.87s\n","  🧪 Batch 2/55 — Loss: 0.0691 — Time: 0.89s\n","  🧪 Batch 3/55 — Loss: 0.0752 — Time: 0.89s\n","  🧪 Batch 4/55 — Loss: 0.1003 — Time: 0.89s\n","  🧪 Batch 5/55 — Loss: 0.0641 — Time: 0.87s\n","  🧪 Batch 6/55 — Loss: 0.0751 — Time: 0.89s\n","  🧪 Batch 7/55 — Loss: 0.0972 — Time: 0.88s\n","  🧪 Batch 8/55 — Loss: 0.0765 — Time: 0.88s\n","  🧪 Batch 9/55 — Loss: 0.0756 — Time: 0.89s\n","  🧪 Batch 10/55 — Loss: 0.0765 — Time: 0.89s\n","  🧪 Batch 11/55 — Loss: 0.0684 — Time: 0.89s\n","  🧪 Batch 12/55 — Loss: 0.0722 — Time: 0.88s\n","  🧪 Batch 13/55 — Loss: 0.0631 — Time: 0.89s\n","  🧪 Batch 14/55 — Loss: 0.0965 — Time: 0.87s\n","  🧪 Batch 15/55 — Loss: 0.0912 — Time: 0.88s\n","  🧪 Batch 16/55 — Loss: 0.0812 — Time: 0.88s\n","  🧪 Batch 17/55 — Loss: 0.0729 — Time: 0.89s\n","  🧪 Batch 18/55 — Loss: 0.0659 — Time: 0.89s\n","  🧪 Batch 19/55 — Loss: 0.0629 — Time: 0.88s\n","  🧪 Batch 20/55 — Loss: 0.0951 — Time: 0.89s\n","  🧪 Batch 21/55 — Loss: 0.0764 — Time: 0.88s\n","  🧪 Batch 22/55 — Loss: 0.0693 — Time: 0.89s\n","  🧪 Batch 23/55 — Loss: 0.0682 — Time: 0.89s\n","  🧪 Batch 24/55 — Loss: 0.0656 — Time: 0.88s\n","  🧪 Batch 25/55 — Loss: 0.0729 — Time: 0.89s\n","  🧪 Batch 26/55 — Loss: 0.0816 — Time: 0.89s\n","  🧪 Batch 27/55 — Loss: 0.0789 — Time: 0.89s\n","  🧪 Batch 28/55 — Loss: 0.0705 — Time: 0.89s\n","  🧪 Batch 29/55 — Loss: 0.0794 — Time: 0.89s\n","  🧪 Batch 30/55 — Loss: 0.0738 — Time: 0.89s\n","  🧪 Batch 31/55 — Loss: 0.0672 — Time: 0.89s\n","  🧪 Batch 32/55 — Loss: 0.0734 — Time: 0.89s\n","  🧪 Batch 33/55 — Loss: 0.0750 — Time: 0.88s\n","  🧪 Batch 34/55 — Loss: 0.0894 — Time: 0.88s\n","  🧪 Batch 35/55 — Loss: 0.1508 — Time: 0.89s\n","  🧪 Batch 36/55 — Loss: 0.0834 — Time: 0.89s\n","  🧪 Batch 37/55 — Loss: 0.0704 — Time: 0.89s\n","  🧪 Batch 38/55 — Loss: 0.0664 — Time: 0.89s\n","  🧪 Batch 39/55 — Loss: 0.0908 — Time: 0.89s\n","  🧪 Batch 40/55 — Loss: 0.0655 — Time: 0.89s\n","  🧪 Batch 41/55 — Loss: 0.0758 — Time: 0.89s\n","  🧪 Batch 42/55 — Loss: 0.0847 — Time: 0.89s\n","  🧪 Batch 43/55 — Loss: 0.0777 — Time: 0.89s\n","  🧪 Batch 44/55 — Loss: 0.0962 — Time: 0.89s\n","  🧪 Batch 45/55 — Loss: 0.0857 — Time: 0.89s\n","  🧪 Batch 46/55 — Loss: 0.0808 — Time: 0.89s\n","  🧪 Batch 47/55 — Loss: 0.0740 — Time: 0.89s\n","  🧪 Batch 48/55 — Loss: 0.0746 — Time: 0.89s\n","  🧪 Batch 49/55 — Loss: 0.0690 — Time: 0.89s\n","  🧪 Batch 50/55 — Loss: 0.0734 — Time: 0.89s\n","  🧪 Batch 51/55 — Loss: 0.0643 — Time: 0.89s\n","  🧪 Batch 52/55 — Loss: 0.0644 — Time: 0.89s\n","  🧪 Batch 53/55 — Loss: 0.0665 — Time: 0.89s\n","  🧪 Batch 54/55 — Loss: 0.0686 — Time: 0.89s\n","  🧪 Batch 55/55 — Loss: 0.0889 — Time: 0.65s\n","🕒 Epoch Time: 52.86s | Avg Batch Time: 0.88s\n","📊 Train Loss: 0.0777 | Val Loss: 0.2425 | IoU: 0.7962 | Dice: 0.8957 | Jaccard: 0.8139 | Precision: 0.9221 | Recall: 0.8735 | Accuracy: 0.9750\n","\n","Epoch 17/100\n","  🧪 Batch 1/55 — Loss: 0.0735 — Time: 0.89s\n","  🧪 Batch 2/55 — Loss: 0.0889 — Time: 0.87s\n","  🧪 Batch 3/55 — Loss: 0.0759 — Time: 0.89s\n","  🧪 Batch 4/55 — Loss: 0.0714 — Time: 0.89s\n","  🧪 Batch 5/55 — Loss: 0.0714 — Time: 0.89s\n","  🧪 Batch 6/55 — Loss: 0.0714 — Time: 0.88s\n","  🧪 Batch 7/55 — Loss: 0.0676 — Time: 0.88s\n","  🧪 Batch 8/55 — Loss: 0.0648 — Time: 0.89s\n","  🧪 Batch 9/55 — Loss: 0.0917 — Time: 0.89s\n","  🧪 Batch 10/55 — Loss: 0.0667 — Time: 0.89s\n","  🧪 Batch 11/55 — Loss: 0.0597 — Time: 0.89s\n","  🧪 Batch 12/55 — Loss: 0.0682 — Time: 0.89s\n","  🧪 Batch 13/55 — Loss: 0.0725 — Time: 0.89s\n","  🧪 Batch 14/55 — Loss: 0.0655 — Time: 0.89s\n","  🧪 Batch 15/55 — Loss: 0.0798 — Time: 0.89s\n","  🧪 Batch 16/55 — Loss: 0.0579 — Time: 0.89s\n","  🧪 Batch 17/55 — Loss: 0.0679 — Time: 0.89s\n","  🧪 Batch 18/55 — Loss: 0.0713 — Time: 0.89s\n","  🧪 Batch 19/55 — Loss: 0.0818 — Time: 0.89s\n","  🧪 Batch 20/55 — Loss: 0.0620 — Time: 0.89s\n","  🧪 Batch 21/55 — Loss: 0.0838 — Time: 0.89s\n","  🧪 Batch 22/55 — Loss: 0.0766 — Time: 0.89s\n","  🧪 Batch 23/55 — Loss: 0.0926 — Time: 0.89s\n","  🧪 Batch 24/55 — Loss: 0.0703 — Time: 0.89s\n","  🧪 Batch 25/55 — Loss: 0.0754 — Time: 0.89s\n","  🧪 Batch 26/55 — Loss: 0.0623 — Time: 0.88s\n","  🧪 Batch 27/55 — Loss: 0.0664 — Time: 0.89s\n","  🧪 Batch 28/55 — Loss: 0.0654 — Time: 0.88s\n","  🧪 Batch 29/55 — Loss: 0.0617 — Time: 0.89s\n","  🧪 Batch 30/55 — Loss: 0.0638 — Time: 0.89s\n","  🧪 Batch 31/55 — Loss: 0.0800 — Time: 0.89s\n","  🧪 Batch 32/55 — Loss: 0.0645 — Time: 0.89s\n","  🧪 Batch 33/55 — Loss: 0.0713 — Time: 0.89s\n","  🧪 Batch 34/55 — Loss: 0.0646 — Time: 0.89s\n","  🧪 Batch 35/55 — Loss: 0.0616 — Time: 0.89s\n","  🧪 Batch 36/55 — Loss: 0.0662 — Time: 0.89s\n","  🧪 Batch 37/55 — Loss: 0.0780 — Time: 0.89s\n","  🧪 Batch 38/55 — Loss: 0.0614 — Time: 0.89s\n","  🧪 Batch 39/55 — Loss: 0.0629 — Time: 0.89s\n","  🧪 Batch 40/55 — Loss: 0.0556 — Time: 0.89s\n","  🧪 Batch 41/55 — Loss: 0.0656 — Time: 0.90s\n","  🧪 Batch 42/55 — Loss: 0.0662 — Time: 0.89s\n","  🧪 Batch 43/55 — Loss: 0.0627 — Time: 0.88s\n","  🧪 Batch 44/55 — Loss: 0.0779 — Time: 0.89s\n","  🧪 Batch 45/55 — Loss: 0.0551 — Time: 0.89s\n","  🧪 Batch 46/55 — Loss: 0.0633 — Time: 0.89s\n","  🧪 Batch 47/55 — Loss: 0.0667 — Time: 0.89s\n","  🧪 Batch 48/55 — Loss: 0.0575 — Time: 0.89s\n","  🧪 Batch 49/55 — Loss: 0.0745 — Time: 0.89s\n","  🧪 Batch 50/55 — Loss: 0.0785 — Time: 0.89s\n","  🧪 Batch 51/55 — Loss: 0.0606 — Time: 0.89s\n","  🧪 Batch 52/55 — Loss: 0.0833 — Time: 0.89s\n","  🧪 Batch 53/55 — Loss: 0.0979 — Time: 0.89s\n","  🧪 Batch 54/55 — Loss: 0.1276 — Time: 0.89s\n","  🧪 Batch 55/55 — Loss: 0.0746 — Time: 0.65s\n","🕒 Epoch Time: 52.86s | Avg Batch Time: 0.88s\n","📊 Train Loss: 0.0714 | Val Loss: 0.2410 | IoU: 0.7967 | Dice: 0.8971 | Jaccard: 0.8164 | Precision: 0.9318 | Recall: 0.8676 | Accuracy: 0.9754\n","\n","Epoch 18/100\n","  🧪 Batch 1/55 — Loss: 0.0649 — Time: 0.88s\n","  🧪 Batch 2/55 — Loss: 0.0620 — Time: 0.88s\n","  🧪 Batch 3/55 — Loss: 0.0799 — Time: 0.89s\n","  🧪 Batch 4/55 — Loss: 0.0607 — Time: 0.89s\n","  🧪 Batch 5/55 — Loss: 0.0535 — Time: 0.89s\n","  🧪 Batch 6/55 — Loss: 0.0635 — Time: 0.88s\n","  🧪 Batch 7/55 — Loss: 0.0799 — Time: 0.89s\n","  🧪 Batch 8/55 — Loss: 0.0638 — Time: 0.89s\n","  🧪 Batch 9/55 — Loss: 0.0602 — Time: 0.89s\n","  🧪 Batch 10/55 — Loss: 0.0597 — Time: 0.89s\n","  🧪 Batch 11/55 — Loss: 0.0811 — Time: 0.89s\n","  🧪 Batch 12/55 — Loss: 0.0778 — Time: 0.88s\n","  🧪 Batch 13/55 — Loss: 0.0567 — Time: 0.89s\n","  🧪 Batch 14/55 — Loss: 0.0622 — Time: 0.89s\n","  🧪 Batch 15/55 — Loss: 0.0614 — Time: 0.89s\n","  🧪 Batch 16/55 — Loss: 0.1122 — Time: 0.89s\n","  🧪 Batch 17/55 — Loss: 0.0733 — Time: 0.89s\n","  🧪 Batch 18/55 — Loss: 0.0637 — Time: 0.89s\n","  🧪 Batch 19/55 — Loss: 0.0685 — Time: 0.89s\n","  🧪 Batch 20/55 — Loss: 0.0623 — Time: 0.89s\n","  🧪 Batch 21/55 — Loss: 0.0763 — Time: 0.89s\n","  🧪 Batch 22/55 — Loss: 0.0573 — Time: 0.89s\n","  🧪 Batch 23/55 — Loss: 0.0722 — Time: 0.89s\n","  🧪 Batch 24/55 — Loss: 0.0642 — Time: 0.89s\n","  🧪 Batch 25/55 — Loss: 0.0736 — Time: 0.89s\n","  🧪 Batch 26/55 — Loss: 0.0627 — Time: 0.89s\n","  🧪 Batch 27/55 — Loss: 0.0596 — Time: 0.89s\n","  🧪 Batch 28/55 — Loss: 0.0534 — Time: 0.88s\n","  🧪 Batch 29/55 — Loss: 0.0624 — Time: 0.89s\n","  🧪 Batch 30/55 — Loss: 0.0603 — Time: 0.89s\n","  🧪 Batch 31/55 — Loss: 0.0573 — Time: 0.89s\n","  🧪 Batch 32/55 — Loss: 0.1135 — Time: 0.89s\n","  🧪 Batch 33/55 — Loss: 0.0561 — Time: 0.89s\n","  🧪 Batch 34/55 — Loss: 0.0643 — Time: 0.89s\n","  🧪 Batch 35/55 — Loss: 0.0577 — Time: 0.89s\n","  🧪 Batch 36/55 — Loss: 0.0769 — Time: 0.89s\n","  🧪 Batch 37/55 — Loss: 0.0725 — Time: 0.89s\n","  🧪 Batch 38/55 — Loss: 0.0661 — Time: 0.89s\n","  🧪 Batch 39/55 — Loss: 0.0615 — Time: 0.89s\n","  🧪 Batch 40/55 — Loss: 0.0702 — Time: 0.89s\n","  🧪 Batch 41/55 — Loss: 0.0561 — Time: 0.88s\n","  🧪 Batch 42/55 — Loss: 0.0842 — Time: 0.88s\n","  🧪 Batch 43/55 — Loss: 0.0629 — Time: 0.88s\n","  🧪 Batch 44/55 — Loss: 0.0629 — Time: 0.88s\n","  🧪 Batch 45/55 — Loss: 0.0647 — Time: 0.89s\n","  🧪 Batch 46/55 — Loss: 0.0555 — Time: 0.88s\n","  🧪 Batch 47/55 — Loss: 0.0741 — Time: 0.88s\n","  🧪 Batch 48/55 — Loss: 0.0693 — Time: 0.89s\n","  🧪 Batch 49/55 — Loss: 0.0683 — Time: 0.89s\n","  🧪 Batch 50/55 — Loss: 0.0706 — Time: 0.89s\n","  🧪 Batch 51/55 — Loss: 0.0732 — Time: 0.89s\n","  🧪 Batch 52/55 — Loss: 0.0621 — Time: 0.89s\n","  🧪 Batch 53/55 — Loss: 0.0727 — Time: 0.89s\n","  🧪 Batch 54/55 — Loss: 0.0761 — Time: 0.89s\n","  🧪 Batch 55/55 — Loss: 0.0621 — Time: 0.64s\n","🕒 Epoch Time: 52.34s | Avg Batch Time: 0.88s\n","📊 Train Loss: 0.0676 | Val Loss: 0.2305 | IoU: 0.8035 | Dice: 0.9018 | Jaccard: 0.8236 | Precision: 0.9132 | Recall: 0.8934 | Accuracy: 0.9762\n","\n","Epoch 19/100\n","  🧪 Batch 1/55 — Loss: 0.0798 — Time: 0.89s\n","  🧪 Batch 2/55 — Loss: 0.0600 — Time: 0.88s\n","  🧪 Batch 3/55 — Loss: 0.0605 — Time: 0.89s\n","  🧪 Batch 4/55 — Loss: 0.0679 — Time: 0.88s\n","  🧪 Batch 5/55 — Loss: 0.0911 — Time: 0.89s\n","  🧪 Batch 6/55 — Loss: 0.0529 — Time: 0.89s\n","  🧪 Batch 7/55 — Loss: 0.0569 — Time: 0.89s\n","  🧪 Batch 8/55 — Loss: 0.0640 — Time: 0.88s\n","  🧪 Batch 9/55 — Loss: 0.0551 — Time: 0.88s\n","  🧪 Batch 10/55 — Loss: 0.0692 — Time: 0.89s\n","  🧪 Batch 11/55 — Loss: 0.0590 — Time: 0.89s\n","  🧪 Batch 12/55 — Loss: 0.0630 — Time: 0.89s\n","  🧪 Batch 13/55 — Loss: 0.0566 — Time: 0.89s\n","  🧪 Batch 14/55 — Loss: 0.0637 — Time: 0.88s\n","  🧪 Batch 15/55 — Loss: 0.0601 — Time: 0.88s\n","  🧪 Batch 16/55 — Loss: 0.0650 — Time: 0.89s\n","  🧪 Batch 17/55 — Loss: 0.0664 — Time: 0.89s\n","  🧪 Batch 18/55 — Loss: 0.0819 — Time: 0.89s\n","  🧪 Batch 19/55 — Loss: 0.0633 — Time: 0.89s\n","  🧪 Batch 20/55 — Loss: 0.0807 — Time: 0.88s\n","  🧪 Batch 21/55 — Loss: 0.0571 — Time: 0.88s\n","  🧪 Batch 22/55 — Loss: 0.0776 — Time: 0.89s\n","  🧪 Batch 23/55 — Loss: 0.0536 — Time: 0.89s\n","  🧪 Batch 24/55 — Loss: 0.0709 — Time: 0.89s\n","  🧪 Batch 25/55 — Loss: 0.0613 — Time: 0.88s\n","  🧪 Batch 26/55 — Loss: 0.0547 — Time: 0.88s\n","  🧪 Batch 27/55 — Loss: 0.0588 — Time: 0.89s\n","  🧪 Batch 28/55 — Loss: 0.0613 — Time: 0.89s\n","  🧪 Batch 29/55 — Loss: 0.0690 — Time: 0.88s\n","  🧪 Batch 30/55 — Loss: 0.0600 — Time: 0.88s\n","  🧪 Batch 31/55 — Loss: 0.0778 — Time: 0.88s\n","  🧪 Batch 32/55 — Loss: 0.0584 — Time: 0.89s\n","  🧪 Batch 33/55 — Loss: 0.0607 — Time: 0.89s\n","  🧪 Batch 34/55 — Loss: 0.0739 — Time: 0.89s\n","  🧪 Batch 35/55 — Loss: 0.0633 — Time: 0.89s\n","  🧪 Batch 36/55 — Loss: 0.0668 — Time: 0.89s\n","  🧪 Batch 37/55 — Loss: 0.0653 — Time: 0.89s\n","  🧪 Batch 38/55 — Loss: 0.0698 — Time: 0.89s\n","  🧪 Batch 39/55 — Loss: 0.0622 — Time: 0.88s\n","  🧪 Batch 40/55 — Loss: 0.0582 — Time: 0.88s\n","  🧪 Batch 41/55 — Loss: 0.0523 — Time: 0.89s\n","  🧪 Batch 42/55 — Loss: 0.0607 — Time: 0.89s\n","  🧪 Batch 43/55 — Loss: 0.0626 — Time: 0.89s\n","  🧪 Batch 44/55 — Loss: 0.0572 — Time: 0.89s\n","  🧪 Batch 45/55 — Loss: 0.0564 — Time: 0.89s\n","  🧪 Batch 46/55 — Loss: 0.0528 — Time: 0.89s\n","  🧪 Batch 47/55 — Loss: 0.0606 — Time: 0.89s\n","  🧪 Batch 48/55 — Loss: 0.0550 — Time: 0.89s\n","  🧪 Batch 49/55 — Loss: 0.0610 — Time: 0.89s\n","  🧪 Batch 50/55 — Loss: 0.0552 — Time: 0.89s\n","  🧪 Batch 51/55 — Loss: 0.0549 — Time: 0.88s\n","  🧪 Batch 52/55 — Loss: 0.0806 — Time: 0.88s\n","  🧪 Batch 53/55 — Loss: 0.0740 — Time: 0.89s\n","  🧪 Batch 54/55 — Loss: 0.0617 — Time: 0.89s\n","  🧪 Batch 55/55 — Loss: 0.0649 — Time: 0.65s\n","🕒 Epoch Time: 52.30s | Avg Batch Time: 0.88s\n","📊 Train Loss: 0.0638 | Val Loss: 0.2404 | IoU: 0.7978 | Dice: 0.8950 | Jaccard: 0.8130 | Precision: 0.9277 | Recall: 0.8670 | Accuracy: 0.9750\n","\n","Epoch 20/100\n","  🧪 Batch 1/55 — Loss: 0.0615 — Time: 0.87s\n","  🧪 Batch 2/55 — Loss: 0.0589 — Time: 0.88s\n","  🧪 Batch 3/55 — Loss: 0.0634 — Time: 0.89s\n","  🧪 Batch 4/55 — Loss: 0.0561 — Time: 0.88s\n","  🧪 Batch 5/55 — Loss: 0.0497 — Time: 0.88s\n","  🧪 Batch 6/55 — Loss: 0.0652 — Time: 0.89s\n","  🧪 Batch 7/55 — Loss: 0.0612 — Time: 0.89s\n","  🧪 Batch 8/55 — Loss: 0.0647 — Time: 0.88s\n","  🧪 Batch 9/55 — Loss: 0.0512 — Time: 0.88s\n","  🧪 Batch 10/55 — Loss: 0.0591 — Time: 0.88s\n","  🧪 Batch 11/55 — Loss: 0.0541 — Time: 0.89s\n","  🧪 Batch 12/55 — Loss: 0.0566 — Time: 0.89s\n","  🧪 Batch 13/55 — Loss: 0.0618 — Time: 0.88s\n","  🧪 Batch 14/55 — Loss: 0.0585 — Time: 0.88s\n","  🧪 Batch 15/55 — Loss: 0.0613 — Time: 0.89s\n","  🧪 Batch 16/55 — Loss: 0.0729 — Time: 0.89s\n","  🧪 Batch 17/55 — Loss: 0.0582 — Time: 0.89s\n","  🧪 Batch 18/55 — Loss: 0.0616 — Time: 0.88s\n","  🧪 Batch 19/55 — Loss: 0.0535 — Time: 0.89s\n","  🧪 Batch 20/55 — Loss: 0.0535 — Time: 0.88s\n","  🧪 Batch 21/55 — Loss: 0.0635 — Time: 0.88s\n","  🧪 Batch 22/55 — Loss: 0.0658 — Time: 0.89s\n","  🧪 Batch 23/55 — Loss: 0.0517 — Time: 0.90s\n","  🧪 Batch 24/55 — Loss: 0.0724 — Time: 0.88s\n","  🧪 Batch 25/55 — Loss: 0.0552 — Time: 0.89s\n","  🧪 Batch 26/55 — Loss: 0.0479 — Time: 0.89s\n","  🧪 Batch 27/55 — Loss: 0.0566 — Time: 0.89s\n","  🧪 Batch 28/55 — Loss: 0.0578 — Time: 0.89s\n","  🧪 Batch 29/55 — Loss: 0.0606 — Time: 0.89s\n","  🧪 Batch 30/55 — Loss: 0.0749 — Time: 0.89s\n","  🧪 Batch 31/55 — Loss: 0.0508 — Time: 0.88s\n","  🧪 Batch 32/55 — Loss: 0.0598 — Time: 0.89s\n","  🧪 Batch 33/55 — Loss: 0.0642 — Time: 0.89s\n","  🧪 Batch 34/55 — Loss: 0.0535 — Time: 0.89s\n","  🧪 Batch 35/55 — Loss: 0.0569 — Time: 0.89s\n","  🧪 Batch 36/55 — Loss: 0.0554 — Time: 0.89s\n","  🧪 Batch 37/55 — Loss: 0.0525 — Time: 0.89s\n","  🧪 Batch 38/55 — Loss: 0.0588 — Time: 0.89s\n","  🧪 Batch 39/55 — Loss: 0.0559 — Time: 0.89s\n","  🧪 Batch 40/55 — Loss: 0.0536 — Time: 0.89s\n","  🧪 Batch 41/55 — Loss: 0.0538 — Time: 0.89s\n","  🧪 Batch 42/55 — Loss: 0.0554 — Time: 0.89s\n","  🧪 Batch 43/55 — Loss: 0.0551 — Time: 0.89s\n","  🧪 Batch 44/55 — Loss: 0.0499 — Time: 0.89s\n","  🧪 Batch 45/55 — Loss: 0.0678 — Time: 0.88s\n","  🧪 Batch 46/55 — Loss: 0.0546 — Time: 0.88s\n","  🧪 Batch 47/55 — Loss: 0.0532 — Time: 0.89s\n","  🧪 Batch 48/55 — Loss: 0.0552 — Time: 0.89s\n","  🧪 Batch 49/55 — Loss: 0.0650 — Time: 0.89s\n","  🧪 Batch 50/55 — Loss: 0.0571 — Time: 0.89s\n","  🧪 Batch 51/55 — Loss: 0.0500 — Time: 0.89s\n","  🧪 Batch 52/55 — Loss: 0.0503 — Time: 0.89s\n","  🧪 Batch 53/55 — Loss: 0.0502 — Time: 0.89s\n","  🧪 Batch 54/55 — Loss: 0.0583 — Time: 0.89s\n","  🧪 Batch 55/55 — Loss: 0.0967 — Time: 0.65s\n","🕒 Epoch Time: 52.76s | Avg Batch Time: 0.88s\n","📊 Train Loss: 0.0586 | Val Loss: 0.2379 | IoU: 0.8011 | Dice: 0.8969 | Jaccard: 0.8160 | Precision: 0.9258 | Recall: 0.8731 | Accuracy: 0.9755\n","\n","Epoch 21/100\n","  🧪 Batch 1/55 — Loss: 0.0490 — Time: 0.89s\n","  🧪 Batch 2/55 — Loss: 0.0545 — Time: 0.88s\n","  🧪 Batch 3/55 — Loss: 0.0499 — Time: 0.89s\n","  🧪 Batch 4/55 — Loss: 0.0507 — Time: 0.89s\n","  🧪 Batch 5/55 — Loss: 0.0552 — Time: 0.88s\n","  🧪 Batch 6/55 — Loss: 0.0472 — Time: 0.88s\n","  🧪 Batch 7/55 — Loss: 0.0686 — Time: 0.89s\n","  🧪 Batch 8/55 — Loss: 0.0661 — Time: 0.88s\n","  🧪 Batch 9/55 — Loss: 0.0550 — Time: 0.89s\n","  🧪 Batch 10/55 — Loss: 0.0463 — Time: 0.89s\n","  🧪 Batch 11/55 — Loss: 0.0505 — Time: 0.89s\n","  🧪 Batch 12/55 — Loss: 0.0525 — Time: 0.89s\n","  🧪 Batch 13/55 — Loss: 0.0474 — Time: 0.89s\n","  🧪 Batch 14/55 — Loss: 0.0457 — Time: 0.89s\n","  🧪 Batch 15/55 — Loss: 0.0591 — Time: 0.89s\n","  🧪 Batch 16/55 — Loss: 0.0590 — Time: 0.88s\n","  🧪 Batch 17/55 — Loss: 0.0565 — Time: 0.88s\n","  🧪 Batch 18/55 — Loss: 0.0501 — Time: 0.89s\n","  🧪 Batch 19/55 — Loss: 0.0610 — Time: 0.89s\n","  🧪 Batch 20/55 — Loss: 0.0523 — Time: 0.88s\n","  🧪 Batch 21/55 — Loss: 0.0521 — Time: 0.88s\n","  🧪 Batch 22/55 — Loss: 0.0627 — Time: 0.89s\n","  🧪 Batch 23/55 — Loss: 0.0615 — Time: 0.89s\n","  🧪 Batch 24/55 — Loss: 0.0473 — Time: 0.89s\n","  🧪 Batch 25/55 — Loss: 0.0809 — Time: 0.89s\n","  🧪 Batch 26/55 — Loss: 0.0575 — Time: 0.89s\n","  🧪 Batch 27/55 — Loss: 0.0535 — Time: 0.89s\n","  🧪 Batch 28/55 — Loss: 0.0501 — Time: 0.89s\n","  🧪 Batch 29/55 — Loss: 0.0502 — Time: 0.89s\n","  🧪 Batch 30/55 — Loss: 0.0626 — Time: 0.88s\n","  🧪 Batch 31/55 — Loss: 0.0543 — Time: 0.89s\n","  🧪 Batch 32/55 — Loss: 0.0487 — Time: 0.89s\n","  🧪 Batch 33/55 — Loss: 0.0522 — Time: 0.89s\n","  🧪 Batch 34/55 — Loss: 0.0792 — Time: 0.89s\n","  🧪 Batch 35/55 — Loss: 0.0530 — Time: 0.89s\n","  🧪 Batch 36/55 — Loss: 0.0535 — Time: 0.89s\n","  🧪 Batch 37/55 — Loss: 0.0547 — Time: 0.89s\n","  🧪 Batch 38/55 — Loss: 0.0516 — Time: 0.88s\n","  🧪 Batch 39/55 — Loss: 0.0562 — Time: 0.89s\n","  🧪 Batch 40/55 — Loss: 0.0556 — Time: 0.89s\n","  🧪 Batch 41/55 — Loss: 0.0589 — Time: 0.89s\n","  🧪 Batch 42/55 — Loss: 0.0506 — Time: 0.89s\n","  🧪 Batch 43/55 — Loss: 0.0722 — Time: 0.89s\n","  🧪 Batch 44/55 — Loss: 0.0611 — Time: 0.89s\n","  🧪 Batch 45/55 — Loss: 0.0648 — Time: 0.89s\n","  🧪 Batch 46/55 — Loss: 0.0520 — Time: 0.88s\n","  🧪 Batch 47/55 — Loss: 0.0541 — Time: 0.89s\n","  🧪 Batch 48/55 — Loss: 0.0479 — Time: 0.89s\n","  🧪 Batch 49/55 — Loss: 0.0464 — Time: 0.89s\n","  🧪 Batch 50/55 — Loss: 0.0618 — Time: 0.89s\n","  🧪 Batch 51/55 — Loss: 0.0482 — Time: 0.89s\n","  🧪 Batch 52/55 — Loss: 0.0575 — Time: 0.89s\n","  🧪 Batch 53/55 — Loss: 0.0451 — Time: 0.89s\n","  🧪 Batch 54/55 — Loss: 0.0511 — Time: 0.89s\n","  🧪 Batch 55/55 — Loss: 0.0466 — Time: 0.65s\n","🕒 Epoch Time: 52.53s | Avg Batch Time: 0.88s\n","📊 Train Loss: 0.0551 | Val Loss: 0.2392 | IoU: 0.8002 | Dice: 0.8970 | Jaccard: 0.8165 | Precision: 0.9340 | Recall: 0.8658 | Accuracy: 0.9756\n","\n","Epoch 22/100\n","  🧪 Batch 1/55 — Loss: 0.0496 — Time: 0.87s\n","  🧪 Batch 2/55 — Loss: 0.0446 — Time: 0.89s\n","  🧪 Batch 3/55 — Loss: 0.0738 — Time: 0.89s\n","  🧪 Batch 4/55 — Loss: 0.0537 — Time: 0.89s\n","  🧪 Batch 5/55 — Loss: 0.0486 — Time: 0.89s\n","  🧪 Batch 6/55 — Loss: 0.0463 — Time: 0.89s\n","  🧪 Batch 7/55 — Loss: 0.0570 — Time: 0.88s\n","  🧪 Batch 8/55 — Loss: 0.0478 — Time: 0.89s\n","  🧪 Batch 9/55 — Loss: 0.0470 — Time: 0.89s\n","  🧪 Batch 10/55 — Loss: 0.0618 — Time: 0.89s\n","  🧪 Batch 11/55 — Loss: 0.0539 — Time: 0.89s\n","  🧪 Batch 12/55 — Loss: 0.0501 — Time: 0.89s\n","  🧪 Batch 13/55 — Loss: 0.0456 — Time: 0.89s\n","  🧪 Batch 14/55 — Loss: 0.0484 — Time: 0.89s\n","  🧪 Batch 15/55 — Loss: 0.0458 — Time: 0.89s\n","  🧪 Batch 16/55 — Loss: 0.0444 — Time: 0.89s\n","  🧪 Batch 17/55 — Loss: 0.0500 — Time: 0.89s\n","  🧪 Batch 18/55 — Loss: 0.0547 — Time: 0.89s\n","  🧪 Batch 19/55 — Loss: 0.0485 — Time: 0.89s\n","  🧪 Batch 20/55 — Loss: 0.0529 — Time: 0.89s\n","  🧪 Batch 21/55 — Loss: 0.0417 — Time: 0.89s\n","  🧪 Batch 22/55 — Loss: 0.0518 — Time: 0.89s\n","  🧪 Batch 23/55 — Loss: 0.0460 — Time: 0.89s\n","  🧪 Batch 24/55 — Loss: 0.0544 — Time: 0.89s\n","  🧪 Batch 25/55 — Loss: 0.0465 — Time: 0.89s\n","  🧪 Batch 26/55 — Loss: 0.0531 — Time: 0.89s\n","  🧪 Batch 27/55 — Loss: 0.0449 — Time: 0.89s\n","  🧪 Batch 28/55 — Loss: 0.0579 — Time: 0.89s\n","  🧪 Batch 29/55 — Loss: 0.0526 — Time: 0.89s\n","  🧪 Batch 30/55 — Loss: 0.0562 — Time: 0.89s\n","  🧪 Batch 31/55 — Loss: 0.0457 — Time: 0.88s\n","  🧪 Batch 32/55 — Loss: 0.0715 — Time: 0.88s\n","  🧪 Batch 33/55 — Loss: 0.0454 — Time: 0.89s\n","  🧪 Batch 34/55 — Loss: 0.0535 — Time: 0.89s\n","  🧪 Batch 35/55 — Loss: 0.0477 — Time: 0.89s\n","  🧪 Batch 36/55 — Loss: 0.0520 — Time: 0.89s\n","  🧪 Batch 37/55 — Loss: 0.0631 — Time: 0.89s\n","  🧪 Batch 38/55 — Loss: 0.0458 — Time: 0.89s\n","  🧪 Batch 39/55 — Loss: 0.0566 — Time: 0.89s\n","  🧪 Batch 40/55 — Loss: 0.0584 — Time: 0.89s\n","  🧪 Batch 41/55 — Loss: 0.0498 — Time: 0.89s\n","  🧪 Batch 42/55 — Loss: 0.0443 — Time: 0.89s\n","  🧪 Batch 43/55 — Loss: 0.0436 — Time: 0.89s\n","  🧪 Batch 44/55 — Loss: 0.0589 — Time: 0.89s\n","  🧪 Batch 45/55 — Loss: 0.0501 — Time: 0.89s\n","  🧪 Batch 46/55 — Loss: 0.0427 — Time: 0.89s\n","  🧪 Batch 47/55 — Loss: 0.0535 — Time: 0.88s\n","  🧪 Batch 48/55 — Loss: 0.0474 — Time: 0.87s\n","  🧪 Batch 49/55 — Loss: 0.0585 — Time: 0.89s\n","  🧪 Batch 50/55 — Loss: 0.0440 — Time: 0.89s\n","  🧪 Batch 51/55 — Loss: 0.0485 — Time: 0.89s\n","  🧪 Batch 52/55 — Loss: 0.0489 — Time: 0.89s\n","  🧪 Batch 53/55 — Loss: 0.0499 — Time: 0.89s\n","  🧪 Batch 54/55 — Loss: 0.0469 — Time: 0.89s\n","  🧪 Batch 55/55 — Loss: 0.0569 — Time: 0.65s\n","🕒 Epoch Time: 52.59s | Avg Batch Time: 0.88s\n","📊 Train Loss: 0.0512 | Val Loss: 0.2400 | IoU: 0.8000 | Dice: 0.8963 | Jaccard: 0.8149 | Precision: 0.9286 | Recall: 0.8693 | Accuracy: 0.9754\n","\n","Epoch 23/100\n","  🧪 Batch 1/55 — Loss: 0.0608 — Time: 0.89s\n","  🧪 Batch 2/55 — Loss: 0.0472 — Time: 0.89s\n","  🧪 Batch 3/55 — Loss: 0.0553 — Time: 0.89s\n","  🧪 Batch 4/55 — Loss: 0.0565 — Time: 0.88s\n","  🧪 Batch 5/55 — Loss: 0.0503 — Time: 0.89s\n","  🧪 Batch 6/55 — Loss: 0.0557 — Time: 0.89s\n","  🧪 Batch 7/55 — Loss: 0.0479 — Time: 0.89s\n","  🧪 Batch 8/55 — Loss: 0.0398 — Time: 0.89s\n","  🧪 Batch 9/55 — Loss: 0.0584 — Time: 0.89s\n","  🧪 Batch 10/55 — Loss: 0.0499 — Time: 0.89s\n","  🧪 Batch 11/55 — Loss: 0.0432 — Time: 0.89s\n","  🧪 Batch 12/55 — Loss: 0.0431 — Time: 0.89s\n","  🧪 Batch 13/55 — Loss: 0.0452 — Time: 0.89s\n","  🧪 Batch 14/55 — Loss: 0.0449 — Time: 0.89s\n","  🧪 Batch 15/55 — Loss: 0.0490 — Time: 0.89s\n","  🧪 Batch 16/55 — Loss: 0.0434 — Time: 0.89s\n","  🧪 Batch 17/55 — Loss: 0.0438 — Time: 0.88s\n","  🧪 Batch 18/55 — Loss: 0.0484 — Time: 0.89s\n","  🧪 Batch 19/55 — Loss: 0.0465 — Time: 0.89s\n","  🧪 Batch 20/55 — Loss: 0.0448 — Time: 0.88s\n","  🧪 Batch 21/55 — Loss: 0.0409 — Time: 0.88s\n","  🧪 Batch 22/55 — Loss: 0.0507 — Time: 0.88s\n","  🧪 Batch 23/55 — Loss: 0.0412 — Time: 0.88s\n","  🧪 Batch 24/55 — Loss: 0.0593 — Time: 0.89s\n","  🧪 Batch 25/55 — Loss: 0.0471 — Time: 0.89s\n","  🧪 Batch 26/55 — Loss: 0.0459 — Time: 0.89s\n","  🧪 Batch 27/55 — Loss: 0.0463 — Time: 0.89s\n","  🧪 Batch 28/55 — Loss: 0.0564 — Time: 0.89s\n","  🧪 Batch 29/55 — Loss: 0.0477 — Time: 0.89s\n","  🧪 Batch 30/55 — Loss: 0.0454 — Time: 0.89s\n","  🧪 Batch 31/55 — Loss: 0.0411 — Time: 0.89s\n","  🧪 Batch 32/55 — Loss: 0.0502 — Time: 0.89s\n","  🧪 Batch 33/55 — Loss: 0.0505 — Time: 0.88s\n","  🧪 Batch 34/55 — Loss: 0.0494 — Time: 0.88s\n","  🧪 Batch 35/55 — Loss: 0.0488 — Time: 0.88s\n","  🧪 Batch 36/55 — Loss: 0.0531 — Time: 0.88s\n","  🧪 Batch 37/55 — Loss: 0.0450 — Time: 0.89s\n","  🧪 Batch 38/55 — Loss: 0.0462 — Time: 0.89s\n","  🧪 Batch 39/55 — Loss: 0.0630 — Time: 0.89s\n","  🧪 Batch 40/55 — Loss: 0.0415 — Time: 0.88s\n","  🧪 Batch 41/55 — Loss: 0.0509 — Time: 0.88s\n","  🧪 Batch 42/55 — Loss: 0.0473 — Time: 0.89s\n","  🧪 Batch 43/55 — Loss: 0.0427 — Time: 0.89s\n","  🧪 Batch 44/55 — Loss: 0.0475 — Time: 0.89s\n","  🧪 Batch 45/55 — Loss: 0.0479 — Time: 0.89s\n","  🧪 Batch 46/55 — Loss: 0.0441 — Time: 0.89s\n","  🧪 Batch 47/55 — Loss: 0.0453 — Time: 0.89s\n","  🧪 Batch 48/55 — Loss: 0.0518 — Time: 0.89s\n","  🧪 Batch 49/55 — Loss: 0.0453 — Time: 0.88s\n","  🧪 Batch 50/55 — Loss: 0.0514 — Time: 0.88s\n","  🧪 Batch 51/55 — Loss: 0.0433 — Time: 0.88s\n","  🧪 Batch 52/55 — Loss: 0.0541 — Time: 0.89s\n","  🧪 Batch 53/55 — Loss: 0.0464 — Time: 0.88s\n","  🧪 Batch 54/55 — Loss: 0.0454 — Time: 0.88s\n","  🧪 Batch 55/55 — Loss: 0.0491 — Time: 0.65s\n","🕒 Epoch Time: 52.04s | Avg Batch Time: 0.88s\n","📊 Train Loss: 0.0483 | Val Loss: 0.2407 | IoU: 0.8046 | Dice: 0.8974 | Jaccard: 0.8168 | Precision: 0.9252 | Recall: 0.8742 | Accuracy: 0.9755\n","⏹️ Early stopping triggered after 23 epochs.\n","\n","🧪 Testing model trained on CombinedDataset on all datasets...\n","| Trained On      | Tested On       |     Loss |      IoU |     Dice |   Jaccard |   Precision |   Recall |   Accuracy |\n","|:----------------|:----------------|---------:|---------:|---------:|----------:|------------:|---------:|-----------:|\n","| CombinedDataset | CombinedDataset | 0.171507 | 0.827139 | 0.924821 |  0.860602 |    0.949715 | 0.901897 |    0.98288 |\n","\n","⏱️ Total Time for Training + Testing: 1208.55 seconds\n","\n","📋 Final Cross-Dataset Testing Summary:\n","| Trained On      | Tested On       |     Loss |      IoU |     Dice |   Jaccard |   Precision |   Recall |   Accuracy |\n","|:----------------|:----------------|---------:|---------:|---------:|----------:|------------:|---------:|-----------:|\n","| CombinedDataset | CombinedDataset | 0.171507 | 0.827139 | 0.924821 |  0.860602 |    0.949715 | 0.901897 |    0.98288 |\n","U-Net Plus Plus Results\n","        Trained On        Tested On      Loss       IoU      Dice   Jaccard  \\\n","0  CombinedDataset  CombinedDataset  0.171507  0.827139  0.924821  0.860602   \n","\n","   Precision    Recall  Accuracy  \n","0   0.949715  0.901897   0.98288  \n"]}]},{"cell_type":"code","source":["evaluation_df = evaluate_model_on_datasets(trained_model, separate_loaders)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"20Wh6megBi3L","executionInfo":{"status":"ok","timestamp":1745270775387,"user_tz":240,"elapsed":4191,"user":{"displayName":"Ashok Kamath","userId":"01269285531323045015"}},"outputId":"57b15fb5-99f2-4008-c49d-042e22fce14e"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","🧪 Evaluating trained model on all test sets...\n","\n","🔍 Testing on: CVC-ClinicDB\n","🔍 Testing on: CVC-ColonDB\n","🔍 Testing on: ETIS-LaribPolypDB\n","🔍 Testing on: Kvasir-SEG\n","\n","⏱️ Total Evaluation Time: 4.15 seconds\n","\n","📋 Test Results Across Datasets (with Average):\n","| Tested On         |     Loss |      IoU |     Dice |   Jaccard |   Precision |   Recall |   Accuracy |\n","|:------------------|---------:|---------:|---------:|----------:|------------:|---------:|-----------:|\n","| CVC-ClinicDB      | 0.121548 | 0.854252 | 0.942273 |  0.891018 |    0.950179 | 0.935242 |   0.989891 |\n","| CVC-ColonDB       | 0.146963 | 0.76528  | 0.920163 |  0.853685 |    0.94232  | 0.901489 |   0.992375 |\n","| ETIS-LaribPolypDB | 0.500283 | 0.486214 | 0.601018 |  0.507218 |    0.97153  | 0.528764 |   0.986279 |\n","| Kvasir-SEG        | 0.210686 | 0.864189 | 0.915843 |  0.845795 |    0.947629 | 0.888326 |   0.975459 |\n","| Average           | 0.24487  | 0.742484 | 0.844824 |  0.774429 |    0.952914 | 0.813456 |   0.986001 |\n"]}]},{"cell_type":"markdown","source":["# DeepLabV3"],"metadata":{"id":"JpkFiTqDZtoq"}},{"cell_type":"code","source":["def deeplabv3_model_factory():\n","    return smp.DeepLabV3(\n","        encoder_name=\"resnet34\",\n","        encoder_weights=\"imagenet\",\n","        in_channels=3,\n","        classes=1,\n","    )"],"metadata":{"id":"GNOe1unSZK9h"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"wh_GhcBrkXOH"},"source":["# BetterNet"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"J6zpk_KakVra"},"outputs":[],"source":["import sys\n","sys.path.append(\"/content/drive/My Drive/GT/DL/GroupProject/BetterNet\")\n","\n","# sys.path.append(\"../BetterNet\")  # add the BetterNet folder to the import path\n","\n","from model import BetterNet  # import the model definition\n","\n","# Define model parameters\n","params = {\n","    \"img_height\": 224,\n","    \"img_width\": 224,\n","    \"img_channels\": 3,\n","    \"mask_channels\": 1  # binary segmentation\n","}\n","\n","# Initialize the BetterNet model\n","model = BetterNet(\n","    input_shape=(params[\"img_height\"], params[\"img_width\"], params[\"img_channels\"]),\n","    num_classes=params[\"mask_channels\"],\n","    dropout_rate=0.5\n",")\n","\n","# Move to GPU if available\n","model = model.cuda() if torch.cuda.is_available() else model\n"]}],"metadata":{"colab":{"provenance":[],"gpuType":"T4","authorship_tag":"ABX9TyMns9O41Oo17f13DK4PzMxk"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"},"accelerator":"GPU"},"nbformat":4,"nbformat_minor":0}