{"cells":[{"cell_type":"code","source":["!pip install segmentation-models-pytorch==0.3.3 --quiet\n","!pip install --upgrade torch torchvision --quiet\n","\n","import torch\n","import segmentation_models_pytorch as smp\n","from torch import nn\n","\n","# Recommended safety flags\n","import torch.backends.cudnn as cudnn\n","cudnn.enabled = True\n","cudnn.benchmark = False\n","cudnn.deterministic = True\n","\n","# Device\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","# Recreate model AFTER setting device\n","model = smp.Unet(\n","    encoder_name=\"resnet34\",\n","    encoder_weights=\"imagenet\",\n","    in_channels=3,\n","    classes=1,\n",")\n","model = model.to(device)\n"],"metadata":{"id":"2VFz6My5MYtN","executionInfo":{"status":"ok","timestamp":1745351170950,"user_tz":240,"elapsed":124930,"user":{"displayName":"Ashok Kamath","userId":"01269285531323045015"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"244253aa-c40e-4353-bfff-5967633efdaf"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[?25l     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m0.0/58.8 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m58.8/58.8 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m68.5/68.5 kB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m106.7/106.7 kB\u001b[0m \u001b[31m9.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m2.2/2.2 MB\u001b[0m \u001b[31m58.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m83.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m85.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m44.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m12.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m11.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m53.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Building wheel for efficientnet-pytorch (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Building wheel for pretrainedmodels (setup.py) ... \u001b[?25l\u001b[?25hdone\n"]},{"output_type":"stream","name":"stderr","text":["Downloading: \"https://download.pytorch.org/models/resnet34-333f7ec4.pth\" to /root/.cache/torch/hub/checkpoints/resnet34-333f7ec4.pth\n","100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 83.3M/83.3M [00:00<00:00, 272MB/s]\n"]}]},{"cell_type":"code","execution_count":5,"metadata":{"executionInfo":{"elapsed":576,"status":"ok","timestamp":1745351171536,"user":{"displayName":"Ashok Kamath","userId":"01269285531323045015"},"user_tz":240},"id":"H-XZJlmUSV_y"},"outputs":[],"source":["import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","from torch.utils.data import DataLoader\n","from torchvision import transforms\n","from tqdm import tqdm\n","import numpy as np\n","import os\n","import pandas as pd\n"]},{"cell_type":"code","source":["# Dummy tensor\n","x = torch.randn(1, 3, 256, 256).to(device)\n","with torch.no_grad():\n","    y = model(x)\n","print(\"Output shape:\", y.shape)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"D7WIPOlIMeXK","executionInfo":{"status":"ok","timestamp":1745351172187,"user_tz":240,"elapsed":653,"user":{"displayName":"Ashok Kamath","userId":"01269285531323045015"}},"outputId":"11c888c6-cdec-4f6e-e6d2-442bf35beaff"},"execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":["Output shape: torch.Size([1, 1, 256, 256])\n"]}]},{"cell_type":"code","execution_count":7,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":20,"status":"ok","timestamp":1745351172208,"user":{"displayName":"Ashok Kamath","userId":"01269285531323045015"},"user_tz":240},"id":"mh1ypB2bYeZW","outputId":"905fde76-b548-4156-8d7d-bf80e9dac44c"},"outputs":[{"output_type":"stream","name":"stdout","text":["CUDA Available: True\n","cuDNN Version: 90100\n"]}],"source":["import torch\n","print(\"CUDA Available:\", torch.cuda.is_available())\n","print(\"cuDNN Version:\", torch.backends.cudnn.version())\n"]},{"cell_type":"code","execution_count":8,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":24444,"status":"ok","timestamp":1745351196653,"user":{"displayName":"Ashok Kamath","userId":"01269285531323045015"},"user_tz":240},"id":"-fSjFnTcwRyW","outputId":"c7a2f434-9331-4ce0-8911-4f4bcab4a1a7"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n","\n","ğŸ“‚ DATASET: CVC-ClinicDB\n","TRAIN IMAGES PATH: /content/drive/My Drive/GT/DL/GroupProject/Datasets/CVC-ClinicDB/train/images\n","TRAIN MASKS PATH:  /content/drive/My Drive/GT/DL/GroupProject/Datasets/CVC-ClinicDB/train/masks\n","VAL IMAGES PATH: /content/drive/My Drive/GT/DL/GroupProject/Datasets/CVC-ClinicDB/validation/images\n","VAL MASKS PATH:  /content/drive/My Drive/GT/DL/GroupProject/Datasets/CVC-ClinicDB/validation/masks\n","TEST IMAGES PATH: /content/drive/My Drive/GT/DL/GroupProject/Datasets/CVC-ClinicDB/test/images\n","TEST MASKS PATH:  /content/drive/My Drive/GT/DL/GroupProject/Datasets/CVC-ClinicDB/test/masks\n","\n","ğŸ“‚ DATASET: CVC-ColonDB\n","TRAIN IMAGES PATH: /content/drive/My Drive/GT/DL/GroupProject/Datasets/CVC-ColonDB/train/images\n","TRAIN MASKS PATH:  /content/drive/My Drive/GT/DL/GroupProject/Datasets/CVC-ColonDB/train/masks\n","VAL IMAGES PATH: /content/drive/My Drive/GT/DL/GroupProject/Datasets/CVC-ColonDB/validation/images\n","VAL MASKS PATH:  /content/drive/My Drive/GT/DL/GroupProject/Datasets/CVC-ColonDB/validation/masks\n","TEST IMAGES PATH: /content/drive/My Drive/GT/DL/GroupProject/Datasets/CVC-ColonDB/test/images\n","TEST MASKS PATH:  /content/drive/My Drive/GT/DL/GroupProject/Datasets/CVC-ColonDB/test/masks\n","\n","ğŸ“‚ DATASET: ETIS-LaribPolypDB\n","TRAIN IMAGES PATH: /content/drive/My Drive/GT/DL/GroupProject/Datasets/ETIS-LaribPolypDB/train/images\n","TRAIN MASKS PATH:  /content/drive/My Drive/GT/DL/GroupProject/Datasets/ETIS-LaribPolypDB/train/masks\n","VAL IMAGES PATH: /content/drive/My Drive/GT/DL/GroupProject/Datasets/ETIS-LaribPolypDB/validation/images\n","VAL MASKS PATH:  /content/drive/My Drive/GT/DL/GroupProject/Datasets/ETIS-LaribPolypDB/validation/masks\n","TEST IMAGES PATH: /content/drive/My Drive/GT/DL/GroupProject/Datasets/ETIS-LaribPolypDB/test/images\n","TEST MASKS PATH:  /content/drive/My Drive/GT/DL/GroupProject/Datasets/ETIS-LaribPolypDB/test/masks\n","\n","ğŸ“‚ DATASET: Kvasir-SEG\n","TRAIN IMAGES PATH: /content/drive/My Drive/GT/DL/GroupProject/Datasets/Kvasir-SEG/train/images\n","TRAIN MASKS PATH:  /content/drive/My Drive/GT/DL/GroupProject/Datasets/Kvasir-SEG/train/masks\n","VAL IMAGES PATH: /content/drive/My Drive/GT/DL/GroupProject/Datasets/Kvasir-SEG/validation/images\n","VAL MASKS PATH:  /content/drive/My Drive/GT/DL/GroupProject/Datasets/Kvasir-SEG/validation/masks\n","TEST IMAGES PATH: /content/drive/My Drive/GT/DL/GroupProject/Datasets/Kvasir-SEG/test/images\n","TEST MASKS PATH:  /content/drive/My Drive/GT/DL/GroupProject/Datasets/Kvasir-SEG/test/masks\n"]}],"source":["# ğŸ“¦ Mount Google Drive\n","from google.colab import drive\n","drive.mount('/content/drive')\n","\n","# ğŸ›¤ï¸ Set base path and dataset names\n","import os\n","dataset_names = ['CVC-ClinicDB', 'CVC-ColonDB', 'ETIS-LaribPolypDB', 'Kvasir-SEG']\n","base_path = '/content/drive/My Drive/GT/DL/GroupProject/Datasets/'\n","\n","# ğŸ“ Build paths for each dataset\n","all_dataset_paths = []\n","\n","for dataset in dataset_names:\n","    dataset_path = os.path.join(base_path, dataset)\n","    paths = {\n","        'name': dataset,\n","        'train': {\n","            'images': os.path.join(dataset_path, 'train', 'images'),\n","            'masks': os.path.join(dataset_path, 'train', 'masks')\n","        },\n","        'val': {\n","            'images': os.path.join(dataset_path, 'validation', 'images'),\n","            'masks': os.path.join(dataset_path, 'validation', 'masks')\n","        },\n","        'test': {\n","            'images': os.path.join(dataset_path, 'test', 'images'),\n","            'masks': os.path.join(dataset_path, 'test', 'masks')\n","        }\n","    }\n","    all_dataset_paths.append(paths)\n","\n","# âœ… Print all paths for each dataset\n","for dataset in all_dataset_paths:\n","    print(f\"\\nğŸ“‚ DATASET: {dataset['name']}\")\n","    for split in ['train', 'val', 'test']:\n","        print(f\"{split.upper()} IMAGES PATH: {dataset[split]['images']}\")\n","        print(f\"{split.upper()} MASKS PATH:  {dataset[split]['masks']}\")\n"]},{"cell_type":"code","execution_count":9,"metadata":{"executionInfo":{"elapsed":0,"status":"ok","timestamp":1745351196655,"user":{"displayName":"Ashok Kamath","userId":"01269285531323045015"},"user_tz":240},"id":"cm6CKpFkOQaT","collapsed":true},"outputs":[],"source":["# import cv2\n","# import os\n","# import matplotlib.pyplot as plt\n","\n","# # Visualize sample pairs for each split\n","# for dataset in all_dataset_paths:\n","#   for split in ['train', 'val', 'test']:\n","#       print(f\"\\nğŸ“‚ DATASET: {dataset['name']}\")\n","\n","#       image_dir = dataset[split]['images']\n","#       mask_dir = dataset[split]['masks']\n","\n","#       # Get sorted file lists\n","#       image_files = sorted(os.listdir(image_dir))\n","#       mask_files = sorted(os.listdir(mask_dir))\n","\n","#       # Pick a sample index safely (in case folders are small)\n","#       sample_idx = min(30, len(image_files) - 1)\n","\n","#       # Read image and mask\n","#       image_path = os.path.join(image_dir, image_files[sample_idx])\n","#       mask_path = os.path.join(mask_dir, mask_files[sample_idx])\n","\n","#       image = cv2.imread(image_path)\n","#       image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n","\n","#       mask = cv2.imread(mask_path, cv2.IMREAD_GRAYSCALE)\n","\n","#       # Plot\n","#       plt.figure(figsize=(10, 4))\n","#       plt.suptitle(f\"{split.upper()} SET SAMPLE\", fontsize=14)\n","\n","#       plt.subplot(1, 2, 1)\n","#       plt.imshow(image)\n","#       plt.title(\"Image\")\n","#       plt.axis(\"off\")\n","\n","#       plt.subplot(1, 2, 2)\n","#       plt.imshow(mask, cmap='gray')\n","#       plt.title(\"Mask\")\n","#       plt.axis(\"off\")\n","\n","#       plt.show()\n"]},{"cell_type":"code","execution_count":10,"metadata":{"executionInfo":{"elapsed":12,"status":"ok","timestamp":1745351196670,"user":{"displayName":"Ashok Kamath","userId":"01269285531323045015"},"user_tz":240},"id":"3qcODMPbQM_T"},"outputs":[],"source":["import os\n","from torch.utils.data import Dataset\n","from PIL import Image\n","\n","class PolypSegmentationDataset(Dataset):\n","    def __init__(self, images_dir, masks_dir, transform=None, mask_transform=None):\n","        self.images_dir = images_dir\n","        self.masks_dir = masks_dir\n","        self.image_filenames = sorted(os.listdir(images_dir))\n","        self.mask_filenames = sorted(os.listdir(masks_dir))\n","        self.transform = transform\n","        self.mask_transform = mask_transform\n","\n","        assert len(self.image_filenames) == len(self.mask_filenames), \\\n","            \"Mismatch between number of images and masks.\"\n","\n","    def __len__(self):\n","        return len(self.image_filenames)\n","\n","    def __getitem__(self, idx):\n","        # Load image and mask\n","        img_path = os.path.join(self.images_dir, self.image_filenames[idx])\n","        mask_path = os.path.join(self.masks_dir, self.mask_filenames[idx])\n","\n","        image = Image.open(img_path).convert(\"RGB\")\n","        mask = Image.open(mask_path).convert(\"L\")  # grayscale mask\n","\n","        # Apply transformations\n","        if self.transform:\n","            image = self.transform(image)\n","        if self.mask_transform:\n","            mask = self.mask_transform(mask)\n","\n","        return image, mask\n"]},{"cell_type":"code","execution_count":11,"metadata":{"executionInfo":{"elapsed":0,"status":"ok","timestamp":1745351196671,"user":{"displayName":"Ashok Kamath","userId":"01269285531323045015"},"user_tz":240},"id":"eIWxpGqESTHv"},"outputs":[],"source":["image_transform = transforms.Compose([\n","    transforms.Resize((256, 256)),\n","    transforms.ToTensor(),\n","])\n","\n","mask_transform = transforms.Compose([\n","    transforms.Resize((256, 256)),\n","    transforms.ToTensor(),  # Grayscale mask in [0,1]\n","])\n"]},{"cell_type":"code","source":["from torch.utils.data import DataLoader, ConcatDataset\n","\n","def get_data_loaders(all_dataset_paths, image_transform, mask_transform, batch_size=16, num_workers=2, pin_memory=True, combine_datasets=False):\n","    \"\"\"\n","    Returns dataloaders either per dataset or combined, depending on `combine_datasets`.\n","\n","    Args:\n","        all_dataset_paths: list of dicts with dataset paths.\n","        image_transform: transform function for input images.\n","        mask_transform: transform function for target masks.\n","        batch_size: batch size for loaders.\n","        num_workers: number of subprocesses to use for data loading.\n","        pin_memory: whether to pin memory during data transfer to GPU.\n","        combine_datasets: if True, returns combined loaders; otherwise returns one set per dataset.\n","\n","    Returns:\n","        List of loaders or single dictionary of loaders (if combined).\n","    \"\"\"\n","    if combine_datasets:\n","        train_datasets, val_datasets, test_datasets = [], [], []\n","\n","        for dataset_paths in all_dataset_paths:\n","            train_datasets.append(PolypSegmentationDataset(\n","                dataset_paths['train']['images'],\n","                dataset_paths['train']['masks'],\n","                image_transform,\n","                mask_transform\n","            ))\n","            val_datasets.append(PolypSegmentationDataset(\n","                dataset_paths['val']['images'],\n","                dataset_paths['val']['masks'],\n","                image_transform,\n","                mask_transform\n","            ))\n","            test_datasets.append(PolypSegmentationDataset(\n","                dataset_paths['test']['images'],\n","                dataset_paths['test']['masks'],\n","                image_transform,\n","                mask_transform\n","            ))\n","\n","        train_dataset = ConcatDataset(train_datasets)\n","        val_dataset = ConcatDataset(val_datasets)\n","        test_dataset = ConcatDataset(test_datasets)\n","\n","        loaders = {\n","            'name': 'CombinedDataset',\n","            'train_dataset': train_dataset,\n","            'val_dataset': val_dataset,\n","            'test_dataset': test_dataset,\n","            'train_loader': DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=num_workers, pin_memory=pin_memory),\n","            'val_loader': DataLoader(val_dataset, batch_size=batch_size, num_workers=num_workers, pin_memory=pin_memory),\n","            'test_loader': DataLoader(test_dataset, batch_size=batch_size, num_workers=num_workers, pin_memory=pin_memory)\n","        }\n","\n","        # âœ… Print summary for combined dataset\n","        print(f\"\\nâœ… Combined dataset loaded:\")\n","        print(f\"Train samples: {len(train_dataset)}\")\n","        print(f\"Val samples:   {len(val_dataset)}\")\n","        print(f\"Test samples:  {len(test_dataset)}\")\n","\n","        return loaders\n","\n","    else:\n","        all_data_loaders = []\n","\n","        for dataset_paths in all_dataset_paths:\n","            dataset_name = dataset_paths['name']\n","\n","            train_dataset = PolypSegmentationDataset(\n","                dataset_paths['train']['images'],\n","                dataset_paths['train']['masks'],\n","                image_transform,\n","                mask_transform\n","            )\n","            val_dataset = PolypSegmentationDataset(\n","                dataset_paths['val']['images'],\n","                dataset_paths['val']['masks'],\n","                image_transform,\n","                mask_transform\n","            )\n","            test_dataset = PolypSegmentationDataset(\n","                dataset_paths['test']['images'],\n","                dataset_paths['test']['masks'],\n","                image_transform,\n","                mask_transform\n","            )\n","\n","            loaders = {\n","                'name': dataset_name,\n","                'train_dataset': train_dataset,\n","                'val_dataset': val_dataset,\n","                'test_dataset': test_dataset,\n","                'train_loader': DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=num_workers, pin_memory=pin_memory),\n","                'val_loader': DataLoader(val_dataset, batch_size=batch_size, num_workers=num_workers, pin_memory=pin_memory),\n","                'test_loader': DataLoader(test_dataset, batch_size=batch_size, num_workers=num_workers, pin_memory=pin_memory)\n","            }\n","\n","            all_data_loaders.append(loaders)\n","\n","            # âœ… Print summary per dataset\n","            print(f\"\\nğŸ“¦ {dataset_name} loaded:\")\n","            print(f\"Train samples: {len(train_dataset)}\")\n","            print(f\"Val samples:   {len(val_dataset)}\")\n","            print(f\"Test samples:  {len(test_dataset)}\")\n","\n","        return all_data_loaders\n"],"metadata":{"id":"0GKmqrE3dcQX","executionInfo":{"status":"ok","timestamp":1745351196981,"user_tz":240,"elapsed":309,"user":{"displayName":"Ashok Kamath","userId":"01269285531323045015"}}},"execution_count":12,"outputs":[]},{"cell_type":"code","source":["combined_loaders = get_data_loaders(all_dataset_paths, image_transform, mask_transform,batch_size = 32, combine_datasets=True)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Cdyg3GybeI5L","executionInfo":{"status":"ok","timestamp":1745351216935,"user_tz":240,"elapsed":19955,"user":{"displayName":"Ashok Kamath","userId":"01269285531323045015"}},"outputId":"b53b6124-53e3-4d59-ed3c-62766878058d"},"execution_count":13,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","âœ… Combined dataset loaded:\n","Train samples: 1748\n","Val samples:   220\n","Test samples:  220\n"]}]},{"cell_type":"code","source":["separate_loaders = get_data_loaders(all_dataset_paths, image_transform, mask_transform, combine_datasets=False)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"JQ2rcTfpezzY","executionInfo":{"status":"ok","timestamp":1745351216940,"user_tz":240,"elapsed":4,"user":{"displayName":"Ashok Kamath","userId":"01269285531323045015"}},"outputId":"92026385-d5ac-4966-82bf-e38766de1ff1"},"execution_count":14,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","ğŸ“¦ CVC-ClinicDB loaded:\n","Train samples: 488\n","Val samples:   62\n","Test samples:  62\n","\n","ğŸ“¦ CVC-ColonDB loaded:\n","Train samples: 304\n","Val samples:   38\n","Test samples:  38\n","\n","ğŸ“¦ ETIS-LaribPolypDB loaded:\n","Train samples: 156\n","Val samples:   20\n","Test samples:  20\n","\n","ğŸ“¦ Kvasir-SEG loaded:\n","Train samples: 800\n","Val samples:   100\n","Test samples:  100\n"]}]},{"cell_type":"code","execution_count":15,"metadata":{"executionInfo":{"elapsed":51,"status":"ok","timestamp":1745351216995,"user":{"displayName":"Ashok Kamath","userId":"01269285531323045015"},"user_tz":240},"id":"PgSaBshOTIz9","colab":{"base_uri":"https://localhost:8080/"},"outputId":"87adff71-0739-4851-c1c4-8e6a37182392"},"outputs":[{"output_type":"stream","name":"stdout","text":["\n","ğŸ§  Combined Train Dataset: 1748 samples\n","\n","ğŸ“¦ CVC-ClinicDB loaded:\n","Train samples: 488\n","Val samples:   62\n","Test samples:  62\n","\n","ğŸ“¦ CVC-ColonDB loaded:\n","Train samples: 304\n","Val samples:   38\n","Test samples:  38\n","\n","ğŸ“¦ ETIS-LaribPolypDB loaded:\n","Train samples: 156\n","Val samples:   20\n","Test samples:  20\n","\n","ğŸ“¦ Kvasir-SEG loaded:\n","Train samples: 800\n","Val samples:   100\n","Test samples:  100\n"]}],"source":["from torch.utils.data import DataLoader, ConcatDataset\n","\n","# ğŸ§  Assume: all_dataset_paths is already defined as in the previous cell\n","# and image_transform, mask_transform are also defined\n","\n","combined_train_datasets = []\n","all_data_loaders = []\n","\n","for dataset_paths in all_dataset_paths:\n","    dataset_name = dataset_paths['name']\n","\n","    # Create datasets\n","    train_dataset = PolypSegmentationDataset(\n","        dataset_paths['train']['images'],\n","        dataset_paths['train']['masks'],\n","        image_transform,\n","        mask_transform\n","    )\n","    val_dataset = PolypSegmentationDataset(\n","        dataset_paths['val']['images'],\n","        dataset_paths['val']['masks'],\n","        image_transform,\n","        mask_transform\n","    )\n","    test_dataset = PolypSegmentationDataset(\n","        dataset_paths['test']['images'],\n","        dataset_paths['test']['masks'],\n","        image_transform,\n","        mask_transform\n","    )\n","\n","    combined_train_datasets.append(train_dataset)\n","\n","    # Create dataloaders\n","    loaders = {\n","        'name': dataset_name,\n","        'train_dataset': train_dataset,\n","        'val_dataset': val_dataset,\n","        'test_dataset': test_dataset,\n","        'train_loader': DataLoader(train_dataset, batch_size=16, shuffle=True, num_workers = 2, pin_memory = True),\n","        'val_loader': DataLoader(val_dataset, batch_size=16, num_workers = 2, pin_memory = True),\n","        'test_loader': DataLoader(test_dataset, batch_size=16, num_workers = 2, pin_memory = True)\n","    }\n","\n","    all_data_loaders.append(loaders)\n","\n","full_train_dataset = ConcatDataset(combined_train_datasets)\n","train_loader = DataLoader(full_train_dataset, batch_size=16, shuffle=True, num_workers=2, pin_memory=True)\n","\n","#all_data_loaders.append(train_loader)\n","print(f\"\\nğŸ§  Combined Train Dataset: {len(full_train_dataset)} samples\")\n","\n","# âœ… Print confirmation for each dataset\n","for loaders in all_data_loaders:\n","    print(f\"\\nğŸ“¦ {loaders['name']} loaded:\")\n","    print(f\"Train samples: {len(loaders['train_dataset'])}\")\n","    print(f\"Val samples:   {len(loaders['val_dataset'])}\")\n","    print(f\"Test samples:  {len(loaders['test_dataset'])}\")\n"]},{"cell_type":"markdown","metadata":{"id":"pJ4lhYqMkJGD"},"source":["# U-Net\n","\n"]},{"cell_type":"code","execution_count":17,"metadata":{"executionInfo":{"elapsed":6,"status":"ok","timestamp":1745351219591,"user":{"displayName":"Ashok Kamath","userId":"01269285531323045015"},"user_tz":240},"id":"064Js6zOT7n3"},"outputs":[],"source":["# Dice + BCE Loss\n","# dice loss is based on dice coefficient, which is a measure of overlap between two samples\n","#  - commonly used in image segmentation tasks\n","#  - Dice loss = 1 - Dice Coefficient\n","#         - Dice coefficient ranges from 0 (no overlap) to 1 (perfect overlap)\n","#\n","def dice_loss(pred, target, smooth=1.):\n","    #\n","    pred = torch.sigmoid(pred).view(-1)\n","\n","    # flatten both tensors to prep them for comparison\n","    pred = pred.view(-1)\n","    target = target.view(-1)\n","\n","    intersection = (pred * target).sum() # pixel wise product\n","    return 1 - ((2. * intersection + smooth) / (pred.sum() + target.sum() + smooth))\n","\n","# binary cross entropy since this is a segmentation task\n","bce_loss = nn.BCEWithLogitsLoss()\n","\n","def combined_loss(pred, target):\n","    return bce_loss(pred, target) + dice_loss(pred, target)\n","\n","def iou_score(preds, masks, threshold=0.5):\n","    preds = torch.sigmoid(preds) > threshold\n","    masks = masks > 0.5\n","    preds = preds.view(-1)\n","    masks = masks.view(-1)\n","\n","    intersection = (preds & masks).float().sum()\n","    union = (preds | masks).float().sum()\n","    return ((intersection + 1e-6) / (union + 1e-6)).item()\n","\n","\n","def compute_metrics(preds, masks, threshold=0.5):\n","    # transform preds from logits to probabilities using torch.sigmoid\n","    # then transform from probabilities to binary using threshold\n","    preds = torch.sigmoid(preds) > threshold\n","\n","    masks = masks > 0.5 # binarize the mask\n","\n","    # flatten the predicted segmentations for the entire batch\n","    # from [B, 1, H, W] to a 1D vector. Basically, treating the entire batch as\n","    # 1 giant image\n","    preds = preds.view(-1)\n","    masks = masks.view(-1)\n","\n","    # use logical & to compute figures necessary for confusion matrix calculations\n","    TP = (preds & masks).sum().float() # bitwise 1 and 1 match\n","    FP = (preds & ~masks).sum().float() # bitwise pred 1 yet mask was 0 (ie false positive)\n","    FN = (~preds & masks).sum().float() # bitwise pred 0 yet mask was 1 (ie false negative)\n","    TN = (~preds & ~masks).sum().float() # bitwise pred 0 and mask 0 (ie true negative)\n","\n","    epsilon = 1e-6 # add epsilon to make sure there's no division by 0\n","    precision = TP / (TP + FP + epsilon)\n","    recall = TP / (TP + FN + epsilon)\n","    accuracy = (TP + TN) / (TP + TN + FP + FN + epsilon)\n","    dice = 2 * TP / (2 * TP + FP + FN + epsilon)\n","    jaccard = TP / (TP + FP + FN + epsilon)\n","\n","    return {\n","        'precision': precision.item(),\n","        'recall': recall.item(),\n","        'accuracy': accuracy.item(),\n","        'dice': dice.item(),\n","        'jaccard': jaccard.item()\n","    }\n","\n"]},{"cell_type":"code","execution_count":18,"metadata":{"executionInfo":{"elapsed":22,"status":"ok","timestamp":1745351219609,"user":{"displayName":"Ashok Kamath","userId":"01269285531323045015"},"user_tz":240},"id":"tamURjMzT9QL"},"outputs":[],"source":["def train_epoch(model, loader, optimizer):\n","    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","    model.train()\n","    running_loss = 0\n","    for imgs, masks in tqdm(loader):\n","        imgs = imgs.cuda() if torch.cuda.is_available() else imgs\n","        masks = masks.cuda() if torch.cuda.is_available() else masks\n","        # print(f\"Image batch shape: {imgs.shape}\")\n","        # print(f\"Mask batch shape: {masks.shape}\")\n","        # print(f\"Image dtype: {imgs.dtype}, device: {imgs.device}\")\n","        # print(f\"Mask dtype: {masks.dtype}, device: {masks.device}\")\n","        preds = model(imgs)\n","        loss = combined_loss(preds, masks)\n","\n","        optimizer.zero_grad()\n","        loss.backward()\n","        optimizer.step()\n","\n","        running_loss += loss.item()\n","\n","    return running_loss / len(loader)\n","def validate_epoch(model, loader):\n","    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","    model.eval()\n","    val_loss = 0\n","    val_iou = 0\n","    metrics_accumulator = {'precision': 0, 'recall': 0, 'accuracy': 0, 'dice': 0, 'jaccard': 0}\n","\n","    with torch.no_grad():\n","        for imgs, masks in loader:\n","            imgs = imgs.to(device)\n","            masks = masks.to(device)\n","\n","            preds = model(imgs)\n","            val_loss += combined_loss(preds, masks).item()\n","            val_iou += iou_score(preds, masks)\n","\n","            metrics = compute_metrics(preds, masks)\n","            for k in metrics_accumulator:\n","                metrics_accumulator[k] += metrics[k]\n","\n","    avg_metrics = {k: v / len(loader) for k, v in metrics_accumulator.items()}\n","    avg_metrics['loss'] = val_loss / len(loader)\n","    avg_metrics['iou'] = val_iou / len(loader)\n","\n","    return avg_metrics\n","\n"]},{"cell_type":"code","source":["if torch.cuda.is_available():\n","    print(f\"ğŸ”‹ Using GPU: {torch.cuda.get_device_name(0)}\")\n","else:\n","    print(\"âš ï¸ GPU not available. Using CPU.\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"K1ndRdSQ9CYR","executionInfo":{"status":"ok","timestamp":1745351219610,"user_tz":240,"elapsed":23,"user":{"displayName":"Ashok Kamath","userId":"01269285531323045015"}},"outputId":"01696f54-6d64-4fba-9a11-d1a062518a98"},"execution_count":19,"outputs":[{"output_type":"stream","name":"stdout","text":["ğŸ”‹ Using GPU: Tesla T4\n"]}]},{"cell_type":"code","source":["import time\n","import torch\n","import pandas as pd\n","import torch.optim as optim\n","\n","def train_and_cross_test(model_class, all_data_loaders, num_epochs=10, lr=1e-4, patience=5):\n","    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","    final_results = []\n","\n","    # â±ï¸ Track total training + testing time\n","    total_start_time = time.time()\n","\n","    for i, train_val_data in enumerate(all_data_loaders):\n","        print(f\"\\nğŸš€ Training on: {train_val_data['name']}\")\n","        model = model_class().to(device)\n","        optimizer = optim.Adam(model.parameters(), lr=lr)\n","\n","        train_loader = train_val_data['train_loader']\n","        val_loader = train_val_data['val_loader']\n","\n","        best_val_loss = float('inf')\n","        best_model_state = None\n","        epochs_without_improvement = 0\n","\n","        for epoch in range(num_epochs):\n","            print(f\"\\nEpoch {epoch+1}/{num_epochs}\")\n","\n","            # â±ï¸ Epoch timing\n","            epoch_start_time = time.time()\n","            batch_times = []\n","\n","            model.train()\n","            running_loss = 0\n","            for batch_idx, (imgs, masks) in enumerate(train_loader):\n","                batch_start_time = time.time()\n","\n","                imgs = imgs.to(device)\n","                masks = masks.to(device)\n","\n","                preds = model(imgs)\n","                loss = combined_loss(preds, masks)\n","\n","                optimizer.zero_grad()\n","                loss.backward()\n","                optimizer.step()\n","\n","                running_loss += loss.item()\n","                batch_duration = time.time() - batch_start_time\n","                batch_times.append(batch_duration)\n","\n","                print(f\"  ğŸ§ª Batch {batch_idx+1}/{len(train_loader)} â€” Loss: {loss.item():.4f} â€” Time: {batch_duration:.2f}s\")\n","\n","            train_loss = running_loss / len(train_loader)\n","            val_metrics = validate_epoch(model, val_loader)\n","            val_loss = val_metrics['loss']\n","\n","            epoch_duration = time.time() - epoch_start_time\n","            avg_batch_time = sum(batch_times) / len(batch_times) if batch_times else 0\n","\n","            print(f\"ğŸ•’ Epoch Time: {epoch_duration:.2f}s | Avg Batch Time: {avg_batch_time:.2f}s\")\n","            print(f\"ğŸ“Š Train Loss: {train_loss:.4f} | \"\n","                  f\"Val Loss: {val_loss:.4f} | \"\n","                  f\"IoU: {val_metrics['iou']:.4f} | \"\n","                  f\"Dice: {val_metrics['dice']:.4f} | \"\n","                  f\"Jaccard: {val_metrics['jaccard']:.4f} | \"\n","                  f\"Precision: {val_metrics['precision']:.4f} | \"\n","                  f\"Recall: {val_metrics['recall']:.4f} | \"\n","                  f\"Accuracy: {val_metrics['accuracy']:.4f}\")\n","\n","            if val_loss < best_val_loss:\n","                best_val_loss = val_loss\n","                best_model_state = model.state_dict()\n","                epochs_without_improvement = 0\n","            else:\n","                epochs_without_improvement += 1\n","                if epochs_without_improvement >= patience:\n","                    print(f\"â¹ï¸ Early stopping triggered after {epoch+1} epochs.\")\n","                    break\n","\n","        # Restore the best model before testing\n","        if best_model_state is not None:\n","            model.load_state_dict(best_model_state)\n","\n","        print(f\"\\nğŸ§ª Testing model trained on {train_val_data['name']} on all datasets...\")\n","\n","        test_results = []\n","        for j, test_data in enumerate(all_data_loaders):\n","            test_loader = test_data['test_loader']\n","            test_metrics = validate_epoch(model, test_loader)\n","\n","            result = {\n","                \"Trained On\": train_val_data['name'],\n","                \"Tested On\": test_data['name'],\n","                \"Loss\": test_metrics['loss'],\n","                \"IoU\": test_metrics['iou'],\n","                \"Dice\": test_metrics['dice'],\n","                \"Jaccard\": test_metrics['jaccard'],\n","                \"Precision\": test_metrics['precision'],\n","                \"Recall\": test_metrics['recall'],\n","                \"Accuracy\": test_metrics['accuracy'],\n","            }\n","\n","            test_results.append(result)\n","            final_results.append(result)\n","\n","        df = pd.DataFrame(test_results)\n","        print(df.to_markdown(index=False))\n","\n","    # â±ï¸ Print total time\n","    total_duration = time.time() - total_start_time\n","    print(f\"\\nâ±ï¸ Total Time for Training + Testing: {total_duration:.2f} seconds\")\n","\n","    print(\"\\nğŸ“‹ Final Cross-Dataset Testing Summary:\")\n","    final_df = pd.DataFrame(final_results)\n","    print(final_df.to_markdown(index=False))\n","    return final_df, model\n"],"metadata":{"id":"kApDtQi03jP2","executionInfo":{"status":"ok","timestamp":1745351219611,"user_tz":240,"elapsed":21,"user":{"displayName":"Ashok Kamath","userId":"01269285531323045015"}}},"execution_count":20,"outputs":[]},{"cell_type":"code","source":["import time\n","import torch\n","import pandas as pd\n","\n","def evaluate_model_on_datasets(trained_model, all_data_loaders):\n","    \"\"\"\n","    Evaluates a trained model on all test sets in the provided data loaders.\n","\n","    Args:\n","        trained_model: A PyTorch model (with weights already loaded).\n","        all_data_loaders: List of dicts containing 'name' and 'test_loader'.\n","\n","    Returns:\n","        DataFrame with test metrics per dataset + appended row of average scores.\n","    \"\"\"\n","    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","    trained_model = trained_model.to(device)\n","    trained_model.eval()\n","\n","    print(\"\\nğŸ§ª Evaluating trained model on all test sets...\\n\")\n","    all_test_results = []\n","\n","    test_start_time = time.time()\n","\n","    for test_data in all_data_loaders:\n","        test_loader = test_data['test_loader']\n","        dataset_name = test_data['name']\n","\n","        print(f\"ğŸ” Testing on: {dataset_name}\")\n","        test_metrics = validate_epoch(trained_model, test_loader)\n","\n","        result = {\n","            \"Tested On\": dataset_name,\n","            \"Loss\": test_metrics['loss'],\n","            \"IoU\": test_metrics['iou'],\n","            \"Dice\": test_metrics['dice'],\n","            \"Jaccard\": test_metrics['jaccard'],\n","            \"Precision\": test_metrics['precision'],\n","            \"Recall\": test_metrics['recall'],\n","            \"Accuracy\": test_metrics['accuracy'],\n","        }\n","\n","        all_test_results.append(result)\n","\n","    test_duration = time.time() - test_start_time\n","    print(f\"\\nâ±ï¸ Total Evaluation Time: {test_duration:.2f} seconds\")\n","\n","    result_df = pd.DataFrame(all_test_results)\n","\n","    # ğŸ”¢ Compute average across all numeric columns\n","    avg_row = result_df.drop(columns=[\"Tested On\"]).mean()\n","    avg_row[\"Tested On\"] = \"Average\"\n","\n","    # Add the average row at the end\n","    result_df = pd.concat([result_df, pd.DataFrame([avg_row])], ignore_index=True)\n","\n","    print(\"\\nğŸ“‹ Test Results Across Datasets (with Average):\")\n","    print(result_df.to_markdown(index=False))\n","\n","    return result_df\n"],"metadata":{"id":"SqV2NhDdu7Ry","executionInfo":{"status":"ok","timestamp":1745351219619,"user_tz":240,"elapsed":8,"user":{"displayName":"Ashok Kamath","userId":"01269285531323045015"}}},"execution_count":21,"outputs":[]},{"cell_type":"markdown","source":["# U-Net"],"metadata":{"id":"0LL2sGRrH_R1"}},{"cell_type":"code","source":["!pip install segmentation-models-pytorch --quiet\n","\n","def unet_model_factory():\n","    return smp.Unet(\n","        encoder_name=\"efficientnet-b0\",     # encoder backbone\n","        encoder_weights=\"imagenet\",  # pretrained weights\n","        in_channels=3,               # rgb images\n","        classes=1,                   # binary segmentation\n","    )\n"],"metadata":{"id":"yv1j0VJ77sVu","executionInfo":{"status":"ok","timestamp":1745353625460,"user_tz":240,"elapsed":2453,"user":{"displayName":"Ashok Kamath","userId":"01269285531323045015"}}},"execution_count":27,"outputs":[]},{"cell_type":"code","source":["u_net_results, trained_model = train_and_cross_test(model_class=unet_model_factory, all_data_loaders=[combined_loaders], num_epochs=100)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"7L4_fPFkQVxg","outputId":"4637372c-5076-4f22-c3d9-6f06f4f97772","executionInfo":{"status":"ok","timestamp":1745354617291,"user_tz":240,"elapsed":979250,"user":{"displayName":"Ashok Kamath","userId":"01269285531323045015"}}},"execution_count":28,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","ğŸš€ Training on: CombinedDataset\n"]},{"output_type":"stream","name":"stderr","text":["Downloading: \"https://github.com/lukemelas/EfficientNet-PyTorch/releases/download/1.0/efficientnet-b0-355c32eb.pth\" to /root/.cache/torch/hub/checkpoints/efficientnet-b0-355c32eb.pth\n","100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20.4M/20.4M [00:00<00:00, 288MB/s]"]},{"output_type":"stream","name":"stdout","text":["\n","Epoch 1/100\n"]},{"output_type":"stream","name":"stderr","text":["\n"]},{"output_type":"stream","name":"stdout","text":["  ğŸ§ª Batch 1/55 â€” Loss: 1.8745 â€” Time: 0.96s\n","  ğŸ§ª Batch 2/55 â€” Loss: 1.8415 â€” Time: 0.47s\n","  ğŸ§ª Batch 3/55 â€” Loss: 1.8808 â€” Time: 0.49s\n","  ğŸ§ª Batch 4/55 â€” Loss: 1.8054 â€” Time: 0.47s\n","  ğŸ§ª Batch 5/55 â€” Loss: 1.7843 â€” Time: 0.53s\n","  ğŸ§ª Batch 6/55 â€” Loss: 1.7477 â€” Time: 0.46s\n","  ğŸ§ª Batch 7/55 â€” Loss: 1.7971 â€” Time: 0.52s\n","  ğŸ§ª Batch 8/55 â€” Loss: 1.8287 â€” Time: 0.44s\n","  ğŸ§ª Batch 9/55 â€” Loss: 1.7617 â€” Time: 0.53s\n","  ğŸ§ª Batch 10/55 â€” Loss: 1.7694 â€” Time: 0.46s\n","  ğŸ§ª Batch 11/55 â€” Loss: 1.7631 â€” Time: 0.52s\n","  ğŸ§ª Batch 12/55 â€” Loss: 1.7226 â€” Time: 0.44s\n","  ğŸ§ª Batch 13/55 â€” Loss: 1.7748 â€” Time: 0.47s\n","  ğŸ§ª Batch 14/55 â€” Loss: 1.6163 â€” Time: 0.46s\n","  ğŸ§ª Batch 15/55 â€” Loss: 1.6513 â€” Time: 0.44s\n","  ğŸ§ª Batch 16/55 â€” Loss: 1.7000 â€” Time: 0.46s\n","  ğŸ§ª Batch 17/55 â€” Loss: 1.6433 â€” Time: 0.44s\n","  ğŸ§ª Batch 18/55 â€” Loss: 1.6629 â€” Time: 0.45s\n","  ğŸ§ª Batch 19/55 â€” Loss: 1.7349 â€” Time: 0.45s\n","  ğŸ§ª Batch 20/55 â€” Loss: 1.6336 â€” Time: 0.50s\n","  ğŸ§ª Batch 21/55 â€” Loss: 1.6076 â€” Time: 0.46s\n","  ğŸ§ª Batch 22/55 â€” Loss: 1.6307 â€” Time: 0.48s\n","  ğŸ§ª Batch 23/55 â€” Loss: 1.6642 â€” Time: 0.56s\n","  ğŸ§ª Batch 24/55 â€” Loss: 1.5223 â€” Time: 0.48s\n","  ğŸ§ª Batch 25/55 â€” Loss: 1.5958 â€” Time: 0.46s\n","  ğŸ§ª Batch 26/55 â€” Loss: 1.5059 â€” Time: 0.46s\n","  ğŸ§ª Batch 27/55 â€” Loss: 1.5289 â€” Time: 0.52s\n","  ğŸ§ª Batch 28/55 â€” Loss: 1.5832 â€” Time: 0.48s\n","  ğŸ§ª Batch 29/55 â€” Loss: 1.4269 â€” Time: 0.52s\n","  ğŸ§ª Batch 30/55 â€” Loss: 1.5186 â€” Time: 0.46s\n","  ğŸ§ª Batch 31/55 â€” Loss: 1.4990 â€” Time: 0.50s\n","  ğŸ§ª Batch 32/55 â€” Loss: 1.5741 â€” Time: 0.51s\n","  ğŸ§ª Batch 33/55 â€” Loss: 1.5295 â€” Time: 0.45s\n","  ğŸ§ª Batch 34/55 â€” Loss: 1.5259 â€” Time: 0.48s\n","  ğŸ§ª Batch 35/55 â€” Loss: 1.4772 â€” Time: 0.47s\n","  ğŸ§ª Batch 36/55 â€” Loss: 1.4826 â€” Time: 0.47s\n","  ğŸ§ª Batch 37/55 â€” Loss: 1.4213 â€” Time: 0.46s\n","  ğŸ§ª Batch 38/55 â€” Loss: 1.3581 â€” Time: 0.49s\n","  ğŸ§ª Batch 39/55 â€” Loss: 1.4404 â€” Time: 0.56s\n","  ğŸ§ª Batch 40/55 â€” Loss: 1.4062 â€” Time: 0.51s\n","  ğŸ§ª Batch 41/55 â€” Loss: 1.4028 â€” Time: 0.49s\n","  ğŸ§ª Batch 42/55 â€” Loss: 1.4774 â€” Time: 0.49s\n","  ğŸ§ª Batch 43/55 â€” Loss: 1.3358 â€” Time: 0.44s\n","  ğŸ§ª Batch 44/55 â€” Loss: 1.3937 â€” Time: 0.48s\n","  ğŸ§ª Batch 45/55 â€” Loss: 1.2843 â€” Time: 0.47s\n","  ğŸ§ª Batch 46/55 â€” Loss: 1.4329 â€” Time: 0.47s\n","  ğŸ§ª Batch 47/55 â€” Loss: 1.3722 â€” Time: 0.49s\n","  ğŸ§ª Batch 48/55 â€” Loss: 1.3399 â€” Time: 0.53s\n","  ğŸ§ª Batch 49/55 â€” Loss: 1.3501 â€” Time: 0.46s\n","  ğŸ§ª Batch 50/55 â€” Loss: 1.2093 â€” Time: 0.52s\n","  ğŸ§ª Batch 51/55 â€” Loss: 1.2769 â€” Time: 0.49s\n","  ğŸ§ª Batch 52/55 â€” Loss: 1.4293 â€” Time: 0.53s\n","  ğŸ§ª Batch 53/55 â€” Loss: 1.2277 â€” Time: 0.49s\n","  ğŸ§ª Batch 54/55 â€” Loss: 1.3143 â€” Time: 0.53s\n","  ğŸ§ª Batch 55/55 â€” Loss: 1.4439 â€” Time: 0.51s\n","ğŸ•’ Epoch Time: 36.37s | Avg Batch Time: 0.49s\n","ğŸ“Š Train Loss: 1.5560 | Val Loss: 1.1802 | IoU: 0.4288 | Dice: 0.5966 | Jaccard: 0.4288 | Precision: 0.5414 | Recall: 0.6664 | Accuracy: 0.9054\n","\n","Epoch 2/100\n","  ğŸ§ª Batch 1/55 â€” Loss: 1.2858 â€” Time: 0.52s\n","  ğŸ§ª Batch 2/55 â€” Loss: 1.3007 â€” Time: 0.52s\n","  ğŸ§ª Batch 3/55 â€” Loss: 1.2975 â€” Time: 0.45s\n","  ğŸ§ª Batch 4/55 â€” Loss: 1.1692 â€” Time: 0.47s\n","  ğŸ§ª Batch 5/55 â€” Loss: 1.2521 â€” Time: 0.46s\n","  ğŸ§ª Batch 6/55 â€” Loss: 1.1737 â€” Time: 0.44s\n","  ğŸ§ª Batch 7/55 â€” Loss: 1.1878 â€” Time: 0.44s\n","  ğŸ§ª Batch 8/55 â€” Loss: 1.1531 â€” Time: 0.44s\n","  ğŸ§ª Batch 9/55 â€” Loss: 1.3334 â€” Time: 0.44s\n","  ğŸ§ª Batch 10/55 â€” Loss: 1.2054 â€” Time: 0.44s\n","  ğŸ§ª Batch 11/55 â€” Loss: 1.0951 â€” Time: 0.44s\n","  ğŸ§ª Batch 12/55 â€” Loss: 1.1566 â€” Time: 0.44s\n","  ğŸ§ª Batch 13/55 â€” Loss: 1.2340 â€” Time: 0.46s\n","  ğŸ§ª Batch 14/55 â€” Loss: 1.1347 â€” Time: 0.48s\n","  ğŸ§ª Batch 15/55 â€” Loss: 1.2456 â€” Time: 0.47s\n","  ğŸ§ª Batch 16/55 â€” Loss: 1.0525 â€” Time: 0.47s\n","  ğŸ§ª Batch 17/55 â€” Loss: 1.1728 â€” Time: 0.46s\n","  ğŸ§ª Batch 18/55 â€” Loss: 1.1977 â€” Time: 0.50s\n","  ğŸ§ª Batch 19/55 â€” Loss: 1.0697 â€” Time: 0.48s\n","  ğŸ§ª Batch 20/55 â€” Loss: 1.0237 â€” Time: 0.46s\n","  ğŸ§ª Batch 21/55 â€” Loss: 1.1102 â€” Time: 0.44s\n","  ğŸ§ª Batch 22/55 â€” Loss: 1.2758 â€” Time: 0.44s\n","  ğŸ§ª Batch 23/55 â€” Loss: 1.0886 â€” Time: 0.44s\n","  ğŸ§ª Batch 24/55 â€” Loss: 1.0442 â€” Time: 0.44s\n","  ğŸ§ª Batch 25/55 â€” Loss: 1.1425 â€” Time: 0.44s\n","  ğŸ§ª Batch 26/55 â€” Loss: 1.0999 â€” Time: 0.44s\n","  ğŸ§ª Batch 27/55 â€” Loss: 1.0961 â€” Time: 0.44s\n","  ğŸ§ª Batch 28/55 â€” Loss: 1.0308 â€” Time: 0.44s\n","  ğŸ§ª Batch 29/55 â€” Loss: 1.1347 â€” Time: 0.44s\n","  ğŸ§ª Batch 30/55 â€” Loss: 1.0305 â€” Time: 0.44s\n","  ğŸ§ª Batch 31/55 â€” Loss: 1.2145 â€” Time: 0.44s\n","  ğŸ§ª Batch 32/55 â€” Loss: 1.0223 â€” Time: 0.44s\n","  ğŸ§ª Batch 33/55 â€” Loss: 1.0967 â€” Time: 0.44s\n","  ğŸ§ª Batch 34/55 â€” Loss: 1.1225 â€” Time: 0.44s\n","  ğŸ§ª Batch 35/55 â€” Loss: 1.0745 â€” Time: 0.44s\n","  ğŸ§ª Batch 36/55 â€” Loss: 1.0598 â€” Time: 0.44s\n","  ğŸ§ª Batch 37/55 â€” Loss: 1.0660 â€” Time: 0.44s\n","  ğŸ§ª Batch 38/55 â€” Loss: 1.0163 â€” Time: 0.44s\n","  ğŸ§ª Batch 39/55 â€” Loss: 1.0318 â€” Time: 0.44s\n","  ğŸ§ª Batch 40/55 â€” Loss: 1.0015 â€” Time: 0.44s\n","  ğŸ§ª Batch 41/55 â€” Loss: 0.9751 â€” Time: 0.44s\n","  ğŸ§ª Batch 42/55 â€” Loss: 1.0484 â€” Time: 0.44s\n","  ğŸ§ª Batch 43/55 â€” Loss: 1.0852 â€” Time: 0.44s\n","  ğŸ§ª Batch 44/55 â€” Loss: 0.9970 â€” Time: 0.47s\n","  ğŸ§ª Batch 45/55 â€” Loss: 0.9995 â€” Time: 0.50s\n","  ğŸ§ª Batch 46/55 â€” Loss: 1.0993 â€” Time: 0.45s\n","  ğŸ§ª Batch 47/55 â€” Loss: 0.9838 â€” Time: 0.44s\n","  ğŸ§ª Batch 48/55 â€” Loss: 1.0125 â€” Time: 0.49s\n","  ğŸ§ª Batch 49/55 â€” Loss: 0.9506 â€” Time: 0.46s\n","  ğŸ§ª Batch 50/55 â€” Loss: 1.0268 â€” Time: 0.44s\n","  ğŸ§ª Batch 51/55 â€” Loss: 1.0522 â€” Time: 0.44s\n","  ğŸ§ª Batch 52/55 â€” Loss: 0.9724 â€” Time: 0.44s\n","  ğŸ§ª Batch 53/55 â€” Loss: 1.0665 â€” Time: 0.44s\n","  ğŸ§ª Batch 54/55 â€” Loss: 1.0001 â€” Time: 0.44s\n","  ğŸ§ª Batch 55/55 â€” Loss: 1.0897 â€” Time: 0.29s\n","ğŸ•’ Epoch Time: 27.95s | Avg Batch Time: 0.45s\n","ğŸ“Š Train Loss: 1.1065 | Val Loss: 0.9615 | IoU: 0.5325 | Dice: 0.6940 | Jaccard: 0.5325 | Precision: 0.8942 | Recall: 0.5684 | Accuracy: 0.9457\n","\n","Epoch 3/100\n","  ğŸ§ª Batch 1/55 â€” Loss: 0.9511 â€” Time: 0.45s\n","  ğŸ§ª Batch 2/55 â€” Loss: 0.9828 â€” Time: 0.44s\n","  ğŸ§ª Batch 3/55 â€” Loss: 0.9779 â€” Time: 0.43s\n","  ğŸ§ª Batch 4/55 â€” Loss: 0.9835 â€” Time: 0.44s\n","  ğŸ§ª Batch 5/55 â€” Loss: 1.0709 â€” Time: 0.44s\n","  ğŸ§ª Batch 6/55 â€” Loss: 1.0061 â€” Time: 0.44s\n","  ğŸ§ª Batch 7/55 â€” Loss: 0.9509 â€” Time: 0.44s\n","  ğŸ§ª Batch 8/55 â€” Loss: 0.9418 â€” Time: 0.43s\n","  ğŸ§ª Batch 9/55 â€” Loss: 0.9294 â€” Time: 0.44s\n","  ğŸ§ª Batch 10/55 â€” Loss: 1.0124 â€” Time: 0.44s\n","  ğŸ§ª Batch 11/55 â€” Loss: 0.9518 â€” Time: 0.44s\n","  ğŸ§ª Batch 12/55 â€” Loss: 0.9514 â€” Time: 0.44s\n","  ğŸ§ª Batch 13/55 â€” Loss: 0.9367 â€” Time: 0.44s\n","  ğŸ§ª Batch 14/55 â€” Loss: 0.9678 â€” Time: 0.45s\n","  ğŸ§ª Batch 15/55 â€” Loss: 0.9966 â€” Time: 0.44s\n","  ğŸ§ª Batch 16/55 â€” Loss: 0.9215 â€” Time: 0.47s\n","  ğŸ§ª Batch 17/55 â€” Loss: 0.8730 â€” Time: 0.50s\n","  ğŸ§ª Batch 18/55 â€” Loss: 0.8290 â€” Time: 0.48s\n","  ğŸ§ª Batch 19/55 â€” Loss: 0.9945 â€” Time: 0.48s\n","  ğŸ§ª Batch 20/55 â€” Loss: 0.8713 â€” Time: 0.49s\n","  ğŸ§ª Batch 21/55 â€” Loss: 0.9321 â€” Time: 0.44s\n","  ğŸ§ª Batch 22/55 â€” Loss: 0.8527 â€” Time: 0.44s\n","  ğŸ§ª Batch 23/55 â€” Loss: 0.8430 â€” Time: 0.44s\n","  ğŸ§ª Batch 24/55 â€” Loss: 0.8411 â€” Time: 0.44s\n","  ğŸ§ª Batch 25/55 â€” Loss: 0.9371 â€” Time: 0.43s\n","  ğŸ§ª Batch 26/55 â€” Loss: 0.8921 â€” Time: 0.44s\n","  ğŸ§ª Batch 27/55 â€” Loss: 0.8839 â€” Time: 0.44s\n","  ğŸ§ª Batch 28/55 â€” Loss: 0.8492 â€” Time: 0.44s\n","  ğŸ§ª Batch 29/55 â€” Loss: 0.9555 â€” Time: 0.44s\n","  ğŸ§ª Batch 30/55 â€” Loss: 0.8264 â€” Time: 0.44s\n","  ğŸ§ª Batch 31/55 â€” Loss: 0.8167 â€” Time: 0.44s\n","  ğŸ§ª Batch 32/55 â€” Loss: 0.9499 â€” Time: 0.44s\n","  ğŸ§ª Batch 33/55 â€” Loss: 0.9163 â€” Time: 0.44s\n","  ğŸ§ª Batch 34/55 â€” Loss: 0.9783 â€” Time: 0.44s\n","  ğŸ§ª Batch 35/55 â€” Loss: 0.7846 â€” Time: 0.44s\n","  ğŸ§ª Batch 36/55 â€” Loss: 0.9272 â€” Time: 0.44s\n","  ğŸ§ª Batch 37/55 â€” Loss: 0.8435 â€” Time: 0.44s\n","  ğŸ§ª Batch 38/55 â€” Loss: 0.8537 â€” Time: 0.44s\n","  ğŸ§ª Batch 39/55 â€” Loss: 0.8476 â€” Time: 0.44s\n","  ğŸ§ª Batch 40/55 â€” Loss: 0.8431 â€” Time: 0.44s\n","  ğŸ§ª Batch 41/55 â€” Loss: 0.8420 â€” Time: 0.44s\n","  ğŸ§ª Batch 42/55 â€” Loss: 0.8376 â€” Time: 0.44s\n","  ğŸ§ª Batch 43/55 â€” Loss: 0.9193 â€” Time: 0.44s\n","  ğŸ§ª Batch 44/55 â€” Loss: 0.8567 â€” Time: 0.44s\n","  ğŸ§ª Batch 45/55 â€” Loss: 0.8798 â€” Time: 0.45s\n","  ğŸ§ª Batch 46/55 â€” Loss: 0.8120 â€” Time: 0.45s\n","  ğŸ§ª Batch 47/55 â€” Loss: 0.8158 â€” Time: 0.44s\n","  ğŸ§ª Batch 48/55 â€” Loss: 0.8386 â€” Time: 0.45s\n","  ğŸ§ª Batch 49/55 â€” Loss: 0.9085 â€” Time: 0.48s\n","  ğŸ§ª Batch 50/55 â€” Loss: 0.9244 â€” Time: 0.46s\n","  ğŸ§ª Batch 51/55 â€” Loss: 0.9171 â€” Time: 0.45s\n","  ğŸ§ª Batch 52/55 â€” Loss: 0.8565 â€” Time: 0.44s\n","  ğŸ§ª Batch 53/55 â€” Loss: 0.7935 â€” Time: 0.44s\n","  ğŸ§ª Batch 54/55 â€” Loss: 0.9129 â€” Time: 0.44s\n","  ğŸ§ª Batch 55/55 â€” Loss: 0.8365 â€” Time: 0.29s\n","ğŸ•’ Epoch Time: 27.38s | Avg Batch Time: 0.44s\n","ğŸ“Š Train Loss: 0.9023 | Val Loss: 0.8640 | IoU: 0.6334 | Dice: 0.7749 | Jaccard: 0.6334 | Precision: 0.9040 | Recall: 0.6796 | Accuracy: 0.9564\n","\n","Epoch 4/100\n","  ğŸ§ª Batch 1/55 â€” Loss: 0.8231 â€” Time: 0.45s\n","  ğŸ§ª Batch 2/55 â€” Loss: 0.7419 â€” Time: 0.45s\n","  ğŸ§ª Batch 3/55 â€” Loss: 0.8532 â€” Time: 0.44s\n","  ğŸ§ª Batch 4/55 â€” Loss: 0.8042 â€” Time: 0.45s\n","  ğŸ§ª Batch 5/55 â€” Loss: 0.7954 â€” Time: 0.44s\n","  ğŸ§ª Batch 6/55 â€” Loss: 0.9050 â€” Time: 0.45s\n","  ğŸ§ª Batch 7/55 â€” Loss: 0.7553 â€” Time: 0.44s\n","  ğŸ§ª Batch 8/55 â€” Loss: 0.7535 â€” Time: 0.44s\n","  ğŸ§ª Batch 9/55 â€” Loss: 0.7932 â€” Time: 0.44s\n","  ğŸ§ª Batch 10/55 â€” Loss: 0.6855 â€” Time: 0.44s\n","  ğŸ§ª Batch 11/55 â€” Loss: 0.7548 â€” Time: 0.44s\n","  ğŸ§ª Batch 12/55 â€” Loss: 0.8342 â€” Time: 0.44s\n","  ğŸ§ª Batch 13/55 â€” Loss: 0.7818 â€” Time: 0.44s\n","  ğŸ§ª Batch 14/55 â€” Loss: 0.6741 â€” Time: 0.44s\n","  ğŸ§ª Batch 15/55 â€” Loss: 0.7858 â€” Time: 0.45s\n","  ğŸ§ª Batch 16/55 â€” Loss: 0.7306 â€” Time: 0.44s\n","  ğŸ§ª Batch 17/55 â€” Loss: 0.9055 â€” Time: 0.44s\n","  ğŸ§ª Batch 18/55 â€” Loss: 0.8089 â€” Time: 0.46s\n","  ğŸ§ª Batch 19/55 â€” Loss: 0.7877 â€” Time: 0.44s\n","  ğŸ§ª Batch 20/55 â€” Loss: 0.8568 â€” Time: 0.50s\n","  ğŸ§ª Batch 21/55 â€” Loss: 0.8231 â€” Time: 0.47s\n","  ğŸ§ª Batch 22/55 â€” Loss: 0.7533 â€” Time: 0.46s\n","  ğŸ§ª Batch 23/55 â€” Loss: 0.7867 â€” Time: 0.50s\n","  ğŸ§ª Batch 24/55 â€” Loss: 0.8531 â€” Time: 0.44s\n","  ğŸ§ª Batch 25/55 â€” Loss: 0.8496 â€” Time: 0.44s\n","  ğŸ§ª Batch 26/55 â€” Loss: 0.8025 â€” Time: 0.44s\n","  ğŸ§ª Batch 27/55 â€” Loss: 0.8001 â€” Time: 0.45s\n","  ğŸ§ª Batch 28/55 â€” Loss: 0.7542 â€” Time: 0.44s\n","  ğŸ§ª Batch 29/55 â€” Loss: 0.7654 â€” Time: 0.44s\n","  ğŸ§ª Batch 30/55 â€” Loss: 0.7459 â€” Time: 0.44s\n","  ğŸ§ª Batch 31/55 â€” Loss: 0.7974 â€” Time: 0.44s\n","  ğŸ§ª Batch 32/55 â€” Loss: 0.7438 â€” Time: 0.44s\n","  ğŸ§ª Batch 33/55 â€” Loss: 0.9557 â€” Time: 0.44s\n","  ğŸ§ª Batch 34/55 â€” Loss: 0.7350 â€” Time: 0.44s\n","  ğŸ§ª Batch 35/55 â€” Loss: 0.8271 â€” Time: 0.44s\n","  ğŸ§ª Batch 36/55 â€” Loss: 0.8198 â€” Time: 0.44s\n","  ğŸ§ª Batch 37/55 â€” Loss: 0.8237 â€” Time: 0.44s\n","  ğŸ§ª Batch 38/55 â€” Loss: 0.8686 â€” Time: 0.44s\n","  ğŸ§ª Batch 39/55 â€” Loss: 0.7201 â€” Time: 0.44s\n","  ğŸ§ª Batch 40/55 â€” Loss: 0.7542 â€” Time: 0.44s\n","  ğŸ§ª Batch 41/55 â€” Loss: 0.6669 â€” Time: 0.44s\n","  ğŸ§ª Batch 42/55 â€” Loss: 0.8323 â€” Time: 0.44s\n","  ğŸ§ª Batch 43/55 â€” Loss: 0.6980 â€” Time: 0.44s\n","  ğŸ§ª Batch 44/55 â€” Loss: 0.7053 â€” Time: 0.44s\n","  ğŸ§ª Batch 45/55 â€” Loss: 0.7759 â€” Time: 0.44s\n","  ğŸ§ª Batch 46/55 â€” Loss: 0.7242 â€” Time: 0.44s\n","  ğŸ§ª Batch 47/55 â€” Loss: 0.7777 â€” Time: 0.58s\n","  ğŸ§ª Batch 48/55 â€” Loss: 0.6720 â€” Time: 0.56s\n","  ğŸ§ª Batch 49/55 â€” Loss: 0.6798 â€” Time: 0.64s\n","  ğŸ§ª Batch 50/55 â€” Loss: 0.7359 â€” Time: 0.65s\n","  ğŸ§ª Batch 51/55 â€” Loss: 0.7501 â€” Time: 0.52s\n","  ğŸ§ª Batch 52/55 â€” Loss: 0.7298 â€” Time: 0.62s\n","  ğŸ§ª Batch 53/55 â€” Loss: 0.6934 â€” Time: 0.46s\n","  ğŸ§ª Batch 54/55 â€” Loss: 0.7939 â€” Time: 0.44s\n","  ğŸ§ª Batch 55/55 â€” Loss: 0.6662 â€” Time: 0.28s\n","ğŸ•’ Epoch Time: 29.60s | Avg Batch Time: 0.46s\n","ğŸ“Š Train Loss: 0.7766 | Val Loss: 0.8124 | IoU: 0.6517 | Dice: 0.7877 | Jaccard: 0.6517 | Precision: 0.8375 | Recall: 0.7443 | Accuracy: 0.9532\n","\n","Epoch 5/100\n","  ğŸ§ª Batch 1/55 â€” Loss: 0.7195 â€” Time: 0.44s\n","  ğŸ§ª Batch 2/55 â€” Loss: 0.6834 â€” Time: 0.45s\n","  ğŸ§ª Batch 3/55 â€” Loss: 0.6630 â€” Time: 0.48s\n","  ğŸ§ª Batch 4/55 â€” Loss: 0.7467 â€” Time: 0.44s\n","  ğŸ§ª Batch 5/55 â€” Loss: 0.6495 â€” Time: 0.44s\n","  ğŸ§ª Batch 6/55 â€” Loss: 0.6716 â€” Time: 0.44s\n","  ğŸ§ª Batch 7/55 â€” Loss: 0.7164 â€” Time: 0.44s\n","  ğŸ§ª Batch 8/55 â€” Loss: 0.6649 â€” Time: 0.45s\n","  ğŸ§ª Batch 9/55 â€” Loss: 0.7387 â€” Time: 0.44s\n","  ğŸ§ª Batch 10/55 â€” Loss: 0.7365 â€” Time: 0.44s\n","  ğŸ§ª Batch 11/55 â€” Loss: 0.7145 â€” Time: 0.44s\n","  ğŸ§ª Batch 12/55 â€” Loss: 0.6690 â€” Time: 0.47s\n","  ğŸ§ª Batch 13/55 â€” Loss: 0.7333 â€” Time: 0.49s\n","  ğŸ§ª Batch 14/55 â€” Loss: 0.6428 â€” Time: 0.48s\n","  ğŸ§ª Batch 15/55 â€” Loss: 0.7258 â€” Time: 0.46s\n","  ğŸ§ª Batch 16/55 â€” Loss: 0.7400 â€” Time: 0.48s\n","  ğŸ§ª Batch 17/55 â€” Loss: 0.6876 â€” Time: 0.45s\n","  ğŸ§ª Batch 18/55 â€” Loss: 0.6745 â€” Time: 0.44s\n","  ğŸ§ª Batch 19/55 â€” Loss: 0.6744 â€” Time: 0.45s\n","  ğŸ§ª Batch 20/55 â€” Loss: 0.7311 â€” Time: 0.44s\n","  ğŸ§ª Batch 21/55 â€” Loss: 0.7027 â€” Time: 0.44s\n","  ğŸ§ª Batch 22/55 â€” Loss: 0.6933 â€” Time: 0.44s\n","  ğŸ§ª Batch 23/55 â€” Loss: 0.6340 â€” Time: 0.44s\n","  ğŸ§ª Batch 24/55 â€” Loss: 0.5903 â€” Time: 0.44s\n","  ğŸ§ª Batch 25/55 â€” Loss: 0.6517 â€” Time: 0.44s\n","  ğŸ§ª Batch 26/55 â€” Loss: 0.5594 â€” Time: 0.44s\n","  ğŸ§ª Batch 27/55 â€” Loss: 0.6046 â€” Time: 0.44s\n","  ğŸ§ª Batch 28/55 â€” Loss: 0.6404 â€” Time: 0.44s\n","  ğŸ§ª Batch 29/55 â€” Loss: 0.6928 â€” Time: 0.44s\n","  ğŸ§ª Batch 30/55 â€” Loss: 0.7298 â€” Time: 0.44s\n","  ğŸ§ª Batch 31/55 â€” Loss: 0.6086 â€” Time: 0.44s\n","  ğŸ§ª Batch 32/55 â€” Loss: 0.7064 â€” Time: 0.44s\n","  ğŸ§ª Batch 33/55 â€” Loss: 0.6515 â€” Time: 0.44s\n","  ğŸ§ª Batch 34/55 â€” Loss: 0.6952 â€” Time: 0.44s\n","  ğŸ§ª Batch 35/55 â€” Loss: 0.6649 â€” Time: 0.44s\n","  ğŸ§ª Batch 36/55 â€” Loss: 0.5714 â€” Time: 0.44s\n","  ğŸ§ª Batch 37/55 â€” Loss: 0.5925 â€” Time: 0.44s\n","  ğŸ§ª Batch 38/55 â€” Loss: 0.6094 â€” Time: 0.44s\n","  ğŸ§ª Batch 39/55 â€” Loss: 0.6205 â€” Time: 0.44s\n","  ğŸ§ª Batch 40/55 â€” Loss: 0.5782 â€” Time: 0.44s\n","  ğŸ§ª Batch 41/55 â€” Loss: 0.5834 â€” Time: 0.44s\n","  ğŸ§ª Batch 42/55 â€” Loss: 0.5911 â€” Time: 0.44s\n","  ğŸ§ª Batch 43/55 â€” Loss: 0.6107 â€” Time: 0.44s\n","  ğŸ§ª Batch 44/55 â€” Loss: 0.6944 â€” Time: 0.44s\n","  ğŸ§ª Batch 45/55 â€” Loss: 0.5970 â€” Time: 0.46s\n","  ğŸ§ª Batch 46/55 â€” Loss: 0.6021 â€” Time: 0.45s\n","  ğŸ§ª Batch 47/55 â€” Loss: 0.5955 â€” Time: 0.52s\n","  ğŸ§ª Batch 48/55 â€” Loss: 0.6132 â€” Time: 0.45s\n","  ğŸ§ª Batch 49/55 â€” Loss: 0.5776 â€” Time: 0.47s\n","  ğŸ§ª Batch 50/55 â€” Loss: 0.6569 â€” Time: 0.44s\n","  ğŸ§ª Batch 51/55 â€” Loss: 0.5957 â€” Time: 0.44s\n","  ğŸ§ª Batch 52/55 â€” Loss: 0.6591 â€” Time: 0.44s\n","  ğŸ§ª Batch 53/55 â€” Loss: 0.6637 â€” Time: 0.44s\n","  ğŸ§ª Batch 54/55 â€” Loss: 0.6022 â€” Time: 0.44s\n","  ğŸ§ª Batch 55/55 â€” Loss: 0.6761 â€” Time: 0.29s\n","ğŸ•’ Epoch Time: 27.86s | Avg Batch Time: 0.44s\n","ğŸ“Š Train Loss: 0.6564 | Val Loss: 0.7189 | IoU: 0.7061 | Dice: 0.8266 | Jaccard: 0.7061 | Precision: 0.9309 | Recall: 0.7445 | Accuracy: 0.9642\n","\n","Epoch 6/100\n","  ğŸ§ª Batch 1/55 â€” Loss: 0.5797 â€” Time: 0.44s\n","  ğŸ§ª Batch 2/55 â€” Loss: 0.5586 â€” Time: 0.44s\n","  ğŸ§ª Batch 3/55 â€” Loss: 0.5183 â€” Time: 0.44s\n","  ğŸ§ª Batch 4/55 â€” Loss: 0.5803 â€” Time: 0.44s\n","  ğŸ§ª Batch 5/55 â€” Loss: 0.5052 â€” Time: 0.44s\n","  ğŸ§ª Batch 6/55 â€” Loss: 0.5875 â€” Time: 0.44s\n","  ğŸ§ª Batch 7/55 â€” Loss: 0.5003 â€” Time: 0.45s\n","  ğŸ§ª Batch 8/55 â€” Loss: 0.6521 â€” Time: 0.44s\n","  ğŸ§ª Batch 9/55 â€” Loss: 0.5595 â€” Time: 0.44s\n","  ğŸ§ª Batch 10/55 â€” Loss: 0.6271 â€” Time: 0.44s\n","  ğŸ§ª Batch 11/55 â€” Loss: 0.5805 â€” Time: 0.44s\n","  ğŸ§ª Batch 12/55 â€” Loss: 0.5830 â€” Time: 0.44s\n","  ğŸ§ª Batch 13/55 â€” Loss: 0.5515 â€” Time: 0.44s\n","  ğŸ§ª Batch 14/55 â€” Loss: 0.5533 â€” Time: 0.44s\n","  ğŸ§ª Batch 15/55 â€” Loss: 0.7011 â€” Time: 0.46s\n","  ğŸ§ª Batch 16/55 â€” Loss: 0.6202 â€” Time: 0.47s\n","  ğŸ§ª Batch 17/55 â€” Loss: 0.5354 â€” Time: 0.46s\n","  ğŸ§ª Batch 18/55 â€” Loss: 0.5695 â€” Time: 0.49s\n","  ğŸ§ª Batch 19/55 â€” Loss: 0.5241 â€” Time: 0.47s\n","  ğŸ§ª Batch 20/55 â€” Loss: 0.5314 â€” Time: 0.48s\n","  ğŸ§ª Batch 21/55 â€” Loss: 0.5392 â€” Time: 0.44s\n","  ğŸ§ª Batch 22/55 â€” Loss: 0.5752 â€” Time: 0.44s\n","  ğŸ§ª Batch 23/55 â€” Loss: 0.5099 â€” Time: 0.44s\n","  ğŸ§ª Batch 24/55 â€” Loss: 0.5929 â€” Time: 0.44s\n","  ğŸ§ª Batch 25/55 â€” Loss: 0.5269 â€” Time: 0.44s\n","  ğŸ§ª Batch 26/55 â€” Loss: 0.6131 â€” Time: 0.44s\n","  ğŸ§ª Batch 27/55 â€” Loss: 0.5464 â€” Time: 0.44s\n","  ğŸ§ª Batch 28/55 â€” Loss: 0.5091 â€” Time: 0.44s\n","  ğŸ§ª Batch 29/55 â€” Loss: 0.5446 â€” Time: 0.44s\n","  ğŸ§ª Batch 30/55 â€” Loss: 0.5714 â€” Time: 0.44s\n","  ğŸ§ª Batch 31/55 â€” Loss: 0.5870 â€” Time: 0.44s\n","  ğŸ§ª Batch 32/55 â€” Loss: 0.4539 â€” Time: 0.44s\n","  ğŸ§ª Batch 33/55 â€” Loss: 0.6430 â€” Time: 0.44s\n","  ğŸ§ª Batch 34/55 â€” Loss: 0.6228 â€” Time: 0.44s\n","  ğŸ§ª Batch 35/55 â€” Loss: 0.4520 â€” Time: 0.44s\n","  ğŸ§ª Batch 36/55 â€” Loss: 0.4905 â€” Time: 0.44s\n","  ğŸ§ª Batch 37/55 â€” Loss: 0.5506 â€” Time: 0.44s\n","  ğŸ§ª Batch 38/55 â€” Loss: 0.6363 â€” Time: 0.44s\n","  ğŸ§ª Batch 39/55 â€” Loss: 0.4877 â€” Time: 0.44s\n","  ğŸ§ª Batch 40/55 â€” Loss: 0.4981 â€” Time: 0.44s\n","  ğŸ§ª Batch 41/55 â€” Loss: 0.6038 â€” Time: 0.44s\n","  ğŸ§ª Batch 42/55 â€” Loss: 0.5750 â€” Time: 0.44s\n","  ğŸ§ª Batch 43/55 â€” Loss: 0.5117 â€” Time: 0.44s\n","  ğŸ§ª Batch 44/55 â€” Loss: 0.5016 â€” Time: 0.44s\n","  ğŸ§ª Batch 45/55 â€” Loss: 0.5225 â€” Time: 0.44s\n","  ğŸ§ª Batch 46/55 â€” Loss: 0.4924 â€” Time: 0.44s\n","  ğŸ§ª Batch 47/55 â€” Loss: 0.5898 â€” Time: 0.44s\n","  ğŸ§ª Batch 48/55 â€” Loss: 0.4587 â€” Time: 0.44s\n","  ğŸ§ª Batch 49/55 â€” Loss: 0.5694 â€” Time: 0.44s\n","  ğŸ§ª Batch 50/55 â€” Loss: 0.6637 â€” Time: 0.49s\n","  ğŸ§ª Batch 51/55 â€” Loss: 0.5273 â€” Time: 0.46s\n","  ğŸ§ª Batch 52/55 â€” Loss: 0.5360 â€” Time: 0.44s\n","  ğŸ§ª Batch 53/55 â€” Loss: 0.6424 â€” Time: 0.44s\n","  ğŸ§ª Batch 54/55 â€” Loss: 0.4601 â€” Time: 0.44s\n","  ğŸ§ª Batch 55/55 â€” Loss: 0.5984 â€” Time: 0.29s\n","ğŸ•’ Epoch Time: 27.30s | Avg Batch Time: 0.44s\n","ğŸ“Š Train Loss: 0.5568 | Val Loss: 0.6410 | IoU: 0.7198 | Dice: 0.8359 | Jaccard: 0.7198 | Precision: 0.9273 | Recall: 0.7617 | Accuracy: 0.9657\n","\n","Epoch 7/100\n","  ğŸ§ª Batch 1/55 â€” Loss: 0.5182 â€” Time: 0.44s\n","  ğŸ§ª Batch 2/55 â€” Loss: 0.5108 â€” Time: 0.44s\n","  ğŸ§ª Batch 3/55 â€” Loss: 0.4649 â€” Time: 0.44s\n","  ğŸ§ª Batch 4/55 â€” Loss: 0.5781 â€” Time: 0.45s\n","  ğŸ§ª Batch 5/55 â€” Loss: 0.4514 â€” Time: 0.44s\n","  ğŸ§ª Batch 6/55 â€” Loss: 0.4783 â€” Time: 0.45s\n","  ğŸ§ª Batch 7/55 â€” Loss: 0.4424 â€” Time: 0.44s\n","  ğŸ§ª Batch 8/55 â€” Loss: 0.5748 â€” Time: 0.44s\n","  ğŸ§ª Batch 9/55 â€” Loss: 0.5415 â€” Time: 0.44s\n","  ğŸ§ª Batch 10/55 â€” Loss: 0.5346 â€” Time: 0.44s\n","  ğŸ§ª Batch 11/55 â€” Loss: 0.5459 â€” Time: 0.44s\n","  ğŸ§ª Batch 12/55 â€” Loss: 0.5670 â€” Time: 0.44s\n","  ğŸ§ª Batch 13/55 â€” Loss: 0.4409 â€” Time: 0.44s\n","  ğŸ§ª Batch 14/55 â€” Loss: 0.4995 â€” Time: 0.44s\n","  ğŸ§ª Batch 15/55 â€” Loss: 0.5289 â€” Time: 0.44s\n","  ğŸ§ª Batch 16/55 â€” Loss: 0.4352 â€” Time: 0.45s\n","  ğŸ§ª Batch 17/55 â€” Loss: 0.5162 â€” Time: 0.45s\n","  ğŸ§ª Batch 18/55 â€” Loss: 0.4601 â€” Time: 0.47s\n","  ğŸ§ª Batch 19/55 â€” Loss: 0.4588 â€” Time: 0.45s\n","  ğŸ§ª Batch 20/55 â€” Loss: 0.5052 â€” Time: 0.47s\n","  ğŸ§ª Batch 21/55 â€” Loss: 0.4571 â€” Time: 0.47s\n","  ğŸ§ª Batch 22/55 â€” Loss: 0.5115 â€” Time: 0.45s\n","  ğŸ§ª Batch 23/55 â€” Loss: 0.4073 â€” Time: 0.44s\n","  ğŸ§ª Batch 24/55 â€” Loss: 0.4845 â€” Time: 0.44s\n","  ğŸ§ª Batch 25/55 â€” Loss: 0.4081 â€” Time: 0.44s\n","  ğŸ§ª Batch 26/55 â€” Loss: 0.3981 â€” Time: 0.44s\n","  ğŸ§ª Batch 27/55 â€” Loss: 0.4960 â€” Time: 0.44s\n","  ğŸ§ª Batch 28/55 â€” Loss: 0.3754 â€” Time: 0.44s\n","  ğŸ§ª Batch 29/55 â€” Loss: 0.5929 â€” Time: 0.44s\n","  ğŸ§ª Batch 30/55 â€” Loss: 0.5629 â€” Time: 0.44s\n","  ğŸ§ª Batch 31/55 â€” Loss: 0.4651 â€” Time: 0.44s\n","  ğŸ§ª Batch 32/55 â€” Loss: 0.4341 â€” Time: 0.44s\n","  ğŸ§ª Batch 33/55 â€” Loss: 0.4423 â€” Time: 0.44s\n","  ğŸ§ª Batch 34/55 â€” Loss: 0.4377 â€” Time: 0.44s\n","  ğŸ§ª Batch 35/55 â€” Loss: 0.3960 â€” Time: 0.44s\n","  ğŸ§ª Batch 36/55 â€” Loss: 0.4012 â€” Time: 0.44s\n","  ğŸ§ª Batch 37/55 â€” Loss: 0.4691 â€” Time: 0.44s\n","  ğŸ§ª Batch 38/55 â€” Loss: 0.3736 â€” Time: 0.44s\n","  ğŸ§ª Batch 39/55 â€” Loss: 0.4520 â€” Time: 0.44s\n","  ğŸ§ª Batch 40/55 â€” Loss: 0.4443 â€” Time: 0.44s\n","  ğŸ§ª Batch 41/55 â€” Loss: 0.4158 â€” Time: 0.44s\n","  ğŸ§ª Batch 42/55 â€” Loss: 0.4187 â€” Time: 0.44s\n","  ğŸ§ª Batch 43/55 â€” Loss: 0.4245 â€” Time: 0.44s\n","  ğŸ§ª Batch 44/55 â€” Loss: 0.4991 â€” Time: 0.44s\n","  ğŸ§ª Batch 45/55 â€” Loss: 0.5336 â€” Time: 0.44s\n","  ğŸ§ª Batch 46/55 â€” Loss: 0.3987 â€” Time: 0.44s\n","  ğŸ§ª Batch 47/55 â€” Loss: 0.4054 â€” Time: 0.44s\n","  ğŸ§ª Batch 48/55 â€” Loss: 0.4329 â€” Time: 0.47s\n","  ğŸ§ª Batch 49/55 â€” Loss: 0.4938 â€” Time: 0.44s\n","  ğŸ§ª Batch 50/55 â€” Loss: 0.4531 â€” Time: 0.47s\n","  ğŸ§ª Batch 51/55 â€” Loss: 0.4052 â€” Time: 0.44s\n","  ğŸ§ª Batch 52/55 â€” Loss: 0.4089 â€” Time: 0.45s\n","  ğŸ§ª Batch 53/55 â€” Loss: 0.4598 â€” Time: 0.44s\n","  ğŸ§ª Batch 54/55 â€” Loss: 0.4206 â€” Time: 0.44s\n","  ğŸ§ª Batch 55/55 â€” Loss: 0.4601 â€” Time: 0.29s\n","ğŸ•’ Epoch Time: 27.09s | Avg Batch Time: 0.44s\n","ğŸ“Š Train Loss: 0.4671 | Val Loss: 0.5535 | IoU: 0.7579 | Dice: 0.8608 | Jaccard: 0.7579 | Precision: 0.9183 | Recall: 0.8121 | Accuracy: 0.9694\n","\n","Epoch 8/100\n","  ğŸ§ª Batch 1/55 â€” Loss: 0.4392 â€” Time: 0.44s\n","  ğŸ§ª Batch 2/55 â€” Loss: 0.4512 â€” Time: 0.44s\n","  ğŸ§ª Batch 3/55 â€” Loss: 0.5866 â€” Time: 0.44s\n","  ğŸ§ª Batch 4/55 â€” Loss: 0.3943 â€” Time: 0.45s\n","  ğŸ§ª Batch 5/55 â€” Loss: 0.4928 â€” Time: 0.44s\n","  ğŸ§ª Batch 6/55 â€” Loss: 0.3995 â€” Time: 0.44s\n","  ğŸ§ª Batch 7/55 â€” Loss: 0.4091 â€” Time: 0.44s\n","  ğŸ§ª Batch 8/55 â€” Loss: 0.4254 â€” Time: 0.44s\n","  ğŸ§ª Batch 9/55 â€” Loss: 0.3924 â€” Time: 0.45s\n","  ğŸ§ª Batch 10/55 â€” Loss: 0.4189 â€” Time: 0.44s\n","  ğŸ§ª Batch 11/55 â€” Loss: 0.3899 â€” Time: 0.44s\n","  ğŸ§ª Batch 12/55 â€” Loss: 0.4136 â€” Time: 0.44s\n","  ğŸ§ª Batch 13/55 â€” Loss: 0.4483 â€” Time: 0.44s\n","  ğŸ§ª Batch 14/55 â€” Loss: 0.4150 â€” Time: 0.44s\n","  ğŸ§ª Batch 15/55 â€” Loss: 0.3393 â€” Time: 0.44s\n","  ğŸ§ª Batch 16/55 â€” Loss: 0.4042 â€” Time: 0.44s\n","  ğŸ§ª Batch 17/55 â€” Loss: 0.3644 â€” Time: 0.45s\n","  ğŸ§ª Batch 18/55 â€” Loss: 0.3838 â€” Time: 0.50s\n","  ğŸ§ª Batch 19/55 â€” Loss: 0.3618 â€” Time: 0.47s\n","  ğŸ§ª Batch 20/55 â€” Loss: 0.4177 â€” Time: 0.44s\n","  ğŸ§ª Batch 21/55 â€” Loss: 0.4860 â€” Time: 0.49s\n","  ğŸ§ª Batch 22/55 â€” Loss: 0.3987 â€” Time: 0.47s\n","  ğŸ§ª Batch 23/55 â€” Loss: 0.4613 â€” Time: 0.44s\n","  ğŸ§ª Batch 24/55 â€” Loss: 0.3416 â€” Time: 0.44s\n","  ğŸ§ª Batch 25/55 â€” Loss: 0.3199 â€” Time: 0.44s\n","  ğŸ§ª Batch 26/55 â€” Loss: 0.4410 â€” Time: 0.44s\n","  ğŸ§ª Batch 27/55 â€” Loss: 0.4177 â€” Time: 0.44s\n","  ğŸ§ª Batch 28/55 â€” Loss: 0.5144 â€” Time: 0.44s\n","  ğŸ§ª Batch 29/55 â€” Loss: 0.3423 â€” Time: 0.44s\n","  ğŸ§ª Batch 30/55 â€” Loss: 0.4235 â€” Time: 0.44s\n","  ğŸ§ª Batch 31/55 â€” Loss: 0.4148 â€” Time: 0.44s\n","  ğŸ§ª Batch 32/55 â€” Loss: 0.4143 â€” Time: 0.44s\n","  ğŸ§ª Batch 33/55 â€” Loss: 0.3616 â€” Time: 0.44s\n","  ğŸ§ª Batch 34/55 â€” Loss: 0.4058 â€” Time: 0.44s\n","  ğŸ§ª Batch 35/55 â€” Loss: 0.3863 â€” Time: 0.44s\n","  ğŸ§ª Batch 36/55 â€” Loss: 0.3374 â€” Time: 0.44s\n","  ğŸ§ª Batch 37/55 â€” Loss: 0.3742 â€” Time: 0.44s\n","  ğŸ§ª Batch 38/55 â€” Loss: 0.3529 â€” Time: 0.44s\n","  ğŸ§ª Batch 39/55 â€” Loss: 0.3398 â€” Time: 0.44s\n","  ğŸ§ª Batch 40/55 â€” Loss: 0.4090 â€” Time: 0.44s\n","  ğŸ§ª Batch 41/55 â€” Loss: 0.3236 â€” Time: 0.44s\n","  ğŸ§ª Batch 42/55 â€” Loss: 0.4672 â€” Time: 0.44s\n","  ğŸ§ª Batch 43/55 â€” Loss: 0.4213 â€” Time: 0.44s\n","  ğŸ§ª Batch 44/55 â€” Loss: 0.3801 â€” Time: 0.44s\n","  ğŸ§ª Batch 45/55 â€” Loss: 0.3896 â€” Time: 0.44s\n","  ğŸ§ª Batch 46/55 â€” Loss: 0.3799 â€” Time: 0.44s\n","  ğŸ§ª Batch 47/55 â€” Loss: 0.3523 â€” Time: 0.44s\n","  ğŸ§ª Batch 48/55 â€” Loss: 0.3200 â€” Time: 0.44s\n","  ğŸ§ª Batch 49/55 â€” Loss: 0.4529 â€” Time: 0.47s\n","  ğŸ§ª Batch 50/55 â€” Loss: 0.4106 â€” Time: 0.44s\n","  ğŸ§ª Batch 51/55 â€” Loss: 0.3399 â€” Time: 0.45s\n","  ğŸ§ª Batch 52/55 â€” Loss: 0.3233 â€” Time: 0.47s\n","  ğŸ§ª Batch 53/55 â€” Loss: 0.4290 â€” Time: 0.44s\n","  ğŸ§ª Batch 54/55 â€” Loss: 0.3543 â€” Time: 0.44s\n","  ğŸ§ª Batch 55/55 â€” Loss: 0.3496 â€” Time: 0.28s\n","ğŸ•’ Epoch Time: 27.19s | Avg Batch Time: 0.44s\n","ğŸ“Š Train Loss: 0.3997 | Val Loss: 0.4801 | IoU: 0.7767 | Dice: 0.8734 | Jaccard: 0.7767 | Precision: 0.9182 | Recall: 0.8342 | Accuracy: 0.9718\n","\n","Epoch 9/100\n","  ğŸ§ª Batch 1/55 â€” Loss: 0.3605 â€” Time: 0.44s\n","  ğŸ§ª Batch 2/55 â€” Loss: 0.3874 â€” Time: 0.44s\n","  ğŸ§ª Batch 3/55 â€” Loss: 0.3335 â€” Time: 0.44s\n","  ğŸ§ª Batch 4/55 â€” Loss: 0.4260 â€” Time: 0.44s\n","  ğŸ§ª Batch 5/55 â€” Loss: 0.3259 â€” Time: 0.44s\n","  ğŸ§ª Batch 6/55 â€” Loss: 0.3708 â€” Time: 0.44s\n","  ğŸ§ª Batch 7/55 â€” Loss: 0.4848 â€” Time: 0.44s\n","  ğŸ§ª Batch 8/55 â€” Loss: 0.3896 â€” Time: 0.44s\n","  ğŸ§ª Batch 9/55 â€” Loss: 0.3979 â€” Time: 0.44s\n","  ğŸ§ª Batch 10/55 â€” Loss: 0.3912 â€” Time: 0.44s\n","  ğŸ§ª Batch 11/55 â€” Loss: 0.3518 â€” Time: 0.44s\n","  ğŸ§ª Batch 12/55 â€” Loss: 0.3710 â€” Time: 0.44s\n","  ğŸ§ª Batch 13/55 â€” Loss: 0.3521 â€” Time: 0.44s\n","  ğŸ§ª Batch 14/55 â€” Loss: 0.3329 â€” Time: 0.44s\n","  ğŸ§ª Batch 15/55 â€” Loss: 0.3043 â€” Time: 0.44s\n","  ğŸ§ª Batch 16/55 â€” Loss: 0.3619 â€” Time: 0.44s\n","  ğŸ§ª Batch 17/55 â€” Loss: 0.3070 â€” Time: 0.44s\n","  ğŸ§ª Batch 18/55 â€” Loss: 0.3376 â€” Time: 0.45s\n","  ğŸ§ª Batch 19/55 â€” Loss: 0.3111 â€” Time: 0.45s\n","  ğŸ§ª Batch 20/55 â€” Loss: 0.3903 â€” Time: 0.47s\n","  ğŸ§ª Batch 21/55 â€” Loss: 0.3286 â€” Time: 0.48s\n","  ğŸ§ª Batch 22/55 â€” Loss: 0.3602 â€” Time: 0.48s\n","  ğŸ§ª Batch 23/55 â€” Loss: 0.3077 â€” Time: 0.47s\n","  ğŸ§ª Batch 24/55 â€” Loss: 0.3679 â€” Time: 0.44s\n","  ğŸ§ª Batch 25/55 â€” Loss: 0.3382 â€” Time: 0.44s\n","  ğŸ§ª Batch 26/55 â€” Loss: 0.3389 â€” Time: 0.44s\n","  ğŸ§ª Batch 27/55 â€” Loss: 0.3048 â€” Time: 0.44s\n","  ğŸ§ª Batch 28/55 â€” Loss: 0.3011 â€” Time: 0.44s\n","  ğŸ§ª Batch 29/55 â€” Loss: 0.3815 â€” Time: 0.44s\n","  ğŸ§ª Batch 30/55 â€” Loss: 0.2721 â€” Time: 0.44s\n","  ğŸ§ª Batch 31/55 â€” Loss: 0.3072 â€” Time: 0.44s\n","  ğŸ§ª Batch 32/55 â€” Loss: 0.3586 â€” Time: 0.44s\n","  ğŸ§ª Batch 33/55 â€” Loss: 0.3330 â€” Time: 0.44s\n","  ğŸ§ª Batch 34/55 â€” Loss: 0.3248 â€” Time: 0.44s\n","  ğŸ§ª Batch 35/55 â€” Loss: 0.3114 â€” Time: 0.44s\n","  ğŸ§ª Batch 36/55 â€” Loss: 0.2996 â€” Time: 0.44s\n","  ğŸ§ª Batch 37/55 â€” Loss: 0.2930 â€” Time: 0.44s\n","  ğŸ§ª Batch 38/55 â€” Loss: 0.2710 â€” Time: 0.44s\n","  ğŸ§ª Batch 39/55 â€” Loss: 0.2882 â€” Time: 0.44s\n","  ğŸ§ª Batch 40/55 â€” Loss: 0.3116 â€” Time: 0.44s\n","  ğŸ§ª Batch 41/55 â€” Loss: 0.3556 â€” Time: 0.44s\n","  ğŸ§ª Batch 42/55 â€” Loss: 0.3985 â€” Time: 0.44s\n","  ğŸ§ª Batch 43/55 â€” Loss: 0.2773 â€” Time: 0.44s\n","  ğŸ§ª Batch 44/55 â€” Loss: 0.3193 â€” Time: 0.44s\n","  ğŸ§ª Batch 45/55 â€” Loss: 0.3082 â€” Time: 0.44s\n","  ğŸ§ª Batch 46/55 â€” Loss: 0.3216 â€” Time: 0.44s\n","  ğŸ§ª Batch 47/55 â€” Loss: 0.3402 â€” Time: 0.44s\n","  ğŸ§ª Batch 48/55 â€” Loss: 0.3207 â€” Time: 0.44s\n","  ğŸ§ª Batch 49/55 â€” Loss: 0.3093 â€” Time: 0.46s\n","  ğŸ§ª Batch 50/55 â€” Loss: 0.3365 â€” Time: 0.44s\n","  ğŸ§ª Batch 51/55 â€” Loss: 0.3771 â€” Time: 0.45s\n","  ğŸ§ª Batch 52/55 â€” Loss: 0.3523 â€” Time: 0.44s\n","  ğŸ§ª Batch 53/55 â€” Loss: 0.3717 â€” Time: 0.44s\n","  ğŸ§ª Batch 54/55 â€” Loss: 0.3952 â€” Time: 0.44s\n","  ğŸ§ª Batch 55/55 â€” Loss: 0.3017 â€” Time: 0.29s\n","ğŸ•’ Epoch Time: 27.25s | Avg Batch Time: 0.44s\n","ğŸ“Š Train Loss: 0.3413 | Val Loss: 0.4535 | IoU: 0.7714 | Dice: 0.8693 | Jaccard: 0.7714 | Precision: 0.9310 | Recall: 0.8172 | Accuracy: 0.9710\n","\n","Epoch 10/100\n","  ğŸ§ª Batch 1/55 â€” Loss: 0.2843 â€” Time: 0.47s\n","  ğŸ§ª Batch 2/55 â€” Loss: 0.2807 â€” Time: 0.44s\n","  ğŸ§ª Batch 3/55 â€” Loss: 0.3130 â€” Time: 0.44s\n","  ğŸ§ª Batch 4/55 â€” Loss: 0.2671 â€” Time: 0.44s\n","  ğŸ§ª Batch 5/55 â€” Loss: 0.2843 â€” Time: 0.46s\n","  ğŸ§ª Batch 6/55 â€” Loss: 0.3017 â€” Time: 0.44s\n","  ğŸ§ª Batch 7/55 â€” Loss: 0.2793 â€” Time: 0.44s\n","  ğŸ§ª Batch 8/55 â€” Loss: 0.3361 â€” Time: 0.44s\n","  ğŸ§ª Batch 9/55 â€” Loss: 0.3378 â€” Time: 0.44s\n","  ğŸ§ª Batch 10/55 â€” Loss: 0.4105 â€” Time: 0.44s\n","  ğŸ§ª Batch 11/55 â€” Loss: 0.3700 â€” Time: 0.44s\n","  ğŸ§ª Batch 12/55 â€” Loss: 0.3442 â€” Time: 0.44s\n","  ğŸ§ª Batch 13/55 â€” Loss: 0.2749 â€” Time: 0.44s\n","  ğŸ§ª Batch 14/55 â€” Loss: 0.3122 â€” Time: 0.44s\n","  ğŸ§ª Batch 15/55 â€” Loss: 0.4476 â€” Time: 0.44s\n","  ğŸ§ª Batch 16/55 â€” Loss: 0.3218 â€” Time: 0.44s\n","  ğŸ§ª Batch 17/55 â€” Loss: 0.2911 â€” Time: 0.45s\n","  ğŸ§ª Batch 18/55 â€” Loss: 0.2859 â€” Time: 0.44s\n","  ğŸ§ª Batch 19/55 â€” Loss: 0.3262 â€” Time: 0.44s\n","  ğŸ§ª Batch 20/55 â€” Loss: 0.3147 â€” Time: 0.46s\n","  ğŸ§ª Batch 21/55 â€” Loss: 0.3273 â€” Time: 0.44s\n","  ğŸ§ª Batch 22/55 â€” Loss: 0.3148 â€” Time: 0.44s\n","  ğŸ§ª Batch 23/55 â€” Loss: 0.3428 â€” Time: 0.48s\n","  ğŸ§ª Batch 24/55 â€” Loss: 0.3630 â€” Time: 0.48s\n","  ğŸ§ª Batch 25/55 â€” Loss: 0.2892 â€” Time: 0.45s\n","  ğŸ§ª Batch 26/55 â€” Loss: 0.2719 â€” Time: 0.47s\n","  ğŸ§ª Batch 27/55 â€” Loss: 0.3125 â€” Time: 0.44s\n","  ğŸ§ª Batch 28/55 â€” Loss: 0.3147 â€” Time: 0.44s\n","  ğŸ§ª Batch 29/55 â€” Loss: 0.3162 â€” Time: 0.44s\n","  ğŸ§ª Batch 30/55 â€” Loss: 0.2761 â€” Time: 0.44s\n","  ğŸ§ª Batch 31/55 â€” Loss: 0.3056 â€” Time: 0.44s\n","  ğŸ§ª Batch 32/55 â€” Loss: 0.2864 â€” Time: 0.44s\n","  ğŸ§ª Batch 33/55 â€” Loss: 0.2848 â€” Time: 0.44s\n","  ğŸ§ª Batch 34/55 â€” Loss: 0.3129 â€” Time: 0.44s\n","  ğŸ§ª Batch 35/55 â€” Loss: 0.2588 â€” Time: 0.44s\n","  ğŸ§ª Batch 36/55 â€” Loss: 0.3460 â€” Time: 0.44s\n","  ğŸ§ª Batch 37/55 â€” Loss: 0.2830 â€” Time: 0.44s\n","  ğŸ§ª Batch 38/55 â€” Loss: 0.2515 â€” Time: 0.44s\n","  ğŸ§ª Batch 39/55 â€” Loss: 0.2948 â€” Time: 0.44s\n","  ğŸ§ª Batch 40/55 â€” Loss: 0.3430 â€” Time: 0.44s\n","  ğŸ§ª Batch 41/55 â€” Loss: 0.2459 â€” Time: 0.44s\n","  ğŸ§ª Batch 42/55 â€” Loss: 0.3131 â€” Time: 0.44s\n","  ğŸ§ª Batch 43/55 â€” Loss: 0.3279 â€” Time: 0.44s\n","  ğŸ§ª Batch 44/55 â€” Loss: 0.2611 â€” Time: 0.44s\n","  ğŸ§ª Batch 45/55 â€” Loss: 0.3505 â€” Time: 0.44s\n","  ğŸ§ª Batch 46/55 â€” Loss: 0.2477 â€” Time: 0.44s\n","  ğŸ§ª Batch 47/55 â€” Loss: 0.2692 â€” Time: 0.44s\n","  ğŸ§ª Batch 48/55 â€” Loss: 0.3430 â€” Time: 0.44s\n","  ğŸ§ª Batch 49/55 â€” Loss: 0.2729 â€” Time: 0.44s\n","  ğŸ§ª Batch 50/55 â€” Loss: 0.2662 â€” Time: 0.44s\n","  ğŸ§ª Batch 51/55 â€” Loss: 0.2689 â€” Time: 0.49s\n","  ğŸ§ª Batch 52/55 â€” Loss: 0.2627 â€” Time: 0.45s\n","  ğŸ§ª Batch 53/55 â€” Loss: 0.2939 â€” Time: 0.44s\n","  ğŸ§ª Batch 54/55 â€” Loss: 0.3674 â€” Time: 0.44s\n","  ğŸ§ª Batch 55/55 â€” Loss: 0.3604 â€” Time: 0.29s\n","ğŸ•’ Epoch Time: 27.49s | Avg Batch Time: 0.44s\n","ğŸ“Š Train Loss: 0.3078 | Val Loss: 0.3903 | IoU: 0.7976 | Dice: 0.8859 | Jaccard: 0.7976 | Precision: 0.8832 | Recall: 0.8907 | Accuracy: 0.9724\n","\n","Epoch 11/100\n","  ğŸ§ª Batch 1/55 â€” Loss: 0.2479 â€” Time: 0.46s\n","  ğŸ§ª Batch 2/55 â€” Loss: 0.2421 â€” Time: 0.44s\n","  ğŸ§ª Batch 3/55 â€” Loss: 0.2441 â€” Time: 0.44s\n","  ğŸ§ª Batch 4/55 â€” Loss: 0.2509 â€” Time: 0.44s\n","  ğŸ§ª Batch 5/55 â€” Loss: 0.2529 â€” Time: 0.44s\n","  ğŸ§ª Batch 6/55 â€” Loss: 0.2454 â€” Time: 0.44s\n","  ğŸ§ª Batch 7/55 â€” Loss: 0.2494 â€” Time: 0.44s\n","  ğŸ§ª Batch 8/55 â€” Loss: 0.2557 â€” Time: 0.44s\n","  ğŸ§ª Batch 9/55 â€” Loss: 0.3378 â€” Time: 0.44s\n","  ğŸ§ª Batch 10/55 â€” Loss: 0.3242 â€” Time: 0.44s\n","  ğŸ§ª Batch 11/55 â€” Loss: 0.2099 â€” Time: 0.44s\n","  ğŸ§ª Batch 12/55 â€” Loss: 0.2585 â€” Time: 0.44s\n","  ğŸ§ª Batch 13/55 â€” Loss: 0.2570 â€” Time: 0.44s\n","  ğŸ§ª Batch 14/55 â€” Loss: 0.2536 â€” Time: 0.44s\n","  ğŸ§ª Batch 15/55 â€” Loss: 0.2945 â€” Time: 0.44s\n","  ğŸ§ª Batch 16/55 â€” Loss: 0.3039 â€” Time: 0.44s\n","  ğŸ§ª Batch 17/55 â€” Loss: 0.3296 â€” Time: 0.44s\n","  ğŸ§ª Batch 18/55 â€” Loss: 0.2421 â€” Time: 0.44s\n","  ğŸ§ª Batch 19/55 â€” Loss: 0.3270 â€” Time: 0.45s\n","  ğŸ§ª Batch 20/55 â€” Loss: 0.2558 â€” Time: 0.47s\n","  ğŸ§ª Batch 21/55 â€” Loss: 0.3452 â€” Time: 0.47s\n","  ğŸ§ª Batch 22/55 â€” Loss: 0.2592 â€” Time: 0.45s\n","  ğŸ§ª Batch 23/55 â€” Loss: 0.2748 â€” Time: 0.48s\n","  ğŸ§ª Batch 24/55 â€” Loss: 0.2528 â€” Time: 0.52s\n","  ğŸ§ª Batch 25/55 â€” Loss: 0.2538 â€” Time: 0.50s\n","  ğŸ§ª Batch 26/55 â€” Loss: 0.2616 â€” Time: 0.45s\n","  ğŸ§ª Batch 27/55 â€” Loss: 0.2791 â€” Time: 0.44s\n","  ğŸ§ª Batch 28/55 â€” Loss: 0.2739 â€” Time: 0.44s\n","  ğŸ§ª Batch 29/55 â€” Loss: 0.2105 â€” Time: 0.44s\n","  ğŸ§ª Batch 30/55 â€” Loss: 0.3003 â€” Time: 0.44s\n","  ğŸ§ª Batch 31/55 â€” Loss: 0.2365 â€” Time: 0.44s\n","  ğŸ§ª Batch 32/55 â€” Loss: 0.2250 â€” Time: 0.44s\n","  ğŸ§ª Batch 33/55 â€” Loss: 0.2705 â€” Time: 0.44s\n","  ğŸ§ª Batch 34/55 â€” Loss: 0.2233 â€” Time: 0.44s\n","  ğŸ§ª Batch 35/55 â€” Loss: 0.2449 â€” Time: 0.44s\n","  ğŸ§ª Batch 36/55 â€” Loss: 0.2868 â€” Time: 0.44s\n","  ğŸ§ª Batch 37/55 â€” Loss: 0.2367 â€” Time: 0.44s\n","  ğŸ§ª Batch 38/55 â€” Loss: 0.3550 â€” Time: 0.44s\n","  ğŸ§ª Batch 39/55 â€” Loss: 0.2580 â€” Time: 0.44s\n","  ğŸ§ª Batch 40/55 â€” Loss: 0.2301 â€” Time: 0.44s\n","  ğŸ§ª Batch 41/55 â€” Loss: 0.2117 â€” Time: 0.44s\n","  ğŸ§ª Batch 42/55 â€” Loss: 0.2480 â€” Time: 0.44s\n","  ğŸ§ª Batch 43/55 â€” Loss: 0.2240 â€” Time: 0.44s\n","  ğŸ§ª Batch 44/55 â€” Loss: 0.2409 â€” Time: 0.44s\n","  ğŸ§ª Batch 45/55 â€” Loss: 0.2658 â€” Time: 0.44s\n","  ğŸ§ª Batch 46/55 â€” Loss: 0.2399 â€” Time: 0.44s\n","  ğŸ§ª Batch 47/55 â€” Loss: 0.2695 â€” Time: 0.44s\n","  ğŸ§ª Batch 48/55 â€” Loss: 0.2257 â€” Time: 0.44s\n","  ğŸ§ª Batch 49/55 â€” Loss: 0.2063 â€” Time: 0.44s\n","  ğŸ§ª Batch 50/55 â€” Loss: 0.2196 â€” Time: 0.44s\n","  ğŸ§ª Batch 51/55 â€” Loss: 0.2749 â€” Time: 0.48s\n","  ğŸ§ª Batch 52/55 â€” Loss: 0.2643 â€” Time: 0.45s\n","  ğŸ§ª Batch 53/55 â€” Loss: 0.2474 â€” Time: 0.44s\n","  ğŸ§ª Batch 54/55 â€” Loss: 0.2298 â€” Time: 0.44s\n","  ğŸ§ª Batch 55/55 â€” Loss: 0.2300 â€” Time: 0.29s\n","ğŸ•’ Epoch Time: 27.52s | Avg Batch Time: 0.44s\n","ğŸ“Š Train Loss: 0.2592 | Val Loss: 0.3615 | IoU: 0.8011 | Dice: 0.8877 | Jaccard: 0.8011 | Precision: 0.9110 | Recall: 0.8671 | Accuracy: 0.9735\n","\n","Epoch 12/100\n","  ğŸ§ª Batch 1/55 â€” Loss: 0.2165 â€” Time: 0.44s\n","  ğŸ§ª Batch 2/55 â€” Loss: 0.2193 â€” Time: 0.43s\n","  ğŸ§ª Batch 3/55 â€” Loss: 0.2654 â€” Time: 0.44s\n","  ğŸ§ª Batch 4/55 â€” Loss: 0.2384 â€” Time: 0.44s\n","  ğŸ§ª Batch 5/55 â€” Loss: 0.2448 â€” Time: 0.44s\n","  ğŸ§ª Batch 6/55 â€” Loss: 0.2008 â€” Time: 0.44s\n","  ğŸ§ª Batch 7/55 â€” Loss: 0.2044 â€” Time: 0.44s\n","  ğŸ§ª Batch 8/55 â€” Loss: 0.1821 â€” Time: 0.44s\n","  ğŸ§ª Batch 9/55 â€” Loss: 0.2096 â€” Time: 0.44s\n","  ğŸ§ª Batch 10/55 â€” Loss: 0.2190 â€” Time: 0.44s\n","  ğŸ§ª Batch 11/55 â€” Loss: 0.2737 â€” Time: 0.44s\n","  ğŸ§ª Batch 12/55 â€” Loss: 0.2116 â€” Time: 0.44s\n","  ğŸ§ª Batch 13/55 â€” Loss: 0.3192 â€” Time: 0.44s\n","  ğŸ§ª Batch 14/55 â€” Loss: 0.2396 â€” Time: 0.44s\n","  ğŸ§ª Batch 15/55 â€” Loss: 0.2139 â€” Time: 0.44s\n","  ğŸ§ª Batch 16/55 â€” Loss: 0.1912 â€” Time: 0.44s\n","  ğŸ§ª Batch 17/55 â€” Loss: 0.2763 â€” Time: 0.44s\n","  ğŸ§ª Batch 18/55 â€” Loss: 0.2407 â€” Time: 0.44s\n","  ğŸ§ª Batch 19/55 â€” Loss: 0.2586 â€” Time: 0.44s\n","  ğŸ§ª Batch 20/55 â€” Loss: 0.2508 â€” Time: 0.46s\n","  ğŸ§ª Batch 21/55 â€” Loss: 0.2256 â€” Time: 0.47s\n","  ğŸ§ª Batch 22/55 â€” Loss: 0.2314 â€” Time: 0.44s\n","  ğŸ§ª Batch 23/55 â€” Loss: 0.2000 â€” Time: 0.44s\n","  ğŸ§ª Batch 24/55 â€” Loss: 0.1856 â€” Time: 0.48s\n","  ğŸ§ª Batch 25/55 â€” Loss: 0.2039 â€” Time: 0.47s\n","  ğŸ§ª Batch 26/55 â€” Loss: 0.2160 â€” Time: 0.45s\n","  ğŸ§ª Batch 27/55 â€” Loss: 0.2629 â€” Time: 0.44s\n","  ğŸ§ª Batch 28/55 â€” Loss: 0.2108 â€” Time: 0.44s\n","  ğŸ§ª Batch 29/55 â€” Loss: 0.1895 â€” Time: 0.44s\n","  ğŸ§ª Batch 30/55 â€” Loss: 0.2443 â€” Time: 0.44s\n","  ğŸ§ª Batch 31/55 â€” Loss: 0.2236 â€” Time: 0.45s\n","  ğŸ§ª Batch 32/55 â€” Loss: 0.2353 â€” Time: 0.44s\n","  ğŸ§ª Batch 33/55 â€” Loss: 0.2167 â€” Time: 0.44s\n","  ğŸ§ª Batch 34/55 â€” Loss: 0.2194 â€” Time: 0.44s\n","  ğŸ§ª Batch 35/55 â€” Loss: 0.2269 â€” Time: 0.44s\n","  ğŸ§ª Batch 36/55 â€” Loss: 0.2408 â€” Time: 0.44s\n","  ğŸ§ª Batch 37/55 â€” Loss: 0.2861 â€” Time: 0.44s\n","  ğŸ§ª Batch 38/55 â€” Loss: 0.2154 â€” Time: 0.44s\n","  ğŸ§ª Batch 39/55 â€” Loss: 0.1899 â€” Time: 0.44s\n","  ğŸ§ª Batch 40/55 â€” Loss: 0.2098 â€” Time: 0.44s\n","  ğŸ§ª Batch 41/55 â€” Loss: 0.2412 â€” Time: 0.44s\n","  ğŸ§ª Batch 42/55 â€” Loss: 0.2293 â€” Time: 0.44s\n","  ğŸ§ª Batch 43/55 â€” Loss: 0.2702 â€” Time: 0.44s\n","  ğŸ§ª Batch 44/55 â€” Loss: 0.2295 â€” Time: 0.44s\n","  ğŸ§ª Batch 45/55 â€” Loss: 0.2309 â€” Time: 0.44s\n","  ğŸ§ª Batch 46/55 â€” Loss: 0.2151 â€” Time: 0.44s\n","  ğŸ§ª Batch 47/55 â€” Loss: 0.2067 â€” Time: 0.44s\n","  ğŸ§ª Batch 48/55 â€” Loss: 0.2221 â€” Time: 0.44s\n","  ğŸ§ª Batch 49/55 â€” Loss: 0.2587 â€” Time: 0.44s\n","  ğŸ§ª Batch 50/55 â€” Loss: 0.2128 â€” Time: 0.44s\n","  ğŸ§ª Batch 51/55 â€” Loss: 0.1948 â€” Time: 0.44s\n","  ğŸ§ª Batch 52/55 â€” Loss: 0.1936 â€” Time: 0.44s\n","  ğŸ§ª Batch 53/55 â€” Loss: 0.1889 â€” Time: 0.44s\n","  ğŸ§ª Batch 54/55 â€” Loss: 0.1977 â€” Time: 0.44s\n","  ğŸ§ª Batch 55/55 â€” Loss: 0.2286 â€” Time: 0.29s\n","ğŸ•’ Epoch Time: 27.48s | Avg Batch Time: 0.44s\n","ğŸ“Š Train Loss: 0.2260 | Val Loss: 0.3458 | IoU: 0.7928 | Dice: 0.8826 | Jaccard: 0.7928 | Precision: 0.8741 | Recall: 0.8925 | Accuracy: 0.9715\n","\n","Epoch 13/100\n","  ğŸ§ª Batch 1/55 â€” Loss: 0.1815 â€” Time: 0.45s\n","  ğŸ§ª Batch 2/55 â€” Loss: 0.1845 â€” Time: 0.44s\n","  ğŸ§ª Batch 3/55 â€” Loss: 0.2142 â€” Time: 0.44s\n","  ğŸ§ª Batch 4/55 â€” Loss: 0.1823 â€” Time: 0.44s\n","  ğŸ§ª Batch 5/55 â€” Loss: 0.1811 â€” Time: 0.44s\n","  ğŸ§ª Batch 6/55 â€” Loss: 0.1891 â€” Time: 0.44s\n","  ğŸ§ª Batch 7/55 â€” Loss: 0.2621 â€” Time: 0.44s\n","  ğŸ§ª Batch 8/55 â€” Loss: 0.1897 â€” Time: 0.44s\n","  ğŸ§ª Batch 9/55 â€” Loss: 0.2062 â€” Time: 0.44s\n","  ğŸ§ª Batch 10/55 â€” Loss: 0.1794 â€” Time: 0.44s\n","  ğŸ§ª Batch 11/55 â€” Loss: 0.1947 â€” Time: 0.44s\n","  ğŸ§ª Batch 12/55 â€” Loss: 0.2887 â€” Time: 0.44s\n","  ğŸ§ª Batch 13/55 â€” Loss: 0.1963 â€” Time: 0.44s\n","  ğŸ§ª Batch 14/55 â€” Loss: 0.2233 â€” Time: 0.44s\n","  ğŸ§ª Batch 15/55 â€” Loss: 0.1805 â€” Time: 0.44s\n","  ğŸ§ª Batch 16/55 â€” Loss: 0.1894 â€” Time: 0.44s\n","  ğŸ§ª Batch 17/55 â€” Loss: 0.1976 â€” Time: 0.44s\n","  ğŸ§ª Batch 18/55 â€” Loss: 0.2602 â€” Time: 0.44s\n","  ğŸ§ª Batch 19/55 â€” Loss: 0.2168 â€” Time: 0.44s\n","  ğŸ§ª Batch 20/55 â€” Loss: 0.3289 â€” Time: 0.44s\n","  ğŸ§ª Batch 21/55 â€” Loss: 0.1927 â€” Time: 0.47s\n","  ğŸ§ª Batch 22/55 â€” Loss: 0.1908 â€” Time: 0.47s\n","  ğŸ§ª Batch 23/55 â€” Loss: 0.2146 â€” Time: 0.47s\n","  ğŸ§ª Batch 24/55 â€” Loss: 0.2094 â€” Time: 0.44s\n","  ğŸ§ª Batch 25/55 â€” Loss: 0.1713 â€” Time: 0.45s\n","  ğŸ§ª Batch 26/55 â€” Loss: 0.2395 â€” Time: 0.49s\n","  ğŸ§ª Batch 27/55 â€” Loss: 0.2208 â€” Time: 0.44s\n","  ğŸ§ª Batch 28/55 â€” Loss: 0.2073 â€” Time: 0.44s\n","  ğŸ§ª Batch 29/55 â€” Loss: 0.2042 â€” Time: 0.44s\n","  ğŸ§ª Batch 30/55 â€” Loss: 0.2075 â€” Time: 0.45s\n","  ğŸ§ª Batch 31/55 â€” Loss: 0.2539 â€” Time: 0.44s\n","  ğŸ§ª Batch 32/55 â€” Loss: 0.1641 â€” Time: 0.44s\n","  ğŸ§ª Batch 33/55 â€” Loss: 0.2195 â€” Time: 0.44s\n","  ğŸ§ª Batch 34/55 â€” Loss: 0.2204 â€” Time: 0.44s\n","  ğŸ§ª Batch 35/55 â€” Loss: 0.2299 â€” Time: 0.44s\n","  ğŸ§ª Batch 36/55 â€” Loss: 0.2170 â€” Time: 0.44s\n","  ğŸ§ª Batch 37/55 â€” Loss: 0.2039 â€” Time: 0.44s\n","  ğŸ§ª Batch 38/55 â€” Loss: 0.2236 â€” Time: 0.44s\n","  ğŸ§ª Batch 39/55 â€” Loss: 0.2221 â€” Time: 0.44s\n","  ğŸ§ª Batch 40/55 â€” Loss: 0.1996 â€” Time: 0.44s\n","  ğŸ§ª Batch 41/55 â€” Loss: 0.1727 â€” Time: 0.44s\n","  ğŸ§ª Batch 42/55 â€” Loss: 0.1793 â€” Time: 0.44s\n","  ğŸ§ª Batch 43/55 â€” Loss: 0.2141 â€” Time: 0.44s\n","  ğŸ§ª Batch 44/55 â€” Loss: 0.2000 â€” Time: 0.44s\n","  ğŸ§ª Batch 45/55 â€” Loss: 0.2132 â€” Time: 0.44s\n","  ğŸ§ª Batch 46/55 â€” Loss: 0.1720 â€” Time: 0.44s\n","  ğŸ§ª Batch 47/55 â€” Loss: 0.3092 â€” Time: 0.44s\n","  ğŸ§ª Batch 48/55 â€” Loss: 0.1958 â€” Time: 0.44s\n","  ğŸ§ª Batch 49/55 â€” Loss: 0.1754 â€” Time: 0.44s\n","  ğŸ§ª Batch 50/55 â€” Loss: 0.2135 â€” Time: 0.45s\n","  ğŸ§ª Batch 51/55 â€” Loss: 0.1820 â€” Time: 0.48s\n","  ğŸ§ª Batch 52/55 â€” Loss: 0.1709 â€” Time: 0.44s\n","  ğŸ§ª Batch 53/55 â€” Loss: 0.2039 â€” Time: 0.44s\n","  ğŸ§ª Batch 54/55 â€” Loss: 0.1703 â€” Time: 0.44s\n","  ğŸ§ª Batch 55/55 â€” Loss: 0.2347 â€” Time: 0.29s\n","ğŸ•’ Epoch Time: 27.54s | Avg Batch Time: 0.44s\n","ğŸ“Š Train Loss: 0.2085 | Val Loss: 0.3149 | IoU: 0.8069 | Dice: 0.8915 | Jaccard: 0.8069 | Precision: 0.9222 | Recall: 0.8642 | Accuracy: 0.9746\n","\n","Epoch 14/100\n","  ğŸ§ª Batch 1/55 â€” Loss: 0.2081 â€” Time: 0.46s\n","  ğŸ§ª Batch 2/55 â€” Loss: 0.1751 â€” Time: 0.44s\n","  ğŸ§ª Batch 3/55 â€” Loss: 0.1574 â€” Time: 0.44s\n","  ğŸ§ª Batch 4/55 â€” Loss: 0.1579 â€” Time: 0.44s\n","  ğŸ§ª Batch 5/55 â€” Loss: 0.1906 â€” Time: 0.44s\n","  ğŸ§ª Batch 6/55 â€” Loss: 0.2310 â€” Time: 0.44s\n","  ğŸ§ª Batch 7/55 â€” Loss: 0.1775 â€” Time: 0.44s\n","  ğŸ§ª Batch 8/55 â€” Loss: 0.2094 â€” Time: 0.44s\n","  ğŸ§ª Batch 9/55 â€” Loss: 0.1736 â€” Time: 0.44s\n","  ğŸ§ª Batch 10/55 â€” Loss: 0.1915 â€” Time: 0.44s\n","  ğŸ§ª Batch 11/55 â€” Loss: 0.1529 â€” Time: 0.44s\n","  ğŸ§ª Batch 12/55 â€” Loss: 0.1581 â€” Time: 0.44s\n","  ğŸ§ª Batch 13/55 â€” Loss: 0.2061 â€” Time: 0.44s\n","  ğŸ§ª Batch 14/55 â€” Loss: 0.1880 â€” Time: 0.44s\n","  ğŸ§ª Batch 15/55 â€” Loss: 0.2790 â€” Time: 0.44s\n","  ğŸ§ª Batch 16/55 â€” Loss: 0.1916 â€” Time: 0.44s\n","  ğŸ§ª Batch 17/55 â€” Loss: 0.1571 â€” Time: 0.44s\n","  ğŸ§ª Batch 18/55 â€” Loss: 0.2004 â€” Time: 0.44s\n","  ğŸ§ª Batch 19/55 â€” Loss: 0.1721 â€” Time: 0.44s\n","  ğŸ§ª Batch 20/55 â€” Loss: 0.1785 â€” Time: 0.45s\n","  ğŸ§ª Batch 21/55 â€” Loss: 0.1938 â€” Time: 0.44s\n","  ğŸ§ª Batch 22/55 â€” Loss: 0.1856 â€” Time: 0.44s\n","  ğŸ§ª Batch 23/55 â€” Loss: 0.2090 â€” Time: 0.44s\n","  ğŸ§ª Batch 24/55 â€” Loss: 0.2673 â€” Time: 0.51s\n","  ğŸ§ª Batch 25/55 â€” Loss: 0.2015 â€” Time: 0.50s\n","  ğŸ§ª Batch 26/55 â€” Loss: 0.1906 â€” Time: 0.46s\n","  ğŸ§ª Batch 27/55 â€” Loss: 0.1811 â€” Time: 0.45s\n","  ğŸ§ª Batch 28/55 â€” Loss: 0.1939 â€” Time: 0.44s\n","  ğŸ§ª Batch 29/55 â€” Loss: 0.2127 â€” Time: 0.44s\n","  ğŸ§ª Batch 30/55 â€” Loss: 0.1961 â€” Time: 0.44s\n","  ğŸ§ª Batch 31/55 â€” Loss: 0.2000 â€” Time: 0.44s\n","  ğŸ§ª Batch 32/55 â€” Loss: 0.1689 â€” Time: 0.44s\n","  ğŸ§ª Batch 33/55 â€” Loss: 0.1873 â€” Time: 0.44s\n","  ğŸ§ª Batch 34/55 â€” Loss: 0.1524 â€” Time: 0.44s\n","  ğŸ§ª Batch 35/55 â€” Loss: 0.1841 â€” Time: 0.44s\n","  ğŸ§ª Batch 36/55 â€” Loss: 0.2132 â€” Time: 0.44s\n","  ğŸ§ª Batch 37/55 â€” Loss: 0.2215 â€” Time: 0.44s\n","  ğŸ§ª Batch 38/55 â€” Loss: 0.1954 â€” Time: 0.44s\n","  ğŸ§ª Batch 39/55 â€” Loss: 0.1461 â€” Time: 0.44s\n","  ğŸ§ª Batch 40/55 â€” Loss: 0.1958 â€” Time: 0.44s\n","  ğŸ§ª Batch 41/55 â€” Loss: 0.1410 â€” Time: 0.44s\n","  ğŸ§ª Batch 42/55 â€” Loss: 0.1614 â€” Time: 0.44s\n","  ğŸ§ª Batch 43/55 â€” Loss: 0.1664 â€” Time: 0.45s\n","  ğŸ§ª Batch 44/55 â€” Loss: 0.1405 â€” Time: 0.44s\n","  ğŸ§ª Batch 45/55 â€” Loss: 0.2030 â€” Time: 0.44s\n","  ğŸ§ª Batch 46/55 â€” Loss: 0.2107 â€” Time: 0.44s\n","  ğŸ§ª Batch 47/55 â€” Loss: 0.1620 â€” Time: 0.44s\n","  ğŸ§ª Batch 48/55 â€” Loss: 0.1719 â€” Time: 0.44s\n","  ğŸ§ª Batch 49/55 â€” Loss: 0.1959 â€” Time: 0.44s\n","  ğŸ§ª Batch 50/55 â€” Loss: 0.1489 â€” Time: 0.44s\n","  ğŸ§ª Batch 51/55 â€” Loss: 0.1555 â€” Time: 0.45s\n","  ğŸ§ª Batch 52/55 â€” Loss: 0.1806 â€” Time: 0.44s\n","  ğŸ§ª Batch 53/55 â€” Loss: 0.1953 â€” Time: 0.44s\n","  ğŸ§ª Batch 54/55 â€” Loss: 0.2260 â€” Time: 0.44s\n","  ğŸ§ª Batch 55/55 â€” Loss: 0.1717 â€” Time: 0.28s\n","ğŸ•’ Epoch Time: 27.48s | Avg Batch Time: 0.44s\n","ğŸ“Š Train Loss: 0.1870 | Val Loss: 0.3067 | IoU: 0.8051 | Dice: 0.8904 | Jaccard: 0.8051 | Precision: 0.9148 | Recall: 0.8687 | Accuracy: 0.9740\n","\n","Epoch 15/100\n","  ğŸ§ª Batch 1/55 â€” Loss: 0.1862 â€” Time: 0.46s\n","  ğŸ§ª Batch 2/55 â€” Loss: 0.1961 â€” Time: 0.44s\n","  ğŸ§ª Batch 3/55 â€” Loss: 0.1625 â€” Time: 0.44s\n","  ğŸ§ª Batch 4/55 â€” Loss: 0.2191 â€” Time: 0.44s\n","  ğŸ§ª Batch 5/55 â€” Loss: 0.1640 â€” Time: 0.46s\n","  ğŸ§ª Batch 6/55 â€” Loss: 0.1984 â€” Time: 0.44s\n","  ğŸ§ª Batch 7/55 â€” Loss: 0.1642 â€” Time: 0.44s\n","  ğŸ§ª Batch 8/55 â€” Loss: 0.1664 â€” Time: 0.44s\n","  ğŸ§ª Batch 9/55 â€” Loss: 0.1690 â€” Time: 0.44s\n","  ğŸ§ª Batch 10/55 â€” Loss: 0.1550 â€” Time: 0.44s\n","  ğŸ§ª Batch 11/55 â€” Loss: 0.1568 â€” Time: 0.44s\n","  ğŸ§ª Batch 12/55 â€” Loss: 0.2157 â€” Time: 0.44s\n","  ğŸ§ª Batch 13/55 â€” Loss: 0.1443 â€” Time: 0.44s\n","  ğŸ§ª Batch 14/55 â€” Loss: 0.1931 â€” Time: 0.44s\n","  ğŸ§ª Batch 15/55 â€” Loss: 0.1459 â€” Time: 0.44s\n","  ğŸ§ª Batch 16/55 â€” Loss: 0.2149 â€” Time: 0.44s\n","  ğŸ§ª Batch 17/55 â€” Loss: 0.1962 â€” Time: 0.44s\n","  ğŸ§ª Batch 18/55 â€” Loss: 0.1927 â€” Time: 0.44s\n","  ğŸ§ª Batch 19/55 â€” Loss: 0.1603 â€” Time: 0.45s\n","  ğŸ§ª Batch 20/55 â€” Loss: 0.1499 â€” Time: 0.44s\n","  ğŸ§ª Batch 21/55 â€” Loss: 0.1579 â€” Time: 0.47s\n","  ğŸ§ª Batch 22/55 â€” Loss: 0.1424 â€” Time: 0.45s\n","  ğŸ§ª Batch 23/55 â€” Loss: 0.1549 â€” Time: 0.49s\n","  ğŸ§ª Batch 24/55 â€” Loss: 0.1587 â€” Time: 0.47s\n","  ğŸ§ª Batch 25/55 â€” Loss: 0.1588 â€” Time: 0.49s\n","  ğŸ§ª Batch 26/55 â€” Loss: 0.1549 â€” Time: 0.46s\n","  ğŸ§ª Batch 27/55 â€” Loss: 0.1711 â€” Time: 0.44s\n","  ğŸ§ª Batch 28/55 â€” Loss: 0.1423 â€” Time: 0.44s\n","  ğŸ§ª Batch 29/55 â€” Loss: 0.1784 â€” Time: 0.44s\n","  ğŸ§ª Batch 30/55 â€” Loss: 0.1912 â€” Time: 0.44s\n","  ğŸ§ª Batch 31/55 â€” Loss: 0.1920 â€” Time: 0.45s\n","  ğŸ§ª Batch 32/55 â€” Loss: 0.1687 â€” Time: 0.44s\n","  ğŸ§ª Batch 33/55 â€” Loss: 0.1915 â€” Time: 0.44s\n","  ğŸ§ª Batch 34/55 â€” Loss: 0.1443 â€” Time: 0.44s\n","  ğŸ§ª Batch 35/55 â€” Loss: 0.1380 â€” Time: 0.44s\n","  ğŸ§ª Batch 36/55 â€” Loss: 0.1331 â€” Time: 0.44s\n","  ğŸ§ª Batch 37/55 â€” Loss: 0.1601 â€” Time: 0.44s\n","  ğŸ§ª Batch 38/55 â€” Loss: 0.1487 â€” Time: 0.44s\n","  ğŸ§ª Batch 39/55 â€” Loss: 0.1448 â€” Time: 0.44s\n","  ğŸ§ª Batch 40/55 â€” Loss: 0.1574 â€” Time: 0.44s\n","  ğŸ§ª Batch 41/55 â€” Loss: 0.1629 â€” Time: 0.44s\n","  ğŸ§ª Batch 42/55 â€” Loss: 0.1928 â€” Time: 0.44s\n","  ğŸ§ª Batch 43/55 â€” Loss: 0.1499 â€” Time: 0.44s\n","  ğŸ§ª Batch 44/55 â€” Loss: 0.2100 â€” Time: 0.44s\n","  ğŸ§ª Batch 45/55 â€” Loss: 0.1266 â€” Time: 0.44s\n","  ğŸ§ª Batch 46/55 â€” Loss: 0.1985 â€” Time: 0.44s\n","  ğŸ§ª Batch 47/55 â€” Loss: 0.1784 â€” Time: 0.44s\n","  ğŸ§ª Batch 48/55 â€” Loss: 0.1283 â€” Time: 0.44s\n","  ğŸ§ª Batch 49/55 â€” Loss: 0.1923 â€” Time: 0.44s\n","  ğŸ§ª Batch 50/55 â€” Loss: 0.1388 â€” Time: 0.45s\n","  ğŸ§ª Batch 51/55 â€” Loss: 0.1382 â€” Time: 0.45s\n","  ğŸ§ª Batch 52/55 â€” Loss: 0.1715 â€” Time: 0.45s\n","  ğŸ§ª Batch 53/55 â€” Loss: 0.2012 â€” Time: 0.44s\n","  ğŸ§ª Batch 54/55 â€” Loss: 0.1687 â€” Time: 0.44s\n","  ğŸ§ª Batch 55/55 â€” Loss: 0.1380 â€” Time: 0.29s\n","ğŸ•’ Epoch Time: 27.60s | Avg Batch Time: 0.44s\n","ğŸ“Š Train Loss: 0.1679 | Val Loss: 0.2913 | IoU: 0.8077 | Dice: 0.8918 | Jaccard: 0.8077 | Precision: 0.9223 | Recall: 0.8650 | Accuracy: 0.9744\n","\n","Epoch 16/100\n","  ğŸ§ª Batch 1/55 â€” Loss: 0.1670 â€” Time: 0.44s\n","  ğŸ§ª Batch 2/55 â€” Loss: 0.1527 â€” Time: 0.44s\n","  ğŸ§ª Batch 3/55 â€” Loss: 0.1517 â€” Time: 0.44s\n","  ğŸ§ª Batch 4/55 â€” Loss: 0.1409 â€” Time: 0.43s\n","  ğŸ§ª Batch 5/55 â€” Loss: 0.1388 â€” Time: 0.44s\n","  ğŸ§ª Batch 6/55 â€” Loss: 0.1789 â€” Time: 0.44s\n","  ğŸ§ª Batch 7/55 â€” Loss: 0.2282 â€” Time: 0.43s\n","  ğŸ§ª Batch 8/55 â€” Loss: 0.1796 â€” Time: 0.44s\n","  ğŸ§ª Batch 9/55 â€” Loss: 0.1465 â€” Time: 0.44s\n","  ğŸ§ª Batch 10/55 â€” Loss: 0.1717 â€” Time: 0.44s\n","  ğŸ§ª Batch 11/55 â€” Loss: 0.1753 â€” Time: 0.44s\n","  ğŸ§ª Batch 12/55 â€” Loss: 0.1899 â€” Time: 0.44s\n","  ğŸ§ª Batch 13/55 â€” Loss: 0.1559 â€” Time: 0.44s\n","  ğŸ§ª Batch 14/55 â€” Loss: 0.1576 â€” Time: 0.44s\n","  ğŸ§ª Batch 15/55 â€” Loss: 0.1528 â€” Time: 0.44s\n","  ğŸ§ª Batch 16/55 â€” Loss: 0.1490 â€” Time: 0.44s\n","  ğŸ§ª Batch 17/55 â€” Loss: 0.1727 â€” Time: 0.44s\n","  ğŸ§ª Batch 18/55 â€” Loss: 0.1421 â€” Time: 0.44s\n","  ğŸ§ª Batch 19/55 â€” Loss: 0.1348 â€” Time: 0.44s\n","  ğŸ§ª Batch 20/55 â€” Loss: 0.1441 â€” Time: 0.44s\n","  ğŸ§ª Batch 21/55 â€” Loss: 0.1418 â€” Time: 0.46s\n","  ğŸ§ª Batch 22/55 â€” Loss: 0.1080 â€” Time: 0.45s\n","  ğŸ§ª Batch 23/55 â€” Loss: 0.1678 â€” Time: 0.47s\n","  ğŸ§ª Batch 24/55 â€” Loss: 0.1608 â€” Time: 0.49s\n","  ğŸ§ª Batch 25/55 â€” Loss: 0.1299 â€” Time: 0.47s\n","  ğŸ§ª Batch 26/55 â€” Loss: 0.1733 â€” Time: 0.44s\n","  ğŸ§ª Batch 27/55 â€” Loss: 0.1629 â€” Time: 0.44s\n","  ğŸ§ª Batch 28/55 â€” Loss: 0.1882 â€” Time: 0.44s\n","  ğŸ§ª Batch 29/55 â€” Loss: 0.2004 â€” Time: 0.44s\n","  ğŸ§ª Batch 30/55 â€” Loss: 0.1296 â€” Time: 0.44s\n","  ğŸ§ª Batch 31/55 â€” Loss: 0.1485 â€” Time: 0.44s\n","  ğŸ§ª Batch 32/55 â€” Loss: 0.1623 â€” Time: 0.44s\n","  ğŸ§ª Batch 33/55 â€” Loss: 0.1460 â€” Time: 0.44s\n","  ğŸ§ª Batch 34/55 â€” Loss: 0.1631 â€” Time: 0.44s\n","  ğŸ§ª Batch 35/55 â€” Loss: 0.1545 â€” Time: 0.44s\n","  ğŸ§ª Batch 36/55 â€” Loss: 0.1650 â€” Time: 0.44s\n","  ğŸ§ª Batch 37/55 â€” Loss: 0.1591 â€” Time: 0.44s\n","  ğŸ§ª Batch 38/55 â€” Loss: 0.1980 â€” Time: 0.44s\n","  ğŸ§ª Batch 39/55 â€” Loss: 0.1506 â€” Time: 0.44s\n","  ğŸ§ª Batch 40/55 â€” Loss: 0.1775 â€” Time: 0.44s\n","  ğŸ§ª Batch 41/55 â€” Loss: 0.1553 â€” Time: 0.44s\n","  ğŸ§ª Batch 42/55 â€” Loss: 0.1856 â€” Time: 0.44s\n","  ğŸ§ª Batch 43/55 â€” Loss: 0.1496 â€” Time: 0.44s\n","  ğŸ§ª Batch 44/55 â€” Loss: 0.1503 â€” Time: 0.44s\n","  ğŸ§ª Batch 45/55 â€” Loss: 0.1347 â€” Time: 0.44s\n","  ğŸ§ª Batch 46/55 â€” Loss: 0.1467 â€” Time: 0.44s\n","  ğŸ§ª Batch 47/55 â€” Loss: 0.1587 â€” Time: 0.44s\n","  ğŸ§ª Batch 48/55 â€” Loss: 0.1578 â€” Time: 0.44s\n","  ğŸ§ª Batch 49/55 â€” Loss: 0.1513 â€” Time: 0.44s\n","  ğŸ§ª Batch 50/55 â€” Loss: 0.1440 â€” Time: 0.44s\n","  ğŸ§ª Batch 51/55 â€” Loss: 0.1269 â€” Time: 0.46s\n","  ğŸ§ª Batch 52/55 â€” Loss: 0.1514 â€” Time: 0.44s\n","  ğŸ§ª Batch 53/55 â€” Loss: 0.1364 â€” Time: 0.44s\n","  ğŸ§ª Batch 54/55 â€” Loss: 0.2060 â€” Time: 0.44s\n","  ğŸ§ª Batch 55/55 â€” Loss: 0.1300 â€” Time: 0.29s\n","ğŸ•’ Epoch Time: 27.63s | Avg Batch Time: 0.44s\n","ğŸ“Š Train Loss: 0.1582 | Val Loss: 0.2757 | IoU: 0.8130 | Dice: 0.8951 | Jaccard: 0.8130 | Precision: 0.9067 | Recall: 0.8854 | Accuracy: 0.9746\n","\n","Epoch 17/100\n","  ğŸ§ª Batch 1/55 â€” Loss: 0.1291 â€” Time: 0.47s\n","  ğŸ§ª Batch 2/55 â€” Loss: 0.1237 â€” Time: 0.44s\n","  ğŸ§ª Batch 3/55 â€” Loss: 0.1227 â€” Time: 0.44s\n","  ğŸ§ª Batch 4/55 â€” Loss: 0.1526 â€” Time: 0.44s\n","  ğŸ§ª Batch 5/55 â€” Loss: 0.1471 â€” Time: 0.44s\n","  ğŸ§ª Batch 6/55 â€” Loss: 0.1318 â€” Time: 0.44s\n","  ğŸ§ª Batch 7/55 â€” Loss: 0.1183 â€” Time: 0.44s\n","  ğŸ§ª Batch 8/55 â€” Loss: 0.1234 â€” Time: 0.44s\n","  ğŸ§ª Batch 9/55 â€” Loss: 0.1172 â€” Time: 0.44s\n","  ğŸ§ª Batch 10/55 â€” Loss: 0.1323 â€” Time: 0.44s\n","  ğŸ§ª Batch 11/55 â€” Loss: 0.1208 â€” Time: 0.44s\n","  ğŸ§ª Batch 12/55 â€” Loss: 0.1334 â€” Time: 0.44s\n","  ğŸ§ª Batch 13/55 â€” Loss: 0.1295 â€” Time: 0.44s\n","  ğŸ§ª Batch 14/55 â€” Loss: 0.1425 â€” Time: 0.44s\n","  ğŸ§ª Batch 15/55 â€” Loss: 0.1570 â€” Time: 0.44s\n","  ğŸ§ª Batch 16/55 â€” Loss: 0.1414 â€” Time: 0.44s\n","  ğŸ§ª Batch 17/55 â€” Loss: 0.1828 â€” Time: 0.44s\n","  ğŸ§ª Batch 18/55 â€” Loss: 0.1207 â€” Time: 0.44s\n","  ğŸ§ª Batch 19/55 â€” Loss: 0.2179 â€” Time: 0.44s\n","  ğŸ§ª Batch 20/55 â€” Loss: 0.1292 â€” Time: 0.45s\n","  ğŸ§ª Batch 21/55 â€” Loss: 0.1526 â€” Time: 0.49s\n","  ğŸ§ª Batch 22/55 â€” Loss: 0.1684 â€” Time: 0.45s\n","  ğŸ§ª Batch 23/55 â€” Loss: 0.1432 â€” Time: 0.45s\n","  ğŸ§ª Batch 24/55 â€” Loss: 0.1924 â€” Time: 0.46s\n","  ğŸ§ª Batch 25/55 â€” Loss: 0.1474 â€” Time: 0.47s\n","  ğŸ§ª Batch 26/55 â€” Loss: 0.1316 â€” Time: 0.47s\n","  ğŸ§ª Batch 27/55 â€” Loss: 0.1840 â€” Time: 0.44s\n","  ğŸ§ª Batch 28/55 â€” Loss: 0.1414 â€” Time: 0.44s\n","  ğŸ§ª Batch 29/55 â€” Loss: 0.1249 â€” Time: 0.44s\n","  ğŸ§ª Batch 30/55 â€” Loss: 0.1313 â€” Time: 0.44s\n","  ğŸ§ª Batch 31/55 â€” Loss: 0.1333 â€” Time: 0.44s\n","  ğŸ§ª Batch 32/55 â€” Loss: 0.2105 â€” Time: 0.45s\n","  ğŸ§ª Batch 33/55 â€” Loss: 0.1629 â€” Time: 0.44s\n","  ğŸ§ª Batch 34/55 â€” Loss: 0.1492 â€” Time: 0.44s\n","  ğŸ§ª Batch 35/55 â€” Loss: 0.1335 â€” Time: 0.44s\n","  ğŸ§ª Batch 36/55 â€” Loss: 0.1425 â€” Time: 0.44s\n","  ğŸ§ª Batch 37/55 â€” Loss: 0.1355 â€” Time: 0.44s\n","  ğŸ§ª Batch 38/55 â€” Loss: 0.1358 â€” Time: 0.44s\n","  ğŸ§ª Batch 39/55 â€” Loss: 0.2036 â€” Time: 0.44s\n","  ğŸ§ª Batch 40/55 â€” Loss: 0.1255 â€” Time: 0.44s\n","  ğŸ§ª Batch 41/55 â€” Loss: 0.1172 â€” Time: 0.44s\n","  ğŸ§ª Batch 42/55 â€” Loss: 0.1748 â€” Time: 0.44s\n","  ğŸ§ª Batch 43/55 â€” Loss: 0.1557 â€” Time: 0.44s\n","  ğŸ§ª Batch 44/55 â€” Loss: 0.1274 â€” Time: 0.44s\n","  ğŸ§ª Batch 45/55 â€” Loss: 0.1533 â€” Time: 0.45s\n","  ğŸ§ª Batch 46/55 â€” Loss: 0.1231 â€” Time: 0.44s\n","  ğŸ§ª Batch 47/55 â€” Loss: 0.1157 â€” Time: 0.44s\n","  ğŸ§ª Batch 48/55 â€” Loss: 0.1457 â€” Time: 0.44s\n","  ğŸ§ª Batch 49/55 â€” Loss: 0.1339 â€” Time: 0.44s\n","  ğŸ§ª Batch 50/55 â€” Loss: 0.1197 â€” Time: 0.45s\n","  ğŸ§ª Batch 51/55 â€” Loss: 0.1258 â€” Time: 0.45s\n","  ğŸ§ª Batch 52/55 â€” Loss: 0.2235 â€” Time: 0.44s\n","  ğŸ§ª Batch 53/55 â€” Loss: 0.2311 â€” Time: 0.44s\n","  ğŸ§ª Batch 54/55 â€” Loss: 0.1447 â€” Time: 0.44s\n","  ğŸ§ª Batch 55/55 â€” Loss: 0.1652 â€” Time: 0.29s\n","ğŸ•’ Epoch Time: 27.62s | Avg Batch Time: 0.44s\n","ğŸ“Š Train Loss: 0.1469 | Val Loss: 0.2730 | IoU: 0.8102 | Dice: 0.8934 | Jaccard: 0.8102 | Precision: 0.9186 | Recall: 0.8704 | Accuracy: 0.9745\n","\n","Epoch 18/100\n","  ğŸ§ª Batch 1/55 â€” Loss: 0.1598 â€” Time: 0.44s\n","  ğŸ§ª Batch 2/55 â€” Loss: 0.1539 â€” Time: 0.44s\n","  ğŸ§ª Batch 3/55 â€” Loss: 0.1566 â€” Time: 0.44s\n","  ğŸ§ª Batch 4/55 â€” Loss: 0.1454 â€” Time: 0.44s\n","  ğŸ§ª Batch 5/55 â€” Loss: 0.1730 â€” Time: 0.44s\n","  ğŸ§ª Batch 6/55 â€” Loss: 0.1218 â€” Time: 0.44s\n","  ğŸ§ª Batch 7/55 â€” Loss: 0.1229 â€” Time: 0.44s\n","  ğŸ§ª Batch 8/55 â€” Loss: 0.1392 â€” Time: 0.44s\n","  ğŸ§ª Batch 9/55 â€” Loss: 0.2328 â€” Time: 0.44s\n","  ğŸ§ª Batch 10/55 â€” Loss: 0.1214 â€” Time: 0.44s\n","  ğŸ§ª Batch 11/55 â€” Loss: 0.1285 â€” Time: 0.44s\n","  ğŸ§ª Batch 12/55 â€” Loss: 0.1360 â€” Time: 0.44s\n","  ğŸ§ª Batch 13/55 â€” Loss: 0.1879 â€” Time: 0.44s\n","  ğŸ§ª Batch 14/55 â€” Loss: 0.1408 â€” Time: 0.44s\n","  ğŸ§ª Batch 15/55 â€” Loss: 0.1490 â€” Time: 0.44s\n","  ğŸ§ª Batch 16/55 â€” Loss: 0.1204 â€” Time: 0.44s\n","  ğŸ§ª Batch 17/55 â€” Loss: 0.1316 â€” Time: 0.44s\n","  ğŸ§ª Batch 18/55 â€” Loss: 0.1641 â€” Time: 0.45s\n","  ğŸ§ª Batch 19/55 â€” Loss: 0.1276 â€” Time: 0.44s\n","  ğŸ§ª Batch 20/55 â€” Loss: 0.1309 â€” Time: 0.48s\n","  ğŸ§ª Batch 21/55 â€” Loss: 0.1276 â€” Time: 0.48s\n","  ğŸ§ª Batch 22/55 â€” Loss: 0.1265 â€” Time: 0.46s\n","  ğŸ§ª Batch 23/55 â€” Loss: 0.1554 â€” Time: 0.44s\n","  ğŸ§ª Batch 24/55 â€” Loss: 0.1120 â€” Time: 0.47s\n","  ğŸ§ª Batch 25/55 â€” Loss: 0.1167 â€” Time: 0.49s\n","  ğŸ§ª Batch 26/55 â€” Loss: 0.1295 â€” Time: 0.44s\n","  ğŸ§ª Batch 27/55 â€” Loss: 0.1271 â€” Time: 0.44s\n","  ğŸ§ª Batch 28/55 â€” Loss: 0.1248 â€” Time: 0.45s\n","  ğŸ§ª Batch 29/55 â€” Loss: 0.1565 â€” Time: 0.46s\n","  ğŸ§ª Batch 30/55 â€” Loss: 0.1144 â€” Time: 0.44s\n","  ğŸ§ª Batch 31/55 â€” Loss: 0.1088 â€” Time: 0.44s\n","  ğŸ§ª Batch 32/55 â€” Loss: 0.1175 â€” Time: 0.44s\n","  ğŸ§ª Batch 33/55 â€” Loss: 0.1041 â€” Time: 0.44s\n","  ğŸ§ª Batch 34/55 â€” Loss: 0.1021 â€” Time: 0.44s\n","  ğŸ§ª Batch 35/55 â€” Loss: 0.1162 â€” Time: 0.44s\n","  ğŸ§ª Batch 36/55 â€” Loss: 0.1078 â€” Time: 0.44s\n","  ğŸ§ª Batch 37/55 â€” Loss: 0.1268 â€” Time: 0.44s\n","  ğŸ§ª Batch 38/55 â€” Loss: 0.1495 â€” Time: 0.44s\n","  ğŸ§ª Batch 39/55 â€” Loss: 0.1197 â€” Time: 0.44s\n","  ğŸ§ª Batch 40/55 â€” Loss: 0.1216 â€” Time: 0.44s\n","  ğŸ§ª Batch 41/55 â€” Loss: 0.1205 â€” Time: 0.44s\n","  ğŸ§ª Batch 42/55 â€” Loss: 0.1022 â€” Time: 0.44s\n","  ğŸ§ª Batch 43/55 â€” Loss: 0.1418 â€” Time: 0.44s\n","  ğŸ§ª Batch 44/55 â€” Loss: 0.1201 â€” Time: 0.44s\n","  ğŸ§ª Batch 45/55 â€” Loss: 0.1156 â€” Time: 0.44s\n","  ğŸ§ª Batch 46/55 â€” Loss: 0.1202 â€” Time: 0.44s\n","  ğŸ§ª Batch 47/55 â€” Loss: 0.1303 â€” Time: 0.44s\n","  ğŸ§ª Batch 48/55 â€” Loss: 0.1347 â€” Time: 0.44s\n","  ğŸ§ª Batch 49/55 â€” Loss: 0.1161 â€” Time: 0.44s\n","  ğŸ§ª Batch 50/55 â€” Loss: 0.1455 â€” Time: 0.44s\n","  ğŸ§ª Batch 51/55 â€” Loss: 0.1227 â€” Time: 0.47s\n","  ğŸ§ª Batch 52/55 â€” Loss: 0.1655 â€” Time: 0.47s\n","  ğŸ§ª Batch 53/55 â€” Loss: 0.1275 â€” Time: 0.44s\n","  ğŸ§ª Batch 54/55 â€” Loss: 0.1312 â€” Time: 0.44s\n","  ğŸ§ª Batch 55/55 â€” Loss: 0.1032 â€” Time: 0.28s\n","ğŸ•’ Epoch Time: 27.65s | Avg Batch Time: 0.44s\n","ğŸ“Š Train Loss: 0.1328 | Val Loss: 0.2640 | IoU: 0.8108 | Dice: 0.8939 | Jaccard: 0.8108 | Precision: 0.9119 | Recall: 0.8779 | Accuracy: 0.9748\n","\n","Epoch 19/100\n","  ğŸ§ª Batch 1/55 â€” Loss: 0.1178 â€” Time: 0.44s\n","  ğŸ§ª Batch 2/55 â€” Loss: 0.1136 â€” Time: 0.45s\n","  ğŸ§ª Batch 3/55 â€” Loss: 0.1070 â€” Time: 0.43s\n","  ğŸ§ª Batch 4/55 â€” Loss: 0.1447 â€” Time: 0.44s\n","  ğŸ§ª Batch 5/55 â€” Loss: 0.1007 â€” Time: 0.44s\n","  ğŸ§ª Batch 6/55 â€” Loss: 0.1991 â€” Time: 0.44s\n","  ğŸ§ª Batch 7/55 â€” Loss: 0.1067 â€” Time: 0.44s\n","  ğŸ§ª Batch 8/55 â€” Loss: 0.1036 â€” Time: 0.44s\n","  ğŸ§ª Batch 9/55 â€” Loss: 0.1371 â€” Time: 0.44s\n","  ğŸ§ª Batch 10/55 â€” Loss: 0.1164 â€” Time: 0.44s\n","  ğŸ§ª Batch 11/55 â€” Loss: 0.1024 â€” Time: 0.44s\n","  ğŸ§ª Batch 12/55 â€” Loss: 0.1301 â€” Time: 0.44s\n","  ğŸ§ª Batch 13/55 â€” Loss: 0.1151 â€” Time: 0.44s\n","  ğŸ§ª Batch 14/55 â€” Loss: 0.1121 â€” Time: 0.44s\n","  ğŸ§ª Batch 15/55 â€” Loss: 0.1212 â€” Time: 0.44s\n","  ğŸ§ª Batch 16/55 â€” Loss: 0.1154 â€” Time: 0.44s\n","  ğŸ§ª Batch 17/55 â€” Loss: 0.1341 â€” Time: 0.44s\n","  ğŸ§ª Batch 18/55 â€” Loss: 0.1128 â€” Time: 0.44s\n","  ğŸ§ª Batch 19/55 â€” Loss: 0.1129 â€” Time: 0.44s\n","  ğŸ§ª Batch 20/55 â€” Loss: 0.1187 â€” Time: 0.44s\n","  ğŸ§ª Batch 21/55 â€” Loss: 0.1130 â€” Time: 0.44s\n","  ğŸ§ª Batch 22/55 â€” Loss: 0.1182 â€” Time: 0.46s\n","  ğŸ§ª Batch 23/55 â€” Loss: 0.1017 â€” Time: 0.47s\n","  ğŸ§ª Batch 24/55 â€” Loss: 0.1382 â€” Time: 0.44s\n","  ğŸ§ª Batch 25/55 â€” Loss: 0.1197 â€” Time: 0.48s\n","  ğŸ§ª Batch 26/55 â€” Loss: 0.1233 â€” Time: 0.44s\n","  ğŸ§ª Batch 27/55 â€” Loss: 0.1177 â€” Time: 0.44s\n","  ğŸ§ª Batch 28/55 â€” Loss: 0.1432 â€” Time: 0.44s\n","  ğŸ§ª Batch 29/55 â€” Loss: 0.1329 â€” Time: 0.45s\n","  ğŸ§ª Batch 30/55 â€” Loss: 0.1335 â€” Time: 0.44s\n","  ğŸ§ª Batch 31/55 â€” Loss: 0.1098 â€” Time: 0.44s\n","  ğŸ§ª Batch 32/55 â€” Loss: 0.1080 â€” Time: 0.44s\n","  ğŸ§ª Batch 33/55 â€” Loss: 0.1640 â€” Time: 0.44s\n","  ğŸ§ª Batch 34/55 â€” Loss: 0.1103 â€” Time: 0.44s\n","  ğŸ§ª Batch 35/55 â€” Loss: 0.1144 â€” Time: 0.44s\n","  ğŸ§ª Batch 36/55 â€” Loss: 0.1079 â€” Time: 0.44s\n","  ğŸ§ª Batch 37/55 â€” Loss: 0.1075 â€” Time: 0.44s\n","  ğŸ§ª Batch 38/55 â€” Loss: 0.1049 â€” Time: 0.44s\n","  ğŸ§ª Batch 39/55 â€” Loss: 0.1078 â€” Time: 0.44s\n","  ğŸ§ª Batch 40/55 â€” Loss: 0.1185 â€” Time: 0.44s\n","  ğŸ§ª Batch 41/55 â€” Loss: 0.1735 â€” Time: 0.44s\n","  ğŸ§ª Batch 42/55 â€” Loss: 0.1122 â€” Time: 0.44s\n","  ğŸ§ª Batch 43/55 â€” Loss: 0.2317 â€” Time: 0.44s\n","  ğŸ§ª Batch 44/55 â€” Loss: 0.1148 â€” Time: 0.44s\n","  ğŸ§ª Batch 45/55 â€” Loss: 0.1169 â€” Time: 0.44s\n","  ğŸ§ª Batch 46/55 â€” Loss: 0.1708 â€” Time: 0.44s\n","  ğŸ§ª Batch 47/55 â€” Loss: 0.1243 â€” Time: 0.44s\n","  ğŸ§ª Batch 48/55 â€” Loss: 0.1157 â€” Time: 0.44s\n","  ğŸ§ª Batch 49/55 â€” Loss: 0.1251 â€” Time: 0.44s\n","  ğŸ§ª Batch 50/55 â€” Loss: 0.1080 â€” Time: 0.44s\n","  ğŸ§ª Batch 51/55 â€” Loss: 0.1137 â€” Time: 0.45s\n","  ğŸ§ª Batch 52/55 â€” Loss: 0.1201 â€” Time: 0.44s\n","  ğŸ§ª Batch 53/55 â€” Loss: 0.1435 â€” Time: 0.44s\n","  ğŸ§ª Batch 54/55 â€” Loss: 0.1405 â€” Time: 0.44s\n","  ğŸ§ª Batch 55/55 â€” Loss: 0.1218 â€” Time: 0.28s\n","ğŸ•’ Epoch Time: 27.47s | Avg Batch Time: 0.44s\n","ğŸ“Š Train Loss: 0.1245 | Val Loss: 0.2692 | IoU: 0.8041 | Dice: 0.8894 | Jaccard: 0.8041 | Precision: 0.9278 | Recall: 0.8562 | Accuracy: 0.9742\n","\n","Epoch 20/100\n","  ğŸ§ª Batch 1/55 â€” Loss: 0.1139 â€” Time: 0.44s\n","  ğŸ§ª Batch 2/55 â€” Loss: 0.1189 â€” Time: 0.44s\n","  ğŸ§ª Batch 3/55 â€” Loss: 0.1083 â€” Time: 0.44s\n","  ğŸ§ª Batch 4/55 â€” Loss: 0.1225 â€” Time: 0.44s\n","  ğŸ§ª Batch 5/55 â€” Loss: 0.1003 â€” Time: 0.44s\n","  ğŸ§ª Batch 6/55 â€” Loss: 0.1241 â€” Time: 0.44s\n","  ğŸ§ª Batch 7/55 â€” Loss: 0.1311 â€” Time: 0.45s\n","  ğŸ§ª Batch 8/55 â€” Loss: 0.1092 â€” Time: 0.44s\n","  ğŸ§ª Batch 9/55 â€” Loss: 0.1694 â€” Time: 0.44s\n","  ğŸ§ª Batch 10/55 â€” Loss: 0.0997 â€” Time: 0.44s\n","  ğŸ§ª Batch 11/55 â€” Loss: 0.1487 â€” Time: 0.44s\n","  ğŸ§ª Batch 12/55 â€” Loss: 0.1149 â€” Time: 0.44s\n","  ğŸ§ª Batch 13/55 â€” Loss: 0.1310 â€” Time: 0.44s\n","  ğŸ§ª Batch 14/55 â€” Loss: 0.1464 â€” Time: 0.44s\n","  ğŸ§ª Batch 15/55 â€” Loss: 0.1373 â€” Time: 0.44s\n","  ğŸ§ª Batch 16/55 â€” Loss: 0.1205 â€” Time: 0.44s\n","  ğŸ§ª Batch 17/55 â€” Loss: 0.1304 â€” Time: 0.44s\n","  ğŸ§ª Batch 18/55 â€” Loss: 0.1150 â€” Time: 0.44s\n","  ğŸ§ª Batch 19/55 â€” Loss: 0.1155 â€” Time: 0.44s\n","  ğŸ§ª Batch 20/55 â€” Loss: 0.1229 â€” Time: 0.46s\n","  ğŸ§ª Batch 21/55 â€” Loss: 0.1150 â€” Time: 0.49s\n","  ğŸ§ª Batch 22/55 â€” Loss: 0.1151 â€” Time: 0.46s\n","  ğŸ§ª Batch 23/55 â€” Loss: 0.1048 â€” Time: 0.44s\n","  ğŸ§ª Batch 24/55 â€” Loss: 0.1122 â€” Time: 0.48s\n","  ğŸ§ª Batch 25/55 â€” Loss: 0.1014 â€” Time: 0.49s\n","  ğŸ§ª Batch 26/55 â€” Loss: 0.1324 â€” Time: 0.46s\n","  ğŸ§ª Batch 27/55 â€” Loss: 0.1061 â€” Time: 0.51s\n","  ğŸ§ª Batch 28/55 â€” Loss: 0.0968 â€” Time: 0.44s\n","  ğŸ§ª Batch 29/55 â€” Loss: 0.1238 â€” Time: 0.44s\n","  ğŸ§ª Batch 30/55 â€” Loss: 0.1144 â€” Time: 0.44s\n","  ğŸ§ª Batch 31/55 â€” Loss: 0.1510 â€” Time: 0.44s\n","  ğŸ§ª Batch 32/55 â€” Loss: 0.1111 â€” Time: 0.44s\n","  ğŸ§ª Batch 33/55 â€” Loss: 0.1109 â€” Time: 0.44s\n","  ğŸ§ª Batch 34/55 â€” Loss: 0.1124 â€” Time: 0.44s\n","  ğŸ§ª Batch 35/55 â€” Loss: 0.1273 â€” Time: 0.44s\n","  ğŸ§ª Batch 36/55 â€” Loss: 0.1077 â€” Time: 0.44s\n","  ğŸ§ª Batch 37/55 â€” Loss: 0.1218 â€” Time: 0.44s\n","  ğŸ§ª Batch 38/55 â€” Loss: 0.0969 â€” Time: 0.44s\n","  ğŸ§ª Batch 39/55 â€” Loss: 0.1309 â€” Time: 0.44s\n","  ğŸ§ª Batch 40/55 â€” Loss: 0.1035 â€” Time: 0.44s\n","  ğŸ§ª Batch 41/55 â€” Loss: 0.1008 â€” Time: 0.44s\n","  ğŸ§ª Batch 42/55 â€” Loss: 0.1096 â€” Time: 0.45s\n","  ğŸ§ª Batch 43/55 â€” Loss: 0.1147 â€” Time: 0.44s\n","  ğŸ§ª Batch 44/55 â€” Loss: 0.1292 â€” Time: 0.44s\n","  ğŸ§ª Batch 45/55 â€” Loss: 0.1466 â€” Time: 0.44s\n","  ğŸ§ª Batch 46/55 â€” Loss: 0.0990 â€” Time: 0.44s\n","  ğŸ§ª Batch 47/55 â€” Loss: 0.1103 â€” Time: 0.44s\n","  ğŸ§ª Batch 48/55 â€” Loss: 0.1136 â€” Time: 0.44s\n","  ğŸ§ª Batch 49/55 â€” Loss: 0.1280 â€” Time: 0.44s\n","  ğŸ§ª Batch 50/55 â€” Loss: 0.1065 â€” Time: 0.44s\n","  ğŸ§ª Batch 51/55 â€” Loss: 0.0981 â€” Time: 0.46s\n","  ğŸ§ª Batch 52/55 â€” Loss: 0.1110 â€” Time: 0.45s\n","  ğŸ§ª Batch 53/55 â€” Loss: 0.1250 â€” Time: 0.44s\n","  ğŸ§ª Batch 54/55 â€” Loss: 0.1083 â€” Time: 0.44s\n","  ğŸ§ª Batch 55/55 â€” Loss: 0.1267 â€” Time: 0.29s\n","ğŸ•’ Epoch Time: 27.74s | Avg Batch Time: 0.44s\n","ğŸ“Š Train Loss: 0.1182 | Val Loss: 0.2606 | IoU: 0.8099 | Dice: 0.8928 | Jaccard: 0.8099 | Precision: 0.9185 | Recall: 0.8703 | Accuracy: 0.9744\n","\n","Epoch 21/100\n","  ğŸ§ª Batch 1/55 â€” Loss: 0.1275 â€” Time: 0.44s\n","  ğŸ§ª Batch 2/55 â€” Loss: 0.1051 â€” Time: 0.44s\n","  ğŸ§ª Batch 3/55 â€” Loss: 0.1095 â€” Time: 0.44s\n","  ğŸ§ª Batch 4/55 â€” Loss: 0.0996 â€” Time: 0.44s\n","  ğŸ§ª Batch 5/55 â€” Loss: 0.1146 â€” Time: 0.44s\n","  ğŸ§ª Batch 6/55 â€” Loss: 0.1062 â€” Time: 0.44s\n","  ğŸ§ª Batch 7/55 â€” Loss: 0.1017 â€” Time: 0.44s\n","  ğŸ§ª Batch 8/55 â€” Loss: 0.1124 â€” Time: 0.44s\n","  ğŸ§ª Batch 9/55 â€” Loss: 0.0977 â€” Time: 0.44s\n","  ğŸ§ª Batch 10/55 â€” Loss: 0.1215 â€” Time: 0.44s\n","  ğŸ§ª Batch 11/55 â€” Loss: 0.0884 â€” Time: 0.44s\n","  ğŸ§ª Batch 12/55 â€” Loss: 0.1215 â€” Time: 0.44s\n","  ğŸ§ª Batch 13/55 â€” Loss: 0.1048 â€” Time: 0.44s\n","  ğŸ§ª Batch 14/55 â€” Loss: 0.1037 â€” Time: 0.44s\n","  ğŸ§ª Batch 15/55 â€” Loss: 0.1022 â€” Time: 0.44s\n","  ğŸ§ª Batch 16/55 â€” Loss: 0.0984 â€” Time: 0.44s\n","  ğŸ§ª Batch 17/55 â€” Loss: 0.0940 â€” Time: 0.44s\n","  ğŸ§ª Batch 18/55 â€” Loss: 0.0994 â€” Time: 0.44s\n","  ğŸ§ª Batch 19/55 â€” Loss: 0.1009 â€” Time: 0.44s\n","  ğŸ§ª Batch 20/55 â€” Loss: 0.1043 â€” Time: 0.44s\n","  ğŸ§ª Batch 21/55 â€” Loss: 0.1049 â€” Time: 0.45s\n","  ğŸ§ª Batch 22/55 â€” Loss: 0.1039 â€” Time: 0.46s\n","  ğŸ§ª Batch 23/55 â€” Loss: 0.0985 â€” Time: 0.44s\n","  ğŸ§ª Batch 24/55 â€” Loss: 0.1056 â€” Time: 0.48s\n","  ğŸ§ª Batch 25/55 â€” Loss: 0.1044 â€” Time: 0.48s\n","  ğŸ§ª Batch 26/55 â€” Loss: 0.0958 â€” Time: 0.48s\n","  ğŸ§ª Batch 27/55 â€” Loss: 0.1027 â€” Time: 0.45s\n","  ğŸ§ª Batch 28/55 â€” Loss: 0.1416 â€” Time: 0.45s\n","  ğŸ§ª Batch 29/55 â€” Loss: 0.1039 â€” Time: 0.45s\n","  ğŸ§ª Batch 30/55 â€” Loss: 0.0882 â€” Time: 0.44s\n","  ğŸ§ª Batch 31/55 â€” Loss: 0.0931 â€” Time: 0.44s\n","  ğŸ§ª Batch 32/55 â€” Loss: 0.1048 â€” Time: 0.44s\n","  ğŸ§ª Batch 33/55 â€” Loss: 0.1363 â€” Time: 0.44s\n","  ğŸ§ª Batch 34/55 â€” Loss: 0.1031 â€” Time: 0.44s\n","  ğŸ§ª Batch 35/55 â€” Loss: 0.1140 â€” Time: 0.44s\n","  ğŸ§ª Batch 36/55 â€” Loss: 0.0989 â€” Time: 0.44s\n","  ğŸ§ª Batch 37/55 â€” Loss: 0.1178 â€” Time: 0.44s\n","  ğŸ§ª Batch 38/55 â€” Loss: 0.1056 â€” Time: 0.44s\n","  ğŸ§ª Batch 39/55 â€” Loss: 0.1495 â€” Time: 0.44s\n","  ğŸ§ª Batch 40/55 â€” Loss: 0.1059 â€” Time: 0.44s\n","  ğŸ§ª Batch 41/55 â€” Loss: 0.1202 â€” Time: 0.44s\n","  ğŸ§ª Batch 42/55 â€” Loss: 0.1126 â€” Time: 0.44s\n","  ğŸ§ª Batch 43/55 â€” Loss: 0.1462 â€” Time: 0.44s\n","  ğŸ§ª Batch 44/55 â€” Loss: 0.1038 â€” Time: 0.44s\n","  ğŸ§ª Batch 45/55 â€” Loss: 0.1167 â€” Time: 0.44s\n","  ğŸ§ª Batch 46/55 â€” Loss: 0.1413 â€” Time: 0.44s\n","  ğŸ§ª Batch 47/55 â€” Loss: 0.1224 â€” Time: 0.44s\n","  ğŸ§ª Batch 48/55 â€” Loss: 0.1167 â€” Time: 0.44s\n","  ğŸ§ª Batch 49/55 â€” Loss: 0.0968 â€” Time: 0.44s\n","  ğŸ§ª Batch 50/55 â€” Loss: 0.1029 â€” Time: 0.44s\n","  ğŸ§ª Batch 51/55 â€” Loss: 0.1048 â€” Time: 0.44s\n","  ğŸ§ª Batch 52/55 â€” Loss: 0.1127 â€” Time: 0.44s\n","  ğŸ§ª Batch 53/55 â€” Loss: 0.1046 â€” Time: 0.44s\n","  ğŸ§ª Batch 54/55 â€” Loss: 0.1008 â€” Time: 0.44s\n","  ğŸ§ª Batch 55/55 â€” Loss: 0.1105 â€” Time: 0.29s\n","ğŸ•’ Epoch Time: 27.54s | Avg Batch Time: 0.44s\n","ğŸ“Š Train Loss: 0.1092 | Val Loss: 0.2513 | IoU: 0.8121 | Dice: 0.8947 | Jaccard: 0.8121 | Precision: 0.9047 | Recall: 0.8866 | Accuracy: 0.9747\n","\n","Epoch 22/100\n","  ğŸ§ª Batch 1/55 â€” Loss: 0.0950 â€” Time: 0.46s\n","  ğŸ§ª Batch 2/55 â€” Loss: 0.0988 â€” Time: 0.44s\n","  ğŸ§ª Batch 3/55 â€” Loss: 0.1941 â€” Time: 0.44s\n","  ğŸ§ª Batch 4/55 â€” Loss: 0.0939 â€” Time: 0.44s\n","  ğŸ§ª Batch 5/55 â€” Loss: 0.0981 â€” Time: 0.44s\n","  ğŸ§ª Batch 6/55 â€” Loss: 0.0931 â€” Time: 0.44s\n","  ğŸ§ª Batch 7/55 â€” Loss: 0.1260 â€” Time: 0.44s\n","  ğŸ§ª Batch 8/55 â€” Loss: 0.1047 â€” Time: 0.44s\n","  ğŸ§ª Batch 9/55 â€” Loss: 0.0959 â€” Time: 0.44s\n","  ğŸ§ª Batch 10/55 â€” Loss: 0.0924 â€” Time: 0.44s\n","  ğŸ§ª Batch 11/55 â€” Loss: 0.1029 â€” Time: 0.44s\n","  ğŸ§ª Batch 12/55 â€” Loss: 0.1375 â€” Time: 0.44s\n","  ğŸ§ª Batch 13/55 â€” Loss: 0.1167 â€” Time: 0.44s\n","  ğŸ§ª Batch 14/55 â€” Loss: 0.1142 â€” Time: 0.44s\n","  ğŸ§ª Batch 15/55 â€” Loss: 0.1487 â€” Time: 0.44s\n","  ğŸ§ª Batch 16/55 â€” Loss: 0.0967 â€” Time: 0.44s\n","  ğŸ§ª Batch 17/55 â€” Loss: 0.1156 â€” Time: 0.44s\n","  ğŸ§ª Batch 18/55 â€” Loss: 0.0985 â€” Time: 0.44s\n","  ğŸ§ª Batch 19/55 â€” Loss: 0.1081 â€” Time: 0.44s\n","  ğŸ§ª Batch 20/55 â€” Loss: 0.1058 â€” Time: 0.44s\n","  ğŸ§ª Batch 21/55 â€” Loss: 0.1071 â€” Time: 0.52s\n","  ğŸ§ª Batch 22/55 â€” Loss: 0.1151 â€” Time: 0.47s\n","  ğŸ§ª Batch 23/55 â€” Loss: 0.1057 â€” Time: 0.44s\n","  ğŸ§ª Batch 24/55 â€” Loss: 0.0865 â€” Time: 0.47s\n","  ğŸ§ª Batch 25/55 â€” Loss: 0.1094 â€” Time: 0.49s\n","  ğŸ§ª Batch 26/55 â€” Loss: 0.0886 â€” Time: 0.46s\n","  ğŸ§ª Batch 27/55 â€” Loss: 0.1536 â€” Time: 0.47s\n","  ğŸ§ª Batch 28/55 â€” Loss: 0.0972 â€” Time: 0.45s\n","  ğŸ§ª Batch 29/55 â€” Loss: 0.1335 â€” Time: 0.44s\n","  ğŸ§ª Batch 30/55 â€” Loss: 0.1034 â€” Time: 0.46s\n","  ğŸ§ª Batch 31/55 â€” Loss: 0.0882 â€” Time: 0.45s\n","  ğŸ§ª Batch 32/55 â€” Loss: 0.1066 â€” Time: 0.44s\n","  ğŸ§ª Batch 33/55 â€” Loss: 0.0911 â€” Time: 0.44s\n","  ğŸ§ª Batch 34/55 â€” Loss: 0.0937 â€” Time: 0.44s\n","  ğŸ§ª Batch 35/55 â€” Loss: 0.0995 â€” Time: 0.44s\n","  ğŸ§ª Batch 36/55 â€” Loss: 0.1112 â€” Time: 0.44s\n","  ğŸ§ª Batch 37/55 â€” Loss: 0.0843 â€” Time: 0.44s\n","  ğŸ§ª Batch 38/55 â€” Loss: 0.1051 â€” Time: 0.44s\n","  ğŸ§ª Batch 39/55 â€” Loss: 0.1178 â€” Time: 0.44s\n","  ğŸ§ª Batch 40/55 â€” Loss: 0.1042 â€” Time: 0.44s\n","  ğŸ§ª Batch 41/55 â€” Loss: 0.0855 â€” Time: 0.44s\n","  ğŸ§ª Batch 42/55 â€” Loss: 0.0841 â€” Time: 0.44s\n","  ğŸ§ª Batch 43/55 â€” Loss: 0.0971 â€” Time: 0.44s\n","  ğŸ§ª Batch 44/55 â€” Loss: 0.0808 â€” Time: 0.44s\n","  ğŸ§ª Batch 45/55 â€” Loss: 0.0918 â€” Time: 0.44s\n","  ğŸ§ª Batch 46/55 â€” Loss: 0.1059 â€” Time: 0.44s\n","  ğŸ§ª Batch 47/55 â€” Loss: 0.1270 â€” Time: 0.44s\n","  ğŸ§ª Batch 48/55 â€” Loss: 0.0876 â€” Time: 0.44s\n","  ğŸ§ª Batch 49/55 â€” Loss: 0.0871 â€” Time: 0.44s\n","  ğŸ§ª Batch 50/55 â€” Loss: 0.1086 â€” Time: 0.44s\n","  ğŸ§ª Batch 51/55 â€” Loss: 0.1127 â€” Time: 0.45s\n","  ğŸ§ª Batch 52/55 â€” Loss: 0.0998 â€” Time: 0.44s\n","  ğŸ§ª Batch 53/55 â€” Loss: 0.0922 â€” Time: 0.44s\n","  ğŸ§ª Batch 54/55 â€” Loss: 0.0968 â€” Time: 0.44s\n","  ğŸ§ª Batch 55/55 â€” Loss: 0.0890 â€” Time: 0.28s\n","ğŸ•’ Epoch Time: 27.76s | Avg Batch Time: 0.44s\n","ğŸ“Š Train Loss: 0.1052 | Val Loss: 0.2487 | IoU: 0.8108 | Dice: 0.8939 | Jaccard: 0.8108 | Precision: 0.9273 | Recall: 0.8645 | Accuracy: 0.9750\n","\n","Epoch 23/100\n","  ğŸ§ª Batch 1/55 â€” Loss: 0.1082 â€” Time: 0.45s\n","  ğŸ§ª Batch 2/55 â€” Loss: 0.1186 â€” Time: 0.44s\n","  ğŸ§ª Batch 3/55 â€” Loss: 0.0934 â€” Time: 0.44s\n","  ğŸ§ª Batch 4/55 â€” Loss: 0.0955 â€” Time: 0.44s\n","  ğŸ§ª Batch 5/55 â€” Loss: 0.1116 â€” Time: 0.44s\n","  ğŸ§ª Batch 6/55 â€” Loss: 0.0952 â€” Time: 0.44s\n","  ğŸ§ª Batch 7/55 â€” Loss: 0.0848 â€” Time: 0.44s\n","  ğŸ§ª Batch 8/55 â€” Loss: 0.1013 â€” Time: 0.44s\n","  ğŸ§ª Batch 9/55 â€” Loss: 0.0971 â€” Time: 0.44s\n","  ğŸ§ª Batch 10/55 â€” Loss: 0.1134 â€” Time: 0.44s\n","  ğŸ§ª Batch 11/55 â€” Loss: 0.0835 â€” Time: 0.44s\n","  ğŸ§ª Batch 12/55 â€” Loss: 0.0803 â€” Time: 0.44s\n","  ğŸ§ª Batch 13/55 â€” Loss: 0.0869 â€” Time: 0.44s\n","  ğŸ§ª Batch 14/55 â€” Loss: 0.1121 â€” Time: 0.44s\n","  ğŸ§ª Batch 15/55 â€” Loss: 0.1171 â€” Time: 0.44s\n","  ğŸ§ª Batch 16/55 â€” Loss: 0.0950 â€” Time: 0.44s\n","  ğŸ§ª Batch 17/55 â€” Loss: 0.1410 â€” Time: 0.44s\n","  ğŸ§ª Batch 18/55 â€” Loss: 0.1113 â€” Time: 0.44s\n","  ğŸ§ª Batch 19/55 â€” Loss: 0.1016 â€” Time: 0.44s\n","  ğŸ§ª Batch 20/55 â€” Loss: 0.1010 â€” Time: 0.45s\n","  ğŸ§ª Batch 21/55 â€” Loss: 0.0903 â€” Time: 0.45s\n","  ğŸ§ª Batch 22/55 â€” Loss: 0.1020 â€” Time: 0.47s\n","  ğŸ§ª Batch 23/55 â€” Loss: 0.0878 â€” Time: 0.46s\n","  ğŸ§ª Batch 24/55 â€” Loss: 0.0915 â€” Time: 0.48s\n","  ğŸ§ª Batch 25/55 â€” Loss: 0.0938 â€” Time: 0.45s\n","  ğŸ§ª Batch 26/55 â€” Loss: 0.1001 â€” Time: 0.48s\n","  ğŸ§ª Batch 27/55 â€” Loss: 0.0834 â€” Time: 0.45s\n","  ğŸ§ª Batch 28/55 â€” Loss: 0.0835 â€” Time: 0.44s\n","  ğŸ§ª Batch 29/55 â€” Loss: 0.1082 â€” Time: 0.44s\n","  ğŸ§ª Batch 30/55 â€” Loss: 0.0872 â€” Time: 0.44s\n","  ğŸ§ª Batch 31/55 â€” Loss: 0.1178 â€” Time: 0.45s\n","  ğŸ§ª Batch 32/55 â€” Loss: 0.1177 â€” Time: 0.44s\n","  ğŸ§ª Batch 33/55 â€” Loss: 0.0950 â€” Time: 0.44s\n","  ğŸ§ª Batch 34/55 â€” Loss: 0.0884 â€” Time: 0.44s\n","  ğŸ§ª Batch 35/55 â€” Loss: 0.1133 â€” Time: 0.44s\n","  ğŸ§ª Batch 36/55 â€” Loss: 0.1146 â€” Time: 0.44s\n","  ğŸ§ª Batch 37/55 â€” Loss: 0.0808 â€” Time: 0.44s\n","  ğŸ§ª Batch 38/55 â€” Loss: 0.0860 â€” Time: 0.44s\n","  ğŸ§ª Batch 39/55 â€” Loss: 0.1139 â€” Time: 0.45s\n","  ğŸ§ª Batch 40/55 â€” Loss: 0.1085 â€” Time: 0.44s\n","  ğŸ§ª Batch 41/55 â€” Loss: 0.0937 â€” Time: 0.44s\n","  ğŸ§ª Batch 42/55 â€” Loss: 0.1106 â€” Time: 0.44s\n","  ğŸ§ª Batch 43/55 â€” Loss: 0.1042 â€” Time: 0.44s\n","  ğŸ§ª Batch 44/55 â€” Loss: 0.1066 â€” Time: 0.44s\n","  ğŸ§ª Batch 45/55 â€” Loss: 0.0980 â€” Time: 0.44s\n","  ğŸ§ª Batch 46/55 â€” Loss: 0.0941 â€” Time: 0.44s\n","  ğŸ§ª Batch 47/55 â€” Loss: 0.0832 â€” Time: 0.44s\n","  ğŸ§ª Batch 48/55 â€” Loss: 0.0957 â€” Time: 0.44s\n","  ğŸ§ª Batch 49/55 â€” Loss: 0.0892 â€” Time: 0.44s\n","  ğŸ§ª Batch 50/55 â€” Loss: 0.1149 â€” Time: 0.44s\n","  ğŸ§ª Batch 51/55 â€” Loss: 0.0996 â€” Time: 0.45s\n","  ğŸ§ª Batch 52/55 â€” Loss: 0.1114 â€” Time: 0.44s\n","  ğŸ§ª Batch 53/55 â€” Loss: 0.1136 â€” Time: 0.44s\n","  ğŸ§ª Batch 54/55 â€” Loss: 0.0943 â€” Time: 0.44s\n","  ğŸ§ª Batch 55/55 â€” Loss: 0.1015 â€” Time: 0.28s\n","ğŸ•’ Epoch Time: 27.66s | Avg Batch Time: 0.44s\n","ğŸ“Š Train Loss: 0.1004 | Val Loss: 0.2470 | IoU: 0.8113 | Dice: 0.8942 | Jaccard: 0.8113 | Precision: 0.9217 | Recall: 0.8697 | Accuracy: 0.9752\n","\n","Epoch 24/100\n","  ğŸ§ª Batch 1/55 â€” Loss: 0.0858 â€” Time: 0.44s\n","  ğŸ§ª Batch 2/55 â€” Loss: 0.0875 â€” Time: 0.44s\n","  ğŸ§ª Batch 3/55 â€” Loss: 0.0849 â€” Time: 0.44s\n","  ğŸ§ª Batch 4/55 â€” Loss: 0.0901 â€” Time: 0.44s\n","  ğŸ§ª Batch 5/55 â€” Loss: 0.0906 â€” Time: 0.44s\n","  ğŸ§ª Batch 6/55 â€” Loss: 0.2342 â€” Time: 0.44s\n","  ğŸ§ª Batch 7/55 â€” Loss: 0.0794 â€” Time: 0.45s\n","  ğŸ§ª Batch 8/55 â€” Loss: 0.1384 â€” Time: 0.44s\n","  ğŸ§ª Batch 9/55 â€” Loss: 0.0827 â€” Time: 0.44s\n","  ğŸ§ª Batch 10/55 â€” Loss: 0.1623 â€” Time: 0.44s\n","  ğŸ§ª Batch 11/55 â€” Loss: 0.0916 â€” Time: 0.44s\n","  ğŸ§ª Batch 12/55 â€” Loss: 0.1683 â€” Time: 0.44s\n","  ğŸ§ª Batch 13/55 â€” Loss: 0.1015 â€” Time: 0.44s\n","  ğŸ§ª Batch 14/55 â€” Loss: 0.1090 â€” Time: 0.44s\n","  ğŸ§ª Batch 15/55 â€” Loss: 0.0910 â€” Time: 0.44s\n","  ğŸ§ª Batch 16/55 â€” Loss: 0.0990 â€” Time: 0.44s\n","  ğŸ§ª Batch 17/55 â€” Loss: 0.0987 â€” Time: 0.44s\n","  ğŸ§ª Batch 18/55 â€” Loss: 0.0965 â€” Time: 0.44s\n","  ğŸ§ª Batch 19/55 â€” Loss: 0.1243 â€” Time: 0.44s\n","  ğŸ§ª Batch 20/55 â€” Loss: 0.1066 â€” Time: 0.46s\n","  ğŸ§ª Batch 21/55 â€” Loss: 0.0932 â€” Time: 0.44s\n","  ğŸ§ª Batch 22/55 â€” Loss: 0.0958 â€” Time: 0.47s\n","  ğŸ§ª Batch 23/55 â€” Loss: 0.0963 â€” Time: 0.49s\n","  ğŸ§ª Batch 24/55 â€” Loss: 0.0960 â€” Time: 0.45s\n","  ğŸ§ª Batch 25/55 â€” Loss: 0.0958 â€” Time: 0.46s\n","  ğŸ§ª Batch 26/55 â€” Loss: 0.0879 â€” Time: 0.44s\n","  ğŸ§ª Batch 27/55 â€” Loss: 0.1018 â€” Time: 0.45s\n","  ğŸ§ª Batch 28/55 â€” Loss: 0.1241 â€” Time: 0.44s\n","  ğŸ§ª Batch 29/55 â€” Loss: 0.1045 â€” Time: 0.44s\n","  ğŸ§ª Batch 30/55 â€” Loss: 0.0945 â€” Time: 0.44s\n","  ğŸ§ª Batch 31/55 â€” Loss: 0.1089 â€” Time: 0.44s\n","  ğŸ§ª Batch 32/55 â€” Loss: 0.0974 â€” Time: 0.44s\n","  ğŸ§ª Batch 33/55 â€” Loss: 0.0812 â€” Time: 0.44s\n","  ğŸ§ª Batch 34/55 â€” Loss: 0.0845 â€” Time: 0.44s\n","  ğŸ§ª Batch 35/55 â€” Loss: 0.0846 â€” Time: 0.44s\n","  ğŸ§ª Batch 36/55 â€” Loss: 0.1082 â€” Time: 0.44s\n","  ğŸ§ª Batch 37/55 â€” Loss: 0.1247 â€” Time: 0.44s\n","  ğŸ§ª Batch 38/55 â€” Loss: 0.0904 â€” Time: 0.44s\n","  ğŸ§ª Batch 39/55 â€” Loss: 0.0880 â€” Time: 0.44s\n","  ğŸ§ª Batch 40/55 â€” Loss: 0.1169 â€” Time: 0.44s\n","  ğŸ§ª Batch 41/55 â€” Loss: 0.0970 â€” Time: 0.44s\n","  ğŸ§ª Batch 42/55 â€” Loss: 0.1148 â€” Time: 0.44s\n","  ğŸ§ª Batch 43/55 â€” Loss: 0.0991 â€” Time: 0.44s\n","  ğŸ§ª Batch 44/55 â€” Loss: 0.1091 â€” Time: 0.44s\n","  ğŸ§ª Batch 45/55 â€” Loss: 0.0904 â€” Time: 0.44s\n","  ğŸ§ª Batch 46/55 â€” Loss: 0.0846 â€” Time: 0.44s\n","  ğŸ§ª Batch 47/55 â€” Loss: 0.1074 â€” Time: 0.44s\n","  ğŸ§ª Batch 48/55 â€” Loss: 0.0827 â€” Time: 0.44s\n","  ğŸ§ª Batch 49/55 â€” Loss: 0.0900 â€” Time: 0.44s\n","  ğŸ§ª Batch 50/55 â€” Loss: 0.1103 â€” Time: 0.44s\n","  ğŸ§ª Batch 51/55 â€” Loss: 0.0902 â€” Time: 0.44s\n","  ğŸ§ª Batch 52/55 â€” Loss: 0.0822 â€” Time: 0.44s\n","  ğŸ§ª Batch 53/55 â€” Loss: 0.0880 â€” Time: 0.44s\n","  ğŸ§ª Batch 54/55 â€” Loss: 0.0833 â€” Time: 0.44s\n","  ğŸ§ª Batch 55/55 â€” Loss: 0.0827 â€” Time: 0.29s\n","ğŸ•’ Epoch Time: 27.72s | Avg Batch Time: 0.44s\n","ğŸ“Š Train Loss: 0.1020 | Val Loss: 0.2373 | IoU: 0.8168 | Dice: 0.8979 | Jaccard: 0.8168 | Precision: 0.9127 | Recall: 0.8851 | Accuracy: 0.9756\n","\n","Epoch 25/100\n","  ğŸ§ª Batch 1/55 â€” Loss: 0.0838 â€” Time: 0.44s\n","  ğŸ§ª Batch 2/55 â€” Loss: 0.0834 â€” Time: 0.44s\n","  ğŸ§ª Batch 3/55 â€” Loss: 0.1046 â€” Time: 0.44s\n","  ğŸ§ª Batch 4/55 â€” Loss: 0.0963 â€” Time: 0.44s\n","  ğŸ§ª Batch 5/55 â€” Loss: 0.1190 â€” Time: 0.44s\n","  ğŸ§ª Batch 6/55 â€” Loss: 0.0909 â€” Time: 0.44s\n","  ğŸ§ª Batch 7/55 â€” Loss: 0.0918 â€” Time: 0.44s\n","  ğŸ§ª Batch 8/55 â€” Loss: 0.0784 â€” Time: 0.44s\n","  ğŸ§ª Batch 9/55 â€” Loss: 0.0894 â€” Time: 0.44s\n","  ğŸ§ª Batch 10/55 â€” Loss: 0.0815 â€” Time: 0.44s\n","  ğŸ§ª Batch 11/55 â€” Loss: 0.0761 â€” Time: 0.44s\n","  ğŸ§ª Batch 12/55 â€” Loss: 0.0918 â€” Time: 0.44s\n","  ğŸ§ª Batch 13/55 â€” Loss: 0.0809 â€” Time: 0.44s\n","  ğŸ§ª Batch 14/55 â€” Loss: 0.0796 â€” Time: 0.44s\n","  ğŸ§ª Batch 15/55 â€” Loss: 0.0835 â€” Time: 0.44s\n","  ğŸ§ª Batch 16/55 â€” Loss: 0.0817 â€” Time: 0.44s\n","  ğŸ§ª Batch 17/55 â€” Loss: 0.0929 â€” Time: 0.44s\n","  ğŸ§ª Batch 18/55 â€” Loss: 0.1659 â€” Time: 0.44s\n","  ğŸ§ª Batch 19/55 â€” Loss: 0.0886 â€” Time: 0.44s\n","  ğŸ§ª Batch 20/55 â€” Loss: 0.1052 â€” Time: 0.44s\n","  ğŸ§ª Batch 21/55 â€” Loss: 0.0804 â€” Time: 0.44s\n","  ğŸ§ª Batch 22/55 â€” Loss: 0.0901 â€” Time: 0.47s\n","  ğŸ§ª Batch 23/55 â€” Loss: 0.0908 â€” Time: 0.45s\n","  ğŸ§ª Batch 24/55 â€” Loss: 0.0804 â€” Time: 0.49s\n","  ğŸ§ª Batch 25/55 â€” Loss: 0.1045 â€” Time: 0.47s\n","  ğŸ§ª Batch 26/55 â€” Loss: 0.0974 â€” Time: 0.44s\n","  ğŸ§ª Batch 27/55 â€” Loss: 0.0900 â€” Time: 0.49s\n","  ğŸ§ª Batch 28/55 â€” Loss: 0.0771 â€” Time: 0.44s\n","  ğŸ§ª Batch 29/55 â€” Loss: 0.1004 â€” Time: 0.44s\n","  ğŸ§ª Batch 30/55 â€” Loss: 0.0956 â€” Time: 0.44s\n","  ğŸ§ª Batch 31/55 â€” Loss: 0.0808 â€” Time: 0.44s\n","  ğŸ§ª Batch 32/55 â€” Loss: 0.0803 â€” Time: 0.44s\n","  ğŸ§ª Batch 33/55 â€” Loss: 0.0894 â€” Time: 0.44s\n","  ğŸ§ª Batch 34/55 â€” Loss: 0.0979 â€” Time: 0.44s\n","  ğŸ§ª Batch 35/55 â€” Loss: 0.0757 â€” Time: 0.44s\n","  ğŸ§ª Batch 36/55 â€” Loss: 0.0788 â€” Time: 0.44s\n","  ğŸ§ª Batch 37/55 â€” Loss: 0.1127 â€” Time: 0.44s\n","  ğŸ§ª Batch 38/55 â€” Loss: 0.0970 â€” Time: 0.44s\n","  ğŸ§ª Batch 39/55 â€” Loss: 0.0865 â€” Time: 0.44s\n","  ğŸ§ª Batch 40/55 â€” Loss: 0.0910 â€” Time: 0.44s\n","  ğŸ§ª Batch 41/55 â€” Loss: 0.1091 â€” Time: 0.44s\n","  ğŸ§ª Batch 42/55 â€” Loss: 0.1531 â€” Time: 0.44s\n","  ğŸ§ª Batch 43/55 â€” Loss: 0.0824 â€” Time: 0.44s\n","  ğŸ§ª Batch 44/55 â€” Loss: 0.0767 â€” Time: 0.44s\n","  ğŸ§ª Batch 45/55 â€” Loss: 0.0807 â€” Time: 0.44s\n","  ğŸ§ª Batch 46/55 â€” Loss: 0.0922 â€” Time: 0.44s\n","  ğŸ§ª Batch 47/55 â€” Loss: 0.1119 â€” Time: 0.45s\n","  ğŸ§ª Batch 48/55 â€” Loss: 0.0923 â€” Time: 0.44s\n","  ğŸ§ª Batch 49/55 â€” Loss: 0.0829 â€” Time: 0.44s\n","  ğŸ§ª Batch 50/55 â€” Loss: 0.0932 â€” Time: 0.44s\n","  ğŸ§ª Batch 51/55 â€” Loss: 0.0792 â€” Time: 0.44s\n","  ğŸ§ª Batch 52/55 â€” Loss: 0.0906 â€” Time: 0.44s\n","  ğŸ§ª Batch 53/55 â€” Loss: 0.0815 â€” Time: 0.44s\n","  ğŸ§ª Batch 54/55 â€” Loss: 0.1071 â€” Time: 0.44s\n","  ğŸ§ª Batch 55/55 â€” Loss: 0.0744 â€” Time: 0.28s\n","ğŸ•’ Epoch Time: 27.53s | Avg Batch Time: 0.44s\n","ğŸ“Š Train Loss: 0.0922 | Val Loss: 0.2372 | IoU: 0.8151 | Dice: 0.8966 | Jaccard: 0.8151 | Precision: 0.9027 | Recall: 0.8923 | Accuracy: 0.9751\n","\n","Epoch 26/100\n","  ğŸ§ª Batch 1/55 â€” Loss: 0.0957 â€” Time: 0.45s\n","  ğŸ§ª Batch 2/55 â€” Loss: 0.0969 â€” Time: 0.44s\n","  ğŸ§ª Batch 3/55 â€” Loss: 0.0945 â€” Time: 0.44s\n","  ğŸ§ª Batch 4/55 â€” Loss: 0.0938 â€” Time: 0.44s\n","  ğŸ§ª Batch 5/55 â€” Loss: 0.0801 â€” Time: 0.44s\n","  ğŸ§ª Batch 6/55 â€” Loss: 0.0781 â€” Time: 0.44s\n","  ğŸ§ª Batch 7/55 â€” Loss: 0.0897 â€” Time: 0.44s\n","  ğŸ§ª Batch 8/55 â€” Loss: 0.0851 â€” Time: 0.44s\n","  ğŸ§ª Batch 9/55 â€” Loss: 0.0852 â€” Time: 0.44s\n","  ğŸ§ª Batch 10/55 â€” Loss: 0.0904 â€” Time: 0.44s\n","  ğŸ§ª Batch 11/55 â€” Loss: 0.0857 â€” Time: 0.44s\n","  ğŸ§ª Batch 12/55 â€” Loss: 0.0945 â€” Time: 0.44s\n","  ğŸ§ª Batch 13/55 â€” Loss: 0.0724 â€” Time: 0.45s\n","  ğŸ§ª Batch 14/55 â€” Loss: 0.0815 â€” Time: 0.44s\n","  ğŸ§ª Batch 15/55 â€” Loss: 0.0828 â€” Time: 0.44s\n","  ğŸ§ª Batch 16/55 â€” Loss: 0.0764 â€” Time: 0.44s\n","  ğŸ§ª Batch 17/55 â€” Loss: 0.0846 â€” Time: 0.44s\n","  ğŸ§ª Batch 18/55 â€” Loss: 0.0871 â€” Time: 0.44s\n","  ğŸ§ª Batch 19/55 â€” Loss: 0.1432 â€” Time: 0.45s\n","  ğŸ§ª Batch 20/55 â€” Loss: 0.0759 â€” Time: 0.44s\n","  ğŸ§ª Batch 21/55 â€” Loss: 0.0927 â€” Time: 0.47s\n","  ğŸ§ª Batch 22/55 â€” Loss: 0.0953 â€” Time: 0.44s\n","  ğŸ§ª Batch 23/55 â€” Loss: 0.0777 â€” Time: 0.45s\n","  ğŸ§ª Batch 24/55 â€” Loss: 0.0775 â€” Time: 0.46s\n","  ğŸ§ª Batch 25/55 â€” Loss: 0.0838 â€” Time: 0.49s\n","  ğŸ§ª Batch 26/55 â€” Loss: 0.0791 â€” Time: 0.44s\n","  ğŸ§ª Batch 27/55 â€” Loss: 0.0855 â€” Time: 0.44s\n","  ğŸ§ª Batch 28/55 â€” Loss: 0.0737 â€” Time: 0.44s\n","  ğŸ§ª Batch 29/55 â€” Loss: 0.0933 â€” Time: 0.44s\n","  ğŸ§ª Batch 30/55 â€” Loss: 0.1123 â€” Time: 0.44s\n","  ğŸ§ª Batch 31/55 â€” Loss: 0.0851 â€” Time: 0.44s\n","  ğŸ§ª Batch 32/55 â€” Loss: 0.0856 â€” Time: 0.45s\n","  ğŸ§ª Batch 33/55 â€” Loss: 0.0770 â€” Time: 0.44s\n","  ğŸ§ª Batch 34/55 â€” Loss: 0.0729 â€” Time: 0.44s\n","  ğŸ§ª Batch 35/55 â€” Loss: 0.0763 â€” Time: 0.44s\n","  ğŸ§ª Batch 36/55 â€” Loss: 0.0823 â€” Time: 0.44s\n","  ğŸ§ª Batch 37/55 â€” Loss: 0.0765 â€” Time: 0.44s\n","  ğŸ§ª Batch 38/55 â€” Loss: 0.0770 â€” Time: 0.44s\n","  ğŸ§ª Batch 39/55 â€” Loss: 0.0979 â€” Time: 0.44s\n","  ğŸ§ª Batch 40/55 â€” Loss: 0.0816 â€” Time: 0.44s\n","  ğŸ§ª Batch 41/55 â€” Loss: 0.0954 â€” Time: 0.44s\n","  ğŸ§ª Batch 42/55 â€” Loss: 0.0684 â€” Time: 0.44s\n","  ğŸ§ª Batch 43/55 â€” Loss: 0.0961 â€” Time: 0.44s\n","  ğŸ§ª Batch 44/55 â€” Loss: 0.0866 â€” Time: 0.44s\n","  ğŸ§ª Batch 45/55 â€” Loss: 0.0765 â€” Time: 0.44s\n","  ğŸ§ª Batch 46/55 â€” Loss: 0.1113 â€” Time: 0.44s\n","  ğŸ§ª Batch 47/55 â€” Loss: 0.0989 â€” Time: 0.44s\n","  ğŸ§ª Batch 48/55 â€” Loss: 0.0744 â€” Time: 0.45s\n","  ğŸ§ª Batch 49/55 â€” Loss: 0.1081 â€” Time: 0.44s\n","  ğŸ§ª Batch 50/55 â€” Loss: 0.0817 â€” Time: 0.44s\n","  ğŸ§ª Batch 51/55 â€” Loss: 0.0944 â€” Time: 0.44s\n","  ğŸ§ª Batch 52/55 â€” Loss: 0.0858 â€” Time: 0.44s\n","  ğŸ§ª Batch 53/55 â€” Loss: 0.0907 â€” Time: 0.44s\n","  ğŸ§ª Batch 54/55 â€” Loss: 0.0921 â€” Time: 0.44s\n","  ğŸ§ª Batch 55/55 â€” Loss: 0.0946 â€” Time: 0.29s\n","ğŸ•’ Epoch Time: 27.61s | Avg Batch Time: 0.44s\n","ğŸ“Š Train Loss: 0.0875 | Val Loss: 0.2373 | IoU: 0.8156 | Dice: 0.8968 | Jaccard: 0.8156 | Precision: 0.9279 | Recall: 0.8697 | Accuracy: 0.9758\n","\n","Epoch 27/100\n","  ğŸ§ª Batch 1/55 â€” Loss: 0.0868 â€” Time: 0.45s\n","  ğŸ§ª Batch 2/55 â€” Loss: 0.0706 â€” Time: 0.44s\n","  ğŸ§ª Batch 3/55 â€” Loss: 0.0928 â€” Time: 0.44s\n","  ğŸ§ª Batch 4/55 â€” Loss: 0.0852 â€” Time: 0.44s\n","  ğŸ§ª Batch 5/55 â€” Loss: 0.0702 â€” Time: 0.45s\n","  ğŸ§ª Batch 6/55 â€” Loss: 0.0671 â€” Time: 0.44s\n","  ğŸ§ª Batch 7/55 â€” Loss: 0.0917 â€” Time: 0.44s\n","  ğŸ§ª Batch 8/55 â€” Loss: 0.0785 â€” Time: 0.45s\n","  ğŸ§ª Batch 9/55 â€” Loss: 0.0708 â€” Time: 0.44s\n","  ğŸ§ª Batch 10/55 â€” Loss: 0.0873 â€” Time: 0.44s\n","  ğŸ§ª Batch 11/55 â€” Loss: 0.0752 â€” Time: 0.44s\n","  ğŸ§ª Batch 12/55 â€” Loss: 0.0734 â€” Time: 0.44s\n","  ğŸ§ª Batch 13/55 â€” Loss: 0.1106 â€” Time: 0.44s\n","  ğŸ§ª Batch 14/55 â€” Loss: 0.0746 â€” Time: 0.44s\n","  ğŸ§ª Batch 15/55 â€” Loss: 0.0843 â€” Time: 0.44s\n","  ğŸ§ª Batch 16/55 â€” Loss: 0.0745 â€” Time: 0.44s\n","  ğŸ§ª Batch 17/55 â€” Loss: 0.0749 â€” Time: 0.44s\n","  ğŸ§ª Batch 18/55 â€” Loss: 0.0734 â€” Time: 0.44s\n","  ğŸ§ª Batch 19/55 â€” Loss: 0.0793 â€” Time: 0.44s\n","  ğŸ§ª Batch 20/55 â€” Loss: 0.0800 â€” Time: 0.45s\n","  ğŸ§ª Batch 21/55 â€” Loss: 0.0674 â€” Time: 0.46s\n","  ğŸ§ª Batch 22/55 â€” Loss: 0.0982 â€” Time: 0.47s\n","  ğŸ§ª Batch 23/55 â€” Loss: 0.1021 â€” Time: 0.44s\n","  ğŸ§ª Batch 24/55 â€” Loss: 0.0777 â€” Time: 0.47s\n","  ğŸ§ª Batch 25/55 â€” Loss: 0.0813 â€” Time: 0.43s\n","  ğŸ§ª Batch 26/55 â€” Loss: 0.0801 â€” Time: 0.45s\n","  ğŸ§ª Batch 27/55 â€” Loss: 0.0745 â€” Time: 0.44s\n","  ğŸ§ª Batch 28/55 â€” Loss: 0.0717 â€” Time: 0.44s\n","  ğŸ§ª Batch 29/55 â€” Loss: 0.0655 â€” Time: 0.44s\n","  ğŸ§ª Batch 30/55 â€” Loss: 0.1022 â€” Time: 0.44s\n","  ğŸ§ª Batch 31/55 â€” Loss: 0.1027 â€” Time: 0.45s\n","  ğŸ§ª Batch 32/55 â€” Loss: 0.0819 â€” Time: 0.44s\n","  ğŸ§ª Batch 33/55 â€” Loss: 0.0785 â€” Time: 0.44s\n","  ğŸ§ª Batch 34/55 â€” Loss: 0.1061 â€” Time: 0.44s\n","  ğŸ§ª Batch 35/55 â€” Loss: 0.0677 â€” Time: 0.44s\n","  ğŸ§ª Batch 36/55 â€” Loss: 0.0646 â€” Time: 0.44s\n","  ğŸ§ª Batch 37/55 â€” Loss: 0.0870 â€” Time: 0.44s\n","  ğŸ§ª Batch 38/55 â€” Loss: 0.0795 â€” Time: 0.44s\n","  ğŸ§ª Batch 39/55 â€” Loss: 0.0760 â€” Time: 0.44s\n","  ğŸ§ª Batch 40/55 â€” Loss: 0.0972 â€” Time: 0.44s\n","  ğŸ§ª Batch 41/55 â€” Loss: 0.0923 â€” Time: 0.44s\n","  ğŸ§ª Batch 42/55 â€” Loss: 0.0812 â€” Time: 0.44s\n","  ğŸ§ª Batch 43/55 â€” Loss: 0.0926 â€” Time: 0.44s\n","  ğŸ§ª Batch 44/55 â€” Loss: 0.0970 â€” Time: 0.44s\n","  ğŸ§ª Batch 45/55 â€” Loss: 0.0874 â€” Time: 0.44s\n","  ğŸ§ª Batch 46/55 â€” Loss: 0.0863 â€” Time: 0.44s\n","  ğŸ§ª Batch 47/55 â€” Loss: 0.0840 â€” Time: 0.44s\n","  ğŸ§ª Batch 48/55 â€” Loss: 0.0683 â€” Time: 0.44s\n","  ğŸ§ª Batch 49/55 â€” Loss: 0.0891 â€” Time: 0.44s\n","  ğŸ§ª Batch 50/55 â€” Loss: 0.0906 â€” Time: 0.44s\n","  ğŸ§ª Batch 51/55 â€” Loss: 0.0717 â€” Time: 0.45s\n","  ğŸ§ª Batch 52/55 â€” Loss: 0.0926 â€” Time: 0.44s\n","  ğŸ§ª Batch 53/55 â€” Loss: 0.0879 â€” Time: 0.44s\n","  ğŸ§ª Batch 54/55 â€” Loss: 0.0681 â€” Time: 0.44s\n","  ğŸ§ª Batch 55/55 â€” Loss: 0.0743 â€” Time: 0.29s\n","ğŸ•’ Epoch Time: 27.64s | Avg Batch Time: 0.44s\n","ğŸ“Š Train Loss: 0.0823 | Val Loss: 0.2366 | IoU: 0.8129 | Dice: 0.8955 | Jaccard: 0.8129 | Precision: 0.9363 | Recall: 0.8599 | Accuracy: 0.9758\n","\n","Epoch 28/100\n","  ğŸ§ª Batch 1/55 â€” Loss: 0.0920 â€” Time: 0.46s\n","  ğŸ§ª Batch 2/55 â€” Loss: 0.0751 â€” Time: 0.44s\n","  ğŸ§ª Batch 3/55 â€” Loss: 0.0759 â€” Time: 0.44s\n","  ğŸ§ª Batch 4/55 â€” Loss: 0.0735 â€” Time: 0.44s\n","  ğŸ§ª Batch 5/55 â€” Loss: 0.0748 â€” Time: 0.44s\n","  ğŸ§ª Batch 6/55 â€” Loss: 0.0811 â€” Time: 0.44s\n","  ğŸ§ª Batch 7/55 â€” Loss: 0.0770 â€” Time: 0.44s\n","  ğŸ§ª Batch 8/55 â€” Loss: 0.0860 â€” Time: 0.44s\n","  ğŸ§ª Batch 9/55 â€” Loss: 0.0773 â€” Time: 0.44s\n","  ğŸ§ª Batch 10/55 â€” Loss: 0.1014 â€” Time: 0.44s\n","  ğŸ§ª Batch 11/55 â€” Loss: 0.0687 â€” Time: 0.44s\n","  ğŸ§ª Batch 12/55 â€” Loss: 0.0806 â€” Time: 0.44s\n","  ğŸ§ª Batch 13/55 â€” Loss: 0.0775 â€” Time: 0.44s\n","  ğŸ§ª Batch 14/55 â€” Loss: 0.0752 â€” Time: 0.44s\n","  ğŸ§ª Batch 15/55 â€” Loss: 0.0840 â€” Time: 0.44s\n","  ğŸ§ª Batch 16/55 â€” Loss: 0.0741 â€” Time: 0.44s\n","  ğŸ§ª Batch 17/55 â€” Loss: 0.0764 â€” Time: 0.44s\n","  ğŸ§ª Batch 18/55 â€” Loss: 0.0736 â€” Time: 0.44s\n","  ğŸ§ª Batch 19/55 â€” Loss: 0.0659 â€” Time: 0.44s\n","  ğŸ§ª Batch 20/55 â€” Loss: 0.0598 â€” Time: 0.44s\n","  ğŸ§ª Batch 21/55 â€” Loss: 0.0687 â€” Time: 0.49s\n","  ğŸ§ª Batch 22/55 â€” Loss: 0.0682 â€” Time: 0.44s\n","  ğŸ§ª Batch 23/55 â€” Loss: 0.0782 â€” Time: 0.44s\n","  ğŸ§ª Batch 24/55 â€” Loss: 0.0628 â€” Time: 0.44s\n","  ğŸ§ª Batch 25/55 â€” Loss: 0.1040 â€” Time: 0.47s\n","  ğŸ§ª Batch 26/55 â€” Loss: 0.1064 â€” Time: 0.44s\n","  ğŸ§ª Batch 27/55 â€” Loss: 0.0691 â€” Time: 0.46s\n","  ğŸ§ª Batch 28/55 â€” Loss: 0.0779 â€” Time: 0.44s\n","  ğŸ§ª Batch 29/55 â€” Loss: 0.0844 â€” Time: 0.44s\n","  ğŸ§ª Batch 30/55 â€” Loss: 0.0676 â€” Time: 0.44s\n","  ğŸ§ª Batch 31/55 â€” Loss: 0.0835 â€” Time: 0.44s\n","  ğŸ§ª Batch 32/55 â€” Loss: 0.0763 â€” Time: 0.44s\n","  ğŸ§ª Batch 33/55 â€” Loss: 0.0706 â€” Time: 0.45s\n","  ğŸ§ª Batch 34/55 â€” Loss: 0.0675 â€” Time: 0.44s\n","  ğŸ§ª Batch 35/55 â€” Loss: 0.0803 â€” Time: 0.44s\n","  ğŸ§ª Batch 36/55 â€” Loss: 0.0661 â€” Time: 0.44s\n","  ğŸ§ª Batch 37/55 â€” Loss: 0.0721 â€” Time: 0.44s\n","  ğŸ§ª Batch 38/55 â€” Loss: 0.0732 â€” Time: 0.44s\n","  ğŸ§ª Batch 39/55 â€” Loss: 0.0790 â€” Time: 0.44s\n","  ğŸ§ª Batch 40/55 â€” Loss: 0.0684 â€” Time: 0.44s\n","  ğŸ§ª Batch 41/55 â€” Loss: 0.0739 â€” Time: 0.44s\n","  ğŸ§ª Batch 42/55 â€” Loss: 0.0875 â€” Time: 0.44s\n","  ğŸ§ª Batch 43/55 â€” Loss: 0.0828 â€” Time: 0.44s\n","  ğŸ§ª Batch 44/55 â€” Loss: 0.0943 â€” Time: 0.44s\n","  ğŸ§ª Batch 45/55 â€” Loss: 0.0712 â€” Time: 0.44s\n","  ğŸ§ª Batch 46/55 â€” Loss: 0.0857 â€” Time: 0.44s\n","  ğŸ§ª Batch 47/55 â€” Loss: 0.0817 â€” Time: 0.44s\n","  ğŸ§ª Batch 48/55 â€” Loss: 0.0832 â€” Time: 0.44s\n","  ğŸ§ª Batch 49/55 â€” Loss: 0.0698 â€” Time: 0.44s\n","  ğŸ§ª Batch 50/55 â€” Loss: 0.0681 â€” Time: 0.44s\n","  ğŸ§ª Batch 51/55 â€” Loss: 0.0770 â€” Time: 0.44s\n","  ğŸ§ª Batch 52/55 â€” Loss: 0.0928 â€” Time: 0.44s\n","  ğŸ§ª Batch 53/55 â€” Loss: 0.0770 â€” Time: 0.44s\n","  ğŸ§ª Batch 54/55 â€” Loss: 0.0794 â€” Time: 0.44s\n","  ğŸ§ª Batch 55/55 â€” Loss: 0.1439 â€” Time: 0.29s\n","ğŸ•’ Epoch Time: 27.51s | Avg Batch Time: 0.44s\n","ğŸ“Š Train Loss: 0.0790 | Val Loss: 0.2260 | IoU: 0.8234 | Dice: 0.9018 | Jaccard: 0.8234 | Precision: 0.9249 | Recall: 0.8812 | Accuracy: 0.9767\n","\n","Epoch 29/100\n","  ğŸ§ª Batch 1/55 â€” Loss: 0.0658 â€” Time: 0.46s\n","  ğŸ§ª Batch 2/55 â€” Loss: 0.0705 â€” Time: 0.44s\n","  ğŸ§ª Batch 3/55 â€” Loss: 0.0688 â€” Time: 0.44s\n","  ğŸ§ª Batch 4/55 â€” Loss: 0.0827 â€” Time: 0.44s\n","  ğŸ§ª Batch 5/55 â€” Loss: 0.0657 â€” Time: 0.44s\n","  ğŸ§ª Batch 6/55 â€” Loss: 0.0962 â€” Time: 0.44s\n","  ğŸ§ª Batch 7/55 â€” Loss: 0.0692 â€” Time: 0.44s\n","  ğŸ§ª Batch 8/55 â€” Loss: 0.0644 â€” Time: 0.44s\n","  ğŸ§ª Batch 9/55 â€” Loss: 0.0755 â€” Time: 0.44s\n","  ğŸ§ª Batch 10/55 â€” Loss: 0.0837 â€” Time: 0.44s\n","  ğŸ§ª Batch 11/55 â€” Loss: 0.0875 â€” Time: 0.44s\n","  ğŸ§ª Batch 12/55 â€” Loss: 0.0833 â€” Time: 0.44s\n","  ğŸ§ª Batch 13/55 â€” Loss: 0.0764 â€” Time: 0.44s\n","  ğŸ§ª Batch 14/55 â€” Loss: 0.0707 â€” Time: 0.44s\n","  ğŸ§ª Batch 15/55 â€” Loss: 0.0874 â€” Time: 0.44s\n","  ğŸ§ª Batch 16/55 â€” Loss: 0.0831 â€” Time: 0.44s\n","  ğŸ§ª Batch 17/55 â€” Loss: 0.0948 â€” Time: 0.44s\n","  ğŸ§ª Batch 18/55 â€” Loss: 0.0855 â€” Time: 0.44s\n","  ğŸ§ª Batch 19/55 â€” Loss: 0.0713 â€” Time: 0.44s\n","  ğŸ§ª Batch 20/55 â€” Loss: 0.0711 â€” Time: 0.45s\n","  ğŸ§ª Batch 21/55 â€” Loss: 0.0727 â€” Time: 0.45s\n","  ğŸ§ª Batch 22/55 â€” Loss: 0.0765 â€” Time: 0.44s\n","  ğŸ§ª Batch 23/55 â€” Loss: 0.0835 â€” Time: 0.44s\n","  ğŸ§ª Batch 24/55 â€” Loss: 0.0749 â€” Time: 0.45s\n","  ğŸ§ª Batch 25/55 â€” Loss: 0.0810 â€” Time: 0.47s\n","  ğŸ§ª Batch 26/55 â€” Loss: 0.0764 â€” Time: 0.44s\n","  ğŸ§ª Batch 27/55 â€” Loss: 0.0710 â€” Time: 0.44s\n","  ğŸ§ª Batch 28/55 â€” Loss: 0.0785 â€” Time: 0.44s\n","  ğŸ§ª Batch 29/55 â€” Loss: 0.0647 â€” Time: 0.44s\n","  ğŸ§ª Batch 30/55 â€” Loss: 0.0887 â€” Time: 0.44s\n","  ğŸ§ª Batch 31/55 â€” Loss: 0.0664 â€” Time: 0.44s\n","  ğŸ§ª Batch 32/55 â€” Loss: 0.0792 â€” Time: 0.44s\n","  ğŸ§ª Batch 33/55 â€” Loss: 0.0833 â€” Time: 0.44s\n","  ğŸ§ª Batch 34/55 â€” Loss: 0.0903 â€” Time: 0.44s\n","  ğŸ§ª Batch 35/55 â€” Loss: 0.0705 â€” Time: 0.44s\n","  ğŸ§ª Batch 36/55 â€” Loss: 0.0768 â€” Time: 0.44s\n","  ğŸ§ª Batch 37/55 â€” Loss: 0.0791 â€” Time: 0.44s\n","  ğŸ§ª Batch 38/55 â€” Loss: 0.0713 â€” Time: 0.44s\n","  ğŸ§ª Batch 39/55 â€” Loss: 0.0818 â€” Time: 0.44s\n","  ğŸ§ª Batch 40/55 â€” Loss: 0.0801 â€” Time: 0.44s\n","  ğŸ§ª Batch 41/55 â€” Loss: 0.0709 â€” Time: 0.44s\n","  ğŸ§ª Batch 42/55 â€” Loss: 0.0751 â€” Time: 0.44s\n","  ğŸ§ª Batch 43/55 â€” Loss: 0.0873 â€” Time: 0.44s\n","  ğŸ§ª Batch 44/55 â€” Loss: 0.0813 â€” Time: 0.44s\n","  ğŸ§ª Batch 45/55 â€” Loss: 0.0808 â€” Time: 0.44s\n","  ğŸ§ª Batch 46/55 â€” Loss: 0.0736 â€” Time: 0.44s\n","  ğŸ§ª Batch 47/55 â€” Loss: 0.0793 â€” Time: 0.44s\n","  ğŸ§ª Batch 48/55 â€” Loss: 0.0691 â€” Time: 0.44s\n","  ğŸ§ª Batch 49/55 â€” Loss: 0.0633 â€” Time: 0.44s\n","  ğŸ§ª Batch 50/55 â€” Loss: 0.0709 â€” Time: 0.44s\n","  ğŸ§ª Batch 51/55 â€” Loss: 0.0645 â€” Time: 0.45s\n","  ğŸ§ª Batch 52/55 â€” Loss: 0.0741 â€” Time: 0.44s\n","  ğŸ§ª Batch 53/55 â€” Loss: 0.0721 â€” Time: 0.44s\n","  ğŸ§ª Batch 54/55 â€” Loss: 0.0612 â€” Time: 0.44s\n","  ğŸ§ª Batch 55/55 â€” Loss: 0.0834 â€” Time: 0.28s\n","ğŸ•’ Epoch Time: 27.44s | Avg Batch Time: 0.44s\n","ğŸ“Š Train Loss: 0.0765 | Val Loss: 0.2289 | IoU: 0.8197 | Dice: 0.8994 | Jaccard: 0.8197 | Precision: 0.9166 | Recall: 0.8846 | Accuracy: 0.9760\n","\n","Epoch 30/100\n","  ğŸ§ª Batch 1/55 â€” Loss: 0.0694 â€” Time: 0.44s\n","  ğŸ§ª Batch 2/55 â€” Loss: 0.0914 â€” Time: 0.44s\n","  ğŸ§ª Batch 3/55 â€” Loss: 0.0739 â€” Time: 0.44s\n","  ğŸ§ª Batch 4/55 â€” Loss: 0.0606 â€” Time: 0.44s\n","  ğŸ§ª Batch 5/55 â€” Loss: 0.0663 â€” Time: 0.45s\n","  ğŸ§ª Batch 6/55 â€” Loss: 0.0937 â€” Time: 0.44s\n","  ğŸ§ª Batch 7/55 â€” Loss: 0.0846 â€” Time: 0.44s\n","  ğŸ§ª Batch 8/55 â€” Loss: 0.0804 â€” Time: 0.44s\n","  ğŸ§ª Batch 9/55 â€” Loss: 0.0728 â€” Time: 0.44s\n","  ğŸ§ª Batch 10/55 â€” Loss: 0.0858 â€” Time: 0.45s\n","  ğŸ§ª Batch 11/55 â€” Loss: 0.0844 â€” Time: 0.44s\n","  ğŸ§ª Batch 12/55 â€” Loss: 0.0665 â€” Time: 0.44s\n","  ğŸ§ª Batch 13/55 â€” Loss: 0.0924 â€” Time: 0.44s\n","  ğŸ§ª Batch 14/55 â€” Loss: 0.0717 â€” Time: 0.44s\n","  ğŸ§ª Batch 15/55 â€” Loss: 0.0726 â€” Time: 0.44s\n","  ğŸ§ª Batch 16/55 â€” Loss: 0.0894 â€” Time: 0.44s\n","  ğŸ§ª Batch 17/55 â€” Loss: 0.0685 â€” Time: 0.44s\n","  ğŸ§ª Batch 18/55 â€” Loss: 0.0714 â€” Time: 0.47s\n","  ğŸ§ª Batch 19/55 â€” Loss: 0.0718 â€” Time: 0.44s\n","  ğŸ§ª Batch 20/55 â€” Loss: 0.0879 â€” Time: 0.44s\n","  ğŸ§ª Batch 21/55 â€” Loss: 0.0601 â€” Time: 0.49s\n","  ğŸ§ª Batch 22/55 â€” Loss: 0.0908 â€” Time: 0.44s\n","  ğŸ§ª Batch 23/55 â€” Loss: 0.0782 â€” Time: 0.44s\n","  ğŸ§ª Batch 24/55 â€” Loss: 0.0642 â€” Time: 0.47s\n","  ğŸ§ª Batch 25/55 â€” Loss: 0.0749 â€” Time: 0.50s\n","  ğŸ§ª Batch 26/55 â€” Loss: 0.0705 â€” Time: 0.44s\n","  ğŸ§ª Batch 27/55 â€” Loss: 0.0750 â€” Time: 0.44s\n","  ğŸ§ª Batch 28/55 â€” Loss: 0.0715 â€” Time: 0.44s\n","  ğŸ§ª Batch 29/55 â€” Loss: 0.0628 â€” Time: 0.45s\n","  ğŸ§ª Batch 30/55 â€” Loss: 0.0677 â€” Time: 0.44s\n","  ğŸ§ª Batch 31/55 â€” Loss: 0.0879 â€” Time: 0.44s\n","  ğŸ§ª Batch 32/55 â€” Loss: 0.0870 â€” Time: 0.44s\n","  ğŸ§ª Batch 33/55 â€” Loss: 0.0702 â€” Time: 0.44s\n","  ğŸ§ª Batch 34/55 â€” Loss: 0.0601 â€” Time: 0.44s\n","  ğŸ§ª Batch 35/55 â€” Loss: 0.0687 â€” Time: 0.44s\n","  ğŸ§ª Batch 36/55 â€” Loss: 0.0760 â€” Time: 0.44s\n","  ğŸ§ª Batch 37/55 â€” Loss: 0.0703 â€” Time: 0.44s\n","  ğŸ§ª Batch 38/55 â€” Loss: 0.0783 â€” Time: 0.44s\n","  ğŸ§ª Batch 39/55 â€” Loss: 0.0747 â€” Time: 0.44s\n","  ğŸ§ª Batch 40/55 â€” Loss: 0.0760 â€” Time: 0.44s\n","  ğŸ§ª Batch 41/55 â€” Loss: 0.0728 â€” Time: 0.44s\n","  ğŸ§ª Batch 42/55 â€” Loss: 0.0687 â€” Time: 0.44s\n","  ğŸ§ª Batch 43/55 â€” Loss: 0.0879 â€” Time: 0.44s\n","  ğŸ§ª Batch 44/55 â€” Loss: 0.0682 â€” Time: 0.44s\n","  ğŸ§ª Batch 45/55 â€” Loss: 0.0704 â€” Time: 0.44s\n","  ğŸ§ª Batch 46/55 â€” Loss: 0.0677 â€” Time: 0.44s\n","  ğŸ§ª Batch 47/55 â€” Loss: 0.0769 â€” Time: 0.44s\n","  ğŸ§ª Batch 48/55 â€” Loss: 0.0677 â€” Time: 0.44s\n","  ğŸ§ª Batch 49/55 â€” Loss: 0.0800 â€” Time: 0.44s\n","  ğŸ§ª Batch 50/55 â€” Loss: 0.0645 â€” Time: 0.44s\n","  ğŸ§ª Batch 51/55 â€” Loss: 0.0653 â€” Time: 0.44s\n","  ğŸ§ª Batch 52/55 â€” Loss: 0.0658 â€” Time: 0.44s\n","  ğŸ§ª Batch 53/55 â€” Loss: 0.0649 â€” Time: 0.44s\n","  ğŸ§ª Batch 54/55 â€” Loss: 0.0636 â€” Time: 0.44s\n","  ğŸ§ª Batch 55/55 â€” Loss: 0.0736 â€” Time: 0.29s\n","ğŸ•’ Epoch Time: 27.54s | Avg Batch Time: 0.44s\n","ğŸ“Š Train Loss: 0.0742 | Val Loss: 0.2255 | IoU: 0.8228 | Dice: 0.9011 | Jaccard: 0.8228 | Precision: 0.9224 | Recall: 0.8829 | Accuracy: 0.9764\n","\n","Epoch 31/100\n","  ğŸ§ª Batch 1/55 â€” Loss: 0.0689 â€” Time: 0.44s\n","  ğŸ§ª Batch 2/55 â€” Loss: 0.0606 â€” Time: 0.44s\n","  ğŸ§ª Batch 3/55 â€” Loss: 0.0742 â€” Time: 0.44s\n","  ğŸ§ª Batch 4/55 â€” Loss: 0.1167 â€” Time: 0.44s\n","  ğŸ§ª Batch 5/55 â€” Loss: 0.0613 â€” Time: 0.44s\n","  ğŸ§ª Batch 6/55 â€” Loss: 0.0801 â€” Time: 0.44s\n","  ğŸ§ª Batch 7/55 â€” Loss: 0.0599 â€” Time: 0.44s\n","  ğŸ§ª Batch 8/55 â€” Loss: 0.0725 â€” Time: 0.44s\n","  ğŸ§ª Batch 9/55 â€” Loss: 0.0649 â€” Time: 0.44s\n","  ğŸ§ª Batch 10/55 â€” Loss: 0.0694 â€” Time: 0.44s\n","  ğŸ§ª Batch 11/55 â€” Loss: 0.0711 â€” Time: 0.44s\n","  ğŸ§ª Batch 12/55 â€” Loss: 0.0704 â€” Time: 0.44s\n","  ğŸ§ª Batch 13/55 â€” Loss: 0.0649 â€” Time: 0.44s\n","  ğŸ§ª Batch 14/55 â€” Loss: 0.0592 â€” Time: 0.44s\n","  ğŸ§ª Batch 15/55 â€” Loss: 0.0699 â€” Time: 0.44s\n","  ğŸ§ª Batch 16/55 â€” Loss: 0.0656 â€” Time: 0.44s\n","  ğŸ§ª Batch 17/55 â€” Loss: 0.0615 â€” Time: 0.44s\n","  ğŸ§ª Batch 18/55 â€” Loss: 0.0744 â€” Time: 0.44s\n","  ğŸ§ª Batch 19/55 â€” Loss: 0.0687 â€” Time: 0.50s\n","  ğŸ§ª Batch 20/55 â€” Loss: 0.0728 â€” Time: 0.55s\n","  ğŸ§ª Batch 21/55 â€” Loss: 0.0690 â€” Time: 0.53s\n","  ğŸ§ª Batch 22/55 â€” Loss: 0.0726 â€” Time: 0.59s\n","  ğŸ§ª Batch 23/55 â€” Loss: 0.0633 â€” Time: 0.56s\n","  ğŸ§ª Batch 24/55 â€” Loss: 0.0664 â€” Time: 0.51s\n","  ğŸ§ª Batch 25/55 â€” Loss: 0.0696 â€” Time: 0.60s\n","  ğŸ§ª Batch 26/55 â€” Loss: 0.0823 â€” Time: 0.65s\n","  ğŸ§ª Batch 27/55 â€” Loss: 0.0651 â€” Time: 0.51s\n","  ğŸ§ª Batch 28/55 â€” Loss: 0.0783 â€” Time: 0.50s\n","  ğŸ§ª Batch 29/55 â€” Loss: 0.0822 â€” Time: 0.44s\n","  ğŸ§ª Batch 30/55 â€” Loss: 0.0627 â€” Time: 0.45s\n","  ğŸ§ª Batch 31/55 â€” Loss: 0.0745 â€” Time: 0.44s\n","  ğŸ§ª Batch 32/55 â€” Loss: 0.0675 â€” Time: 0.44s\n","  ğŸ§ª Batch 33/55 â€” Loss: 0.0726 â€” Time: 0.44s\n","  ğŸ§ª Batch 34/55 â€” Loss: 0.0785 â€” Time: 0.44s\n","  ğŸ§ª Batch 35/55 â€” Loss: 0.0571 â€” Time: 0.44s\n","  ğŸ§ª Batch 36/55 â€” Loss: 0.0637 â€” Time: 0.44s\n","  ğŸ§ª Batch 37/55 â€” Loss: 0.0948 â€” Time: 0.44s\n","  ğŸ§ª Batch 38/55 â€” Loss: 0.0623 â€” Time: 0.44s\n","  ğŸ§ª Batch 39/55 â€” Loss: 0.0665 â€” Time: 0.44s\n","  ğŸ§ª Batch 40/55 â€” Loss: 0.0709 â€” Time: 0.44s\n","  ğŸ§ª Batch 41/55 â€” Loss: 0.0582 â€” Time: 0.44s\n","  ğŸ§ª Batch 42/55 â€” Loss: 0.0796 â€” Time: 0.44s\n","  ğŸ§ª Batch 43/55 â€” Loss: 0.0848 â€” Time: 0.44s\n","  ğŸ§ª Batch 44/55 â€” Loss: 0.0730 â€” Time: 0.44s\n","  ğŸ§ª Batch 45/55 â€” Loss: 0.0710 â€” Time: 0.44s\n","  ğŸ§ª Batch 46/55 â€” Loss: 0.0929 â€” Time: 0.44s\n","  ğŸ§ª Batch 47/55 â€” Loss: 0.0715 â€” Time: 0.44s\n","  ğŸ§ª Batch 48/55 â€” Loss: 0.0739 â€” Time: 0.44s\n","  ğŸ§ª Batch 49/55 â€” Loss: 0.0735 â€” Time: 0.44s\n","  ğŸ§ª Batch 50/55 â€” Loss: 0.0766 â€” Time: 0.49s\n","  ğŸ§ª Batch 51/55 â€” Loss: 0.0736 â€” Time: 0.44s\n","  ğŸ§ª Batch 52/55 â€” Loss: 0.0720 â€” Time: 0.45s\n","  ğŸ§ª Batch 53/55 â€” Loss: 0.0741 â€” Time: 0.44s\n","  ğŸ§ª Batch 54/55 â€” Loss: 0.0715 â€” Time: 0.44s\n","  ğŸ§ª Batch 55/55 â€” Loss: 0.0686 â€” Time: 0.29s\n","ğŸ•’ Epoch Time: 28.33s | Avg Batch Time: 0.46s\n","ğŸ“Š Train Loss: 0.0717 | Val Loss: 0.2299 | IoU: 0.8185 | Dice: 0.8987 | Jaccard: 0.8185 | Precision: 0.9112 | Recall: 0.8884 | Accuracy: 0.9757\n","\n","Epoch 32/100\n","  ğŸ§ª Batch 1/55 â€” Loss: 0.0682 â€” Time: 0.44s\n","  ğŸ§ª Batch 2/55 â€” Loss: 0.0881 â€” Time: 0.44s\n","  ğŸ§ª Batch 3/55 â€” Loss: 0.0689 â€” Time: 0.44s\n","  ğŸ§ª Batch 4/55 â€” Loss: 0.0670 â€” Time: 0.44s\n","  ğŸ§ª Batch 5/55 â€” Loss: 0.0682 â€” Time: 0.44s\n","  ğŸ§ª Batch 6/55 â€” Loss: 0.0614 â€” Time: 0.44s\n","  ğŸ§ª Batch 7/55 â€” Loss: 0.0665 â€” Time: 0.44s\n","  ğŸ§ª Batch 8/55 â€” Loss: 0.0656 â€” Time: 0.44s\n","  ğŸ§ª Batch 9/55 â€” Loss: 0.0702 â€” Time: 0.44s\n","  ğŸ§ª Batch 10/55 â€” Loss: 0.0619 â€” Time: 0.44s\n","  ğŸ§ª Batch 11/55 â€” Loss: 0.0588 â€” Time: 0.44s\n","  ğŸ§ª Batch 12/55 â€” Loss: 0.0687 â€” Time: 0.44s\n","  ğŸ§ª Batch 13/55 â€” Loss: 0.0755 â€” Time: 0.44s\n","  ğŸ§ª Batch 14/55 â€” Loss: 0.0584 â€” Time: 0.44s\n","  ğŸ§ª Batch 15/55 â€” Loss: 0.0791 â€” Time: 0.44s\n","  ğŸ§ª Batch 16/55 â€” Loss: 0.0650 â€” Time: 0.44s\n","  ğŸ§ª Batch 17/55 â€” Loss: 0.0643 â€” Time: 0.44s\n","  ğŸ§ª Batch 18/55 â€” Loss: 0.0624 â€” Time: 0.44s\n","  ğŸ§ª Batch 19/55 â€” Loss: 0.0652 â€” Time: 0.44s\n","  ğŸ§ª Batch 20/55 â€” Loss: 0.0724 â€” Time: 0.44s\n","  ğŸ§ª Batch 21/55 â€” Loss: 0.0710 â€” Time: 0.44s\n","  ğŸ§ª Batch 22/55 â€” Loss: 0.0657 â€” Time: 0.44s\n","  ğŸ§ª Batch 23/55 â€” Loss: 0.0622 â€” Time: 0.49s\n","  ğŸ§ª Batch 24/55 â€” Loss: 0.0606 â€” Time: 0.49s\n","  ğŸ§ª Batch 25/55 â€” Loss: 0.0725 â€” Time: 0.44s\n","  ğŸ§ª Batch 26/55 â€” Loss: 0.0739 â€” Time: 0.44s\n","  ğŸ§ª Batch 27/55 â€” Loss: 0.0679 â€” Time: 0.44s\n","  ğŸ§ª Batch 28/55 â€” Loss: 0.0613 â€” Time: 0.44s\n","  ğŸ§ª Batch 29/55 â€” Loss: 0.0684 â€” Time: 0.44s\n","  ğŸ§ª Batch 30/55 â€” Loss: 0.0673 â€” Time: 0.44s\n","  ğŸ§ª Batch 31/55 â€” Loss: 0.0631 â€” Time: 0.45s\n","  ğŸ§ª Batch 32/55 â€” Loss: 0.0615 â€” Time: 0.44s\n","  ğŸ§ª Batch 33/55 â€” Loss: 0.0718 â€” Time: 0.44s\n","  ğŸ§ª Batch 34/55 â€” Loss: 0.0813 â€” Time: 0.44s\n","  ğŸ§ª Batch 35/55 â€” Loss: 0.0612 â€” Time: 0.44s\n","  ğŸ§ª Batch 36/55 â€” Loss: 0.0589 â€” Time: 0.44s\n","  ğŸ§ª Batch 37/55 â€” Loss: 0.0677 â€” Time: 0.44s\n","  ğŸ§ª Batch 38/55 â€” Loss: 0.0732 â€” Time: 0.44s\n","  ğŸ§ª Batch 39/55 â€” Loss: 0.0590 â€” Time: 0.44s\n","  ğŸ§ª Batch 40/55 â€” Loss: 0.0634 â€” Time: 0.44s\n","  ğŸ§ª Batch 41/55 â€” Loss: 0.0647 â€” Time: 0.44s\n","  ğŸ§ª Batch 42/55 â€” Loss: 0.0683 â€” Time: 0.44s\n","  ğŸ§ª Batch 43/55 â€” Loss: 0.0725 â€” Time: 0.44s\n","  ğŸ§ª Batch 44/55 â€” Loss: 0.0674 â€” Time: 0.44s\n","  ğŸ§ª Batch 45/55 â€” Loss: 0.0691 â€” Time: 0.44s\n","  ğŸ§ª Batch 46/55 â€” Loss: 0.0656 â€” Time: 0.44s\n","  ğŸ§ª Batch 47/55 â€” Loss: 0.0698 â€” Time: 0.44s\n","  ğŸ§ª Batch 48/55 â€” Loss: 0.0755 â€” Time: 0.44s\n","  ğŸ§ª Batch 49/55 â€” Loss: 0.0675 â€” Time: 0.44s\n","  ğŸ§ª Batch 50/55 â€” Loss: 0.0709 â€” Time: 0.46s\n","  ğŸ§ª Batch 51/55 â€” Loss: 0.0649 â€” Time: 0.45s\n","  ğŸ§ª Batch 52/55 â€” Loss: 0.0662 â€” Time: 0.44s\n","  ğŸ§ª Batch 53/55 â€” Loss: 0.0626 â€” Time: 0.44s\n","  ğŸ§ª Batch 54/55 â€” Loss: 0.0634 â€” Time: 0.44s\n","  ğŸ§ª Batch 55/55 â€” Loss: 0.0578 â€” Time: 0.29s\n","ğŸ•’ Epoch Time: 27.39s | Avg Batch Time: 0.44s\n","ğŸ“Š Train Loss: 0.0672 | Val Loss: 0.2291 | IoU: 0.8180 | Dice: 0.8984 | Jaccard: 0.8180 | Precision: 0.9151 | Recall: 0.8838 | Accuracy: 0.9757\n","\n","Epoch 33/100\n","  ğŸ§ª Batch 1/55 â€” Loss: 0.0605 â€” Time: 0.44s\n","  ğŸ§ª Batch 2/55 â€” Loss: 0.0770 â€” Time: 0.43s\n","  ğŸ§ª Batch 3/55 â€” Loss: 0.0582 â€” Time: 0.44s\n","  ğŸ§ª Batch 4/55 â€” Loss: 0.0527 â€” Time: 0.44s\n","  ğŸ§ª Batch 5/55 â€” Loss: 0.0636 â€” Time: 0.44s\n","  ğŸ§ª Batch 6/55 â€” Loss: 0.0713 â€” Time: 0.44s\n","  ğŸ§ª Batch 7/55 â€” Loss: 0.0583 â€” Time: 0.44s\n","  ğŸ§ª Batch 8/55 â€” Loss: 0.0555 â€” Time: 0.44s\n","  ğŸ§ª Batch 9/55 â€” Loss: 0.0677 â€” Time: 0.44s\n","  ğŸ§ª Batch 10/55 â€” Loss: 0.0538 â€” Time: 0.44s\n","  ğŸ§ª Batch 11/55 â€” Loss: 0.0663 â€” Time: 0.44s\n","  ğŸ§ª Batch 12/55 â€” Loss: 0.0584 â€” Time: 0.44s\n","  ğŸ§ª Batch 13/55 â€” Loss: 0.0681 â€” Time: 0.44s\n","  ğŸ§ª Batch 14/55 â€” Loss: 0.0847 â€” Time: 0.44s\n","  ğŸ§ª Batch 15/55 â€” Loss: 0.0581 â€” Time: 0.44s\n","  ğŸ§ª Batch 16/55 â€” Loss: 0.0707 â€” Time: 0.44s\n","  ğŸ§ª Batch 17/55 â€” Loss: 0.0687 â€” Time: 0.44s\n","  ğŸ§ª Batch 18/55 â€” Loss: 0.0659 â€” Time: 0.44s\n","  ğŸ§ª Batch 19/55 â€” Loss: 0.0595 â€” Time: 0.45s\n","  ğŸ§ª Batch 20/55 â€” Loss: 0.0713 â€” Time: 0.45s\n","  ğŸ§ª Batch 21/55 â€” Loss: 0.0667 â€” Time: 0.46s\n","  ğŸ§ª Batch 22/55 â€” Loss: 0.0668 â€” Time: 0.48s\n","  ğŸ§ª Batch 23/55 â€” Loss: 0.0594 â€” Time: 0.47s\n","  ğŸ§ª Batch 24/55 â€” Loss: 0.0723 â€” Time: 0.51s\n","  ğŸ§ª Batch 25/55 â€” Loss: 0.0687 â€” Time: 0.48s\n","  ğŸ§ª Batch 26/55 â€” Loss: 0.0634 â€” Time: 0.47s\n","  ğŸ§ª Batch 27/55 â€” Loss: 0.0696 â€” Time: 0.46s\n","  ğŸ§ª Batch 28/55 â€” Loss: 0.0706 â€” Time: 0.45s\n","  ğŸ§ª Batch 29/55 â€” Loss: 0.0635 â€” Time: 0.44s\n","  ğŸ§ª Batch 30/55 â€” Loss: 0.0665 â€” Time: 0.44s\n","  ğŸ§ª Batch 31/55 â€” Loss: 0.0579 â€” Time: 0.43s\n","  ğŸ§ª Batch 32/55 â€” Loss: 0.0571 â€” Time: 0.44s\n","  ğŸ§ª Batch 33/55 â€” Loss: 0.0685 â€” Time: 0.44s\n","  ğŸ§ª Batch 34/55 â€” Loss: 0.0682 â€” Time: 0.44s\n","  ğŸ§ª Batch 35/55 â€” Loss: 0.0598 â€” Time: 0.44s\n","  ğŸ§ª Batch 36/55 â€” Loss: 0.0631 â€” Time: 0.44s\n","  ğŸ§ª Batch 37/55 â€” Loss: 0.0659 â€” Time: 0.44s\n","  ğŸ§ª Batch 38/55 â€” Loss: 0.0595 â€” Time: 0.44s\n","  ğŸ§ª Batch 39/55 â€” Loss: 0.0734 â€” Time: 0.44s\n","  ğŸ§ª Batch 40/55 â€” Loss: 0.0762 â€” Time: 0.44s\n","  ğŸ§ª Batch 41/55 â€” Loss: 0.0607 â€” Time: 0.44s\n","  ğŸ§ª Batch 42/55 â€” Loss: 0.0706 â€” Time: 0.44s\n","  ğŸ§ª Batch 43/55 â€” Loss: 0.0653 â€” Time: 0.44s\n","  ğŸ§ª Batch 44/55 â€” Loss: 0.0625 â€” Time: 0.44s\n","  ğŸ§ª Batch 45/55 â€” Loss: 0.0650 â€” Time: 0.44s\n","  ğŸ§ª Batch 46/55 â€” Loss: 0.0612 â€” Time: 0.44s\n","  ğŸ§ª Batch 47/55 â€” Loss: 0.0590 â€” Time: 0.44s\n","  ğŸ§ª Batch 48/55 â€” Loss: 0.0642 â€” Time: 0.44s\n","  ğŸ§ª Batch 49/55 â€” Loss: 0.0558 â€” Time: 0.44s\n","  ğŸ§ª Batch 50/55 â€” Loss: 0.0610 â€” Time: 0.45s\n","  ğŸ§ª Batch 51/55 â€” Loss: 0.1009 â€” Time: 0.44s\n","  ğŸ§ª Batch 52/55 â€” Loss: 0.0806 â€” Time: 0.44s\n","  ğŸ§ª Batch 53/55 â€” Loss: 0.0609 â€” Time: 0.44s\n","  ğŸ§ª Batch 54/55 â€” Loss: 0.0605 â€” Time: 0.44s\n","  ğŸ§ª Batch 55/55 â€” Loss: 0.0712 â€” Time: 0.29s\n","ğŸ•’ Epoch Time: 27.56s | Avg Batch Time: 0.44s\n","ğŸ“Š Train Loss: 0.0656 | Val Loss: 0.2326 | IoU: 0.8161 | Dice: 0.8972 | Jaccard: 0.8161 | Precision: 0.9272 | Recall: 0.8711 | Accuracy: 0.9757\n","\n","Epoch 34/100\n","  ğŸ§ª Batch 1/55 â€” Loss: 0.0626 â€” Time: 0.45s\n","  ğŸ§ª Batch 2/55 â€” Loss: 0.0562 â€” Time: 0.44s\n","  ğŸ§ª Batch 3/55 â€” Loss: 0.0616 â€” Time: 0.44s\n","  ğŸ§ª Batch 4/55 â€” Loss: 0.0555 â€” Time: 0.44s\n","  ğŸ§ª Batch 5/55 â€” Loss: 0.0612 â€” Time: 0.44s\n","  ğŸ§ª Batch 6/55 â€” Loss: 0.0692 â€” Time: 0.44s\n","  ğŸ§ª Batch 7/55 â€” Loss: 0.0796 â€” Time: 0.44s\n","  ğŸ§ª Batch 8/55 â€” Loss: 0.0715 â€” Time: 0.44s\n","  ğŸ§ª Batch 9/55 â€” Loss: 0.0574 â€” Time: 0.44s\n","  ğŸ§ª Batch 10/55 â€” Loss: 0.0537 â€” Time: 0.44s\n","  ğŸ§ª Batch 11/55 â€” Loss: 0.0701 â€” Time: 0.43s\n","  ğŸ§ª Batch 12/55 â€” Loss: 0.0581 â€” Time: 0.44s\n","  ğŸ§ª Batch 13/55 â€” Loss: 0.0629 â€” Time: 0.44s\n","  ğŸ§ª Batch 14/55 â€” Loss: 0.0540 â€” Time: 0.44s\n","  ğŸ§ª Batch 15/55 â€” Loss: 0.0707 â€” Time: 0.44s\n","  ğŸ§ª Batch 16/55 â€” Loss: 0.0585 â€” Time: 0.44s\n","  ğŸ§ª Batch 17/55 â€” Loss: 0.0663 â€” Time: 0.44s\n","  ğŸ§ª Batch 18/55 â€” Loss: 0.0541 â€” Time: 0.44s\n","  ğŸ§ª Batch 19/55 â€” Loss: 0.0699 â€” Time: 0.44s\n","  ğŸ§ª Batch 20/55 â€” Loss: 0.0605 â€” Time: 0.44s\n","  ğŸ§ª Batch 21/55 â€” Loss: 0.0653 â€” Time: 0.44s\n","  ğŸ§ª Batch 22/55 â€” Loss: 0.0558 â€” Time: 0.44s\n","  ğŸ§ª Batch 23/55 â€” Loss: 0.0654 â€” Time: 0.47s\n","  ğŸ§ª Batch 24/55 â€” Loss: 0.0594 â€” Time: 0.45s\n","  ğŸ§ª Batch 25/55 â€” Loss: 0.0612 â€” Time: 0.50s\n","  ğŸ§ª Batch 26/55 â€” Loss: 0.0633 â€” Time: 0.51s\n","  ğŸ§ª Batch 27/55 â€” Loss: 0.0617 â€” Time: 0.44s\n","  ğŸ§ª Batch 28/55 â€” Loss: 0.0620 â€” Time: 0.44s\n","  ğŸ§ª Batch 29/55 â€” Loss: 0.0676 â€” Time: 0.44s\n","  ğŸ§ª Batch 30/55 â€” Loss: 0.0701 â€” Time: 0.45s\n","  ğŸ§ª Batch 31/55 â€” Loss: 0.0566 â€” Time: 0.44s\n","  ğŸ§ª Batch 32/55 â€” Loss: 0.0676 â€” Time: 0.45s\n","  ğŸ§ª Batch 33/55 â€” Loss: 0.0646 â€” Time: 0.44s\n","  ğŸ§ª Batch 34/55 â€” Loss: 0.0570 â€” Time: 0.44s\n","  ğŸ§ª Batch 35/55 â€” Loss: 0.0642 â€” Time: 0.44s\n","  ğŸ§ª Batch 36/55 â€” Loss: 0.0609 â€” Time: 0.44s\n","  ğŸ§ª Batch 37/55 â€” Loss: 0.0728 â€” Time: 0.44s\n","  ğŸ§ª Batch 38/55 â€” Loss: 0.0697 â€” Time: 0.44s\n","  ğŸ§ª Batch 39/55 â€” Loss: 0.0863 â€” Time: 0.44s\n","  ğŸ§ª Batch 40/55 â€” Loss: 0.0620 â€” Time: 0.44s\n","  ğŸ§ª Batch 41/55 â€” Loss: 0.0662 â€” Time: 0.44s\n","  ğŸ§ª Batch 42/55 â€” Loss: 0.0655 â€” Time: 0.44s\n","  ğŸ§ª Batch 43/55 â€” Loss: 0.0719 â€” Time: 0.44s\n","  ğŸ§ª Batch 44/55 â€” Loss: 0.0558 â€” Time: 0.44s\n","  ğŸ§ª Batch 45/55 â€” Loss: 0.0646 â€” Time: 0.44s\n","  ğŸ§ª Batch 46/55 â€” Loss: 0.0640 â€” Time: 0.44s\n","  ğŸ§ª Batch 47/55 â€” Loss: 0.0659 â€” Time: 0.44s\n","  ğŸ§ª Batch 48/55 â€” Loss: 0.0621 â€” Time: 0.44s\n","  ğŸ§ª Batch 49/55 â€” Loss: 0.0563 â€” Time: 0.44s\n","  ğŸ§ª Batch 50/55 â€” Loss: 0.0682 â€” Time: 0.44s\n","  ğŸ§ª Batch 51/55 â€” Loss: 0.0597 â€” Time: 0.44s\n","  ğŸ§ª Batch 52/55 â€” Loss: 0.0883 â€” Time: 0.44s\n","  ğŸ§ª Batch 53/55 â€” Loss: 0.0685 â€” Time: 0.44s\n","  ğŸ§ª Batch 54/55 â€” Loss: 0.0828 â€” Time: 0.44s\n","  ğŸ§ª Batch 55/55 â€” Loss: 0.0611 â€” Time: 0.29s\n","ğŸ•’ Epoch Time: 27.54s | Avg Batch Time: 0.44s\n","ğŸ“Š Train Loss: 0.0645 | Val Loss: 0.2258 | IoU: 0.8212 | Dice: 0.9004 | Jaccard: 0.8212 | Precision: 0.9303 | Recall: 0.8746 | Accuracy: 0.9765\n","\n","Epoch 35/100\n","  ğŸ§ª Batch 1/55 â€” Loss: 0.0660 â€” Time: 0.54s\n","  ğŸ§ª Batch 2/55 â€” Loss: 0.0674 â€” Time: 0.47s\n","  ğŸ§ª Batch 3/55 â€” Loss: 0.0648 â€” Time: 0.52s\n","  ğŸ§ª Batch 4/55 â€” Loss: 0.0613 â€” Time: 0.44s\n","  ğŸ§ª Batch 5/55 â€” Loss: 0.0656 â€” Time: 0.53s\n","  ğŸ§ª Batch 6/55 â€” Loss: 0.0608 â€” Time: 0.52s\n","  ğŸ§ª Batch 7/55 â€” Loss: 0.0636 â€” Time: 0.43s\n","  ğŸ§ª Batch 8/55 â€” Loss: 0.0638 â€” Time: 0.44s\n","  ğŸ§ª Batch 9/55 â€” Loss: 0.0570 â€” Time: 0.44s\n","  ğŸ§ª Batch 10/55 â€” Loss: 0.0737 â€” Time: 0.44s\n","  ğŸ§ª Batch 11/55 â€” Loss: 0.0731 â€” Time: 0.44s\n","  ğŸ§ª Batch 12/55 â€” Loss: 0.0586 â€” Time: 0.44s\n","  ğŸ§ª Batch 13/55 â€” Loss: 0.0604 â€” Time: 0.44s\n","  ğŸ§ª Batch 14/55 â€” Loss: 0.0564 â€” Time: 0.44s\n","  ğŸ§ª Batch 15/55 â€” Loss: 0.0556 â€” Time: 0.44s\n","  ğŸ§ª Batch 16/55 â€” Loss: 0.0597 â€” Time: 0.44s\n","  ğŸ§ª Batch 17/55 â€” Loss: 0.0653 â€” Time: 0.44s\n","  ğŸ§ª Batch 18/55 â€” Loss: 0.0593 â€” Time: 0.45s\n","  ğŸ§ª Batch 19/55 â€” Loss: 0.0771 â€” Time: 0.47s\n","  ğŸ§ª Batch 20/55 â€” Loss: 0.0591 â€” Time: 0.44s\n","  ğŸ§ª Batch 21/55 â€” Loss: 0.0613 â€” Time: 0.44s\n","  ğŸ§ª Batch 22/55 â€” Loss: 0.0587 â€” Time: 0.46s\n","  ğŸ§ª Batch 23/55 â€” Loss: 0.0566 â€” Time: 0.47s\n","  ğŸ§ª Batch 24/55 â€” Loss: 0.0661 â€” Time: 0.44s\n","  ğŸ§ª Batch 25/55 â€” Loss: 0.0795 â€” Time: 0.44s\n","  ğŸ§ª Batch 26/55 â€” Loss: 0.0561 â€” Time: 0.44s\n","  ğŸ§ª Batch 27/55 â€” Loss: 0.0584 â€” Time: 0.45s\n","  ğŸ§ª Batch 28/55 â€” Loss: 0.0665 â€” Time: 0.44s\n","  ğŸ§ª Batch 29/55 â€” Loss: 0.0568 â€” Time: 0.44s\n","  ğŸ§ª Batch 30/55 â€” Loss: 0.0601 â€” Time: 0.44s\n","  ğŸ§ª Batch 31/55 â€” Loss: 0.0566 â€” Time: 0.44s\n","  ğŸ§ª Batch 32/55 â€” Loss: 0.0549 â€” Time: 0.44s\n","  ğŸ§ª Batch 33/55 â€” Loss: 0.0740 â€” Time: 0.44s\n","  ğŸ§ª Batch 34/55 â€” Loss: 0.0618 â€” Time: 0.44s\n","  ğŸ§ª Batch 35/55 â€” Loss: 0.0601 â€” Time: 0.44s\n","  ğŸ§ª Batch 36/55 â€” Loss: 0.0614 â€” Time: 0.44s\n","  ğŸ§ª Batch 37/55 â€” Loss: 0.0605 â€” Time: 0.44s\n","  ğŸ§ª Batch 38/55 â€” Loss: 0.0629 â€” Time: 0.44s\n","  ğŸ§ª Batch 39/55 â€” Loss: 0.0653 â€” Time: 0.44s\n","  ğŸ§ª Batch 40/55 â€” Loss: 0.0628 â€” Time: 0.44s\n","  ğŸ§ª Batch 41/55 â€” Loss: 0.0640 â€” Time: 0.44s\n","  ğŸ§ª Batch 42/55 â€” Loss: 0.0612 â€” Time: 0.44s\n","  ğŸ§ª Batch 43/55 â€” Loss: 0.0618 â€” Time: 0.44s\n","  ğŸ§ª Batch 44/55 â€” Loss: 0.0641 â€” Time: 0.44s\n","  ğŸ§ª Batch 45/55 â€” Loss: 0.0831 â€” Time: 0.44s\n","  ğŸ§ª Batch 46/55 â€” Loss: 0.0742 â€” Time: 0.44s\n","  ğŸ§ª Batch 47/55 â€” Loss: 0.0552 â€” Time: 0.44s\n","  ğŸ§ª Batch 48/55 â€” Loss: 0.0637 â€” Time: 0.44s\n","  ğŸ§ª Batch 49/55 â€” Loss: 0.0592 â€” Time: 0.45s\n","  ğŸ§ª Batch 50/55 â€” Loss: 0.0664 â€” Time: 0.49s\n","  ğŸ§ª Batch 51/55 â€” Loss: 0.0698 â€” Time: 0.46s\n","  ğŸ§ª Batch 52/55 â€” Loss: 0.0591 â€” Time: 0.44s\n","  ğŸ§ª Batch 53/55 â€” Loss: 0.0594 â€” Time: 0.44s\n","  ğŸ§ª Batch 54/55 â€” Loss: 0.0598 â€” Time: 0.44s\n","  ğŸ§ª Batch 55/55 â€” Loss: 0.0587 â€” Time: 0.29s\n","ğŸ•’ Epoch Time: 28.09s | Avg Batch Time: 0.45s\n","ğŸ“Š Train Loss: 0.0631 | Val Loss: 0.2292 | IoU: 0.8167 | Dice: 0.8976 | Jaccard: 0.8167 | Precision: 0.9302 | Recall: 0.8687 | Accuracy: 0.9759\n","â¹ï¸ Early stopping triggered after 35 epochs.\n","\n","ğŸ§ª Testing model trained on CombinedDataset on all datasets...\n","| Trained On      | Tested On       |     Loss |      IoU |     Dice |   Jaccard |   Precision |   Recall |   Accuracy |\n","|:----------------|:----------------|---------:|---------:|---------:|----------:|------------:|---------:|-----------:|\n","| CombinedDataset | CombinedDataset | 0.185574 | 0.841047 | 0.913258 |  0.841047 |    0.939811 | 0.888972 |   0.980911 |\n","\n","â±ï¸ Total Time for Training + Testing: 979.14 seconds\n","\n","ğŸ“‹ Final Cross-Dataset Testing Summary:\n","| Trained On      | Tested On       |     Loss |      IoU |     Dice |   Jaccard |   Precision |   Recall |   Accuracy |\n","|:----------------|:----------------|---------:|---------:|---------:|----------:|------------:|---------:|-----------:|\n","| CombinedDataset | CombinedDataset | 0.185574 | 0.841047 | 0.913258 |  0.841047 |    0.939811 | 0.888972 |   0.980911 |\n"]}]},{"cell_type":"code","source":["evaluation_df = evaluate_model_on_datasets(trained_model, separate_loaders)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"nh2VRy5px8l1","executionInfo":{"status":"ok","timestamp":1745354753664,"user_tz":240,"elapsed":3087,"user":{"displayName":"Ashok Kamath","userId":"01269285531323045015"}},"outputId":"e9b01812-a418-40c6-dd7d-08d594d16c77"},"execution_count":29,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","ğŸ§ª Evaluating trained model on all test sets...\n","\n","ğŸ” Testing on: CVC-ClinicDB\n","ğŸ” Testing on: CVC-ColonDB\n","ğŸ” Testing on: ETIS-LaribPolypDB\n","ğŸ” Testing on: Kvasir-SEG\n","\n","â±ï¸ Total Evaluation Time: 3.05 seconds\n","\n","ğŸ“‹ Test Results Across Datasets (with Average):\n","| Tested On         |     Loss |      IoU |     Dice |   Jaccard |   Precision |   Recall |   Accuracy |\n","|:------------------|---------:|---------:|---------:|----------:|------------:|---------:|-----------:|\n","| CVC-ClinicDB      | 0.173286 | 0.846071 | 0.915336 |  0.846071 |    0.944182 | 0.892699 |   0.984647 |\n","| CVC-ColonDB       | 0.138843 | 0.85811  | 0.923507 |  0.85811  |    0.93774  | 0.909704 |   0.992436 |\n","| ETIS-LaribPolypDB | 0.464115 | 0.51178  | 0.638788 |  0.51178  |    0.938224 | 0.540285 |   0.984364 |\n","| Kvasir-SEG        | 0.198417 | 0.848067 | 0.9172   |  0.848067 |    0.933992 | 0.901806 |   0.975417 |\n","| Average           | 0.243665 | 0.766007 | 0.848708 |  0.766007 |    0.938534 | 0.811123 |   0.984216 |\n"]}]},{"cell_type":"markdown","source":["# U-Net++ (no pretrained weights)\n"],"metadata":{"id":"KrTaZDMkA6ZR"}},{"cell_type":"code","source":["def unet_no_PT_weights_model_factory():\n","    return smp.UnetPlusPlus(\n","        encoder_name=\"resnet34\",     # Same encoder as your U-Net\n","        encoder_weights=None,  # pretrained on ImageNet\n","        in_channels=3,               # rgb input\n","        classes=1,                   # Binary segmentation (e.g., polyps)\n","    )"],"metadata":{"id":"auu5otDqA_bc","executionInfo":{"status":"ok","timestamp":1745351391759,"user_tz":240,"elapsed":4,"user":{"displayName":"Ashok Kamath","userId":"01269285531323045015"}}},"execution_count":25,"outputs":[]},{"cell_type":"code","source":["u_net_results, trained_model = train_and_cross_test(model_class=unet_no_PT_weights_model_factory, all_data_loaders=[combined_loaders], num_epochs=1)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ygPmgsg-BJb-","executionInfo":{"status":"ok","timestamp":1745352335537,"user_tz":240,"elapsed":942525,"user":{"displayName":"Ashok Kamath","userId":"01269285531323045015"}},"outputId":"64b3ab0e-5689-452d-90ba-658f82f7dbef"},"execution_count":26,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","ğŸš€ Training on: CombinedDataset\n","\n","Epoch 1/1\n","  ğŸ§ª Batch 1/55 â€” Loss: 1.5803 â€” Time: 1.50s\n","  ğŸ§ª Batch 2/55 â€” Loss: 1.5544 â€” Time: 0.81s\n","  ğŸ§ª Batch 3/55 â€” Loss: 1.5796 â€” Time: 0.85s\n","  ğŸ§ª Batch 4/55 â€” Loss: 1.5413 â€” Time: 0.81s\n","  ğŸ§ª Batch 5/55 â€” Loss: 1.3381 â€” Time: 0.87s\n","  ğŸ§ª Batch 6/55 â€” Loss: 1.4447 â€” Time: 0.81s\n","  ğŸ§ª Batch 7/55 â€” Loss: 1.4720 â€” Time: 0.87s\n","  ğŸ§ª Batch 8/55 â€” Loss: 1.3836 â€” Time: 0.81s\n","  ğŸ§ª Batch 9/55 â€” Loss: 1.4174 â€” Time: 0.86s\n","  ğŸ§ª Batch 10/55 â€” Loss: 1.4023 â€” Time: 0.81s\n","  ğŸ§ª Batch 11/55 â€” Loss: 1.4084 â€” Time: 0.86s\n","  ğŸ§ª Batch 12/55 â€” Loss: 1.4311 â€” Time: 0.81s\n","  ğŸ§ª Batch 13/55 â€” Loss: 1.4196 â€” Time: 0.85s\n","  ğŸ§ª Batch 14/55 â€” Loss: 1.3889 â€” Time: 0.81s\n","  ğŸ§ª Batch 15/55 â€” Loss: 1.3947 â€” Time: 0.86s\n","  ğŸ§ª Batch 16/55 â€” Loss: 1.3385 â€” Time: 0.81s\n","  ğŸ§ª Batch 17/55 â€” Loss: 1.2801 â€” Time: 0.86s\n","  ğŸ§ª Batch 18/55 â€” Loss: 1.3026 â€” Time: 0.81s\n","  ğŸ§ª Batch 19/55 â€” Loss: 1.3147 â€” Time: 0.86s\n","  ğŸ§ª Batch 20/55 â€” Loss: 1.3848 â€” Time: 0.81s\n","  ğŸ§ª Batch 21/55 â€” Loss: 1.3280 â€” Time: 0.85s\n","  ğŸ§ª Batch 22/55 â€” Loss: 1.2660 â€” Time: 0.81s\n","  ğŸ§ª Batch 23/55 â€” Loss: 1.3048 â€” Time: 0.87s\n","  ğŸ§ª Batch 24/55 â€” Loss: 1.2565 â€” Time: 0.81s\n","  ğŸ§ª Batch 25/55 â€” Loss: 1.2840 â€” Time: 0.86s\n","  ğŸ§ª Batch 26/55 â€” Loss: 1.2480 â€” Time: 0.81s\n","  ğŸ§ª Batch 27/55 â€” Loss: 1.2328 â€” Time: 0.86s\n","  ğŸ§ª Batch 28/55 â€” Loss: 1.2507 â€” Time: 0.81s\n","  ğŸ§ª Batch 29/55 â€” Loss: 1.2301 â€” Time: 0.87s\n","  ğŸ§ª Batch 30/55 â€” Loss: 1.2163 â€” Time: 0.81s\n","  ğŸ§ª Batch 31/55 â€” Loss: 1.2627 â€” Time: 0.87s\n","  ğŸ§ª Batch 32/55 â€” Loss: 1.1651 â€” Time: 0.81s\n","  ğŸ§ª Batch 33/55 â€” Loss: 1.2276 â€” Time: 0.86s\n","  ğŸ§ª Batch 34/55 â€” Loss: 1.2249 â€” Time: 0.81s\n","  ğŸ§ª Batch 35/55 â€” Loss: 1.2243 â€” Time: 0.87s\n","  ğŸ§ª Batch 36/55 â€” Loss: 1.1073 â€” Time: 0.81s\n","  ğŸ§ª Batch 37/55 â€” Loss: 1.1914 â€” Time: 0.86s\n","  ğŸ§ª Batch 38/55 â€” Loss: 1.1872 â€” Time: 0.81s\n","  ğŸ§ª Batch 39/55 â€” Loss: 1.1546 â€” Time: 0.85s\n","  ğŸ§ª Batch 40/55 â€” Loss: 1.1994 â€” Time: 0.81s\n","  ğŸ§ª Batch 41/55 â€” Loss: 1.1606 â€” Time: 0.87s\n","  ğŸ§ª Batch 42/55 â€” Loss: 1.2128 â€” Time: 0.81s\n","  ğŸ§ª Batch 43/55 â€” Loss: 1.1585 â€” Time: 0.86s\n","  ğŸ§ª Batch 44/55 â€” Loss: 1.1507 â€” Time: 0.81s\n","  ğŸ§ª Batch 45/55 â€” Loss: 1.2612 â€” Time: 0.87s\n","  ğŸ§ª Batch 46/55 â€” Loss: 1.1644 â€” Time: 0.81s\n","  ğŸ§ª Batch 47/55 â€” Loss: 1.0804 â€” Time: 0.85s\n","  ğŸ§ª Batch 48/55 â€” Loss: 1.1080 â€” Time: 0.81s\n","  ğŸ§ª Batch 49/55 â€” Loss: 1.1066 â€” Time: 0.85s\n","  ğŸ§ª Batch 50/55 â€” Loss: 1.1869 â€” Time: 0.81s\n","  ğŸ§ª Batch 51/55 â€” Loss: 1.0564 â€” Time: 0.86s\n","  ğŸ§ª Batch 52/55 â€” Loss: 1.0895 â€” Time: 0.82s\n","  ğŸ§ª Batch 53/55 â€” Loss: 1.1339 â€” Time: 0.86s\n","  ğŸ§ª Batch 54/55 â€” Loss: 1.1496 â€” Time: 0.82s\n","  ğŸ§ª Batch 55/55 â€” Loss: 1.1784 â€” Time: 0.66s\n","ğŸ•’ Epoch Time: 816.18s | Avg Batch Time: 0.84s\n","ğŸ“Š Train Loss: 1.2752 | Val Loss: 1.1272 | IoU: 0.2016 | Dice: 0.3245 | Jaccard: 0.2016 | Precision: 0.4498 | Recall: 0.2655 | Accuracy: 0.8880\n","\n","ğŸ§ª Testing model trained on CombinedDataset on all datasets...\n","| Trained On      | Tested On       |    Loss |      IoU |     Dice |   Jaccard |   Precision |   Recall |   Accuracy |\n","|:----------------|:----------------|--------:|---------:|---------:|----------:|------------:|---------:|-----------:|\n","| CombinedDataset | CombinedDataset | 1.12169 | 0.209385 | 0.333648 |  0.209385 |    0.465571 | 0.273435 |   0.890515 |\n","\n","â±ï¸ Total Time for Training + Testing: 942.43 seconds\n","\n","ğŸ“‹ Final Cross-Dataset Testing Summary:\n","| Trained On      | Tested On       |    Loss |      IoU |     Dice |   Jaccard |   Precision |   Recall |   Accuracy |\n","|:----------------|:----------------|--------:|---------:|---------:|----------:|------------:|---------:|-----------:|\n","| CombinedDataset | CombinedDataset | 1.12169 | 0.209385 | 0.333648 |  0.209385 |    0.465571 | 0.273435 |   0.890515 |\n"]}]},{"cell_type":"code","source":["evaluation_df = evaluate_model_on_datasets(trained_model, separate_loaders)"],"metadata":{"id":"tkhBjs3VBtuu"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"KcFJ3kp07J0Q"},"source":["# U-Net++"]},{"cell_type":"code","source":["# This crashes the runtime, which restarts the whole Colab environment\n","import os\n","os.kill(os.getpid(), 9)\n"],"metadata":{"id":"DJ7jifFq839k"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!nvidia-smi\n","\n","torch.cuda.empty_cache()\n","torch.cuda.reset_peak_memory_stats()\n","\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"yWJrfG-Y7u5e","executionInfo":{"status":"ok","timestamp":1745265770201,"user_tz":240,"elapsed":215,"user":{"displayName":"Ashok Kamath","userId":"01269285531323045015"}},"outputId":"df1af7e9-06e3-4334-c1e9-d06e36d3a9e2"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Mon Apr 21 20:02:49 2025       \n","+-----------------------------------------------------------------------------------------+\n","| NVIDIA-SMI 550.54.15              Driver Version: 550.54.15      CUDA Version: 12.4     |\n","|-----------------------------------------+------------------------+----------------------+\n","| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n","| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n","|                                         |                        |               MIG M. |\n","|=========================================+========================+======================|\n","|   0  Tesla T4                       Off |   00000000:00:04.0 Off |                    0 |\n","| N/A   75C    P0             33W /   70W |   15092MiB /  15360MiB |      0%      Default |\n","|                                         |                        |                  N/A |\n","+-----------------------------------------+------------------------+----------------------+\n","                                                                                         \n","+-----------------------------------------------------------------------------------------+\n","| Processes:                                                                              |\n","|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n","|        ID   ID                                                               Usage      |\n","|=========================================================================================|\n","+-----------------------------------------------------------------------------------------+\n"]}]},{"cell_type":"code","execution_count":null,"metadata":{"id":"AfCQLIBz7GMp"},"outputs":[],"source":["def unetpp_model_factory():\n","    return smp.UnetPlusPlus(\n","        encoder_name=\"resnet34\",     # Same encoder as your U-Net\n","        encoder_weights=\"imagenet\",  # pretrained on ImageNet\n","        in_channels=3,               # rgb input\n","        classes=1,                   # Binary segmentation (e.g., polyps)\n","    )\n"]},{"cell_type":"code","source":["u_net_pp_results, trained_model = train_and_cross_test(model_class=unetpp_model_factory, all_data_loaders=[combined_loaders], num_epochs=100)\n","#u_net_pp_results = train_and_cross_test(model_class=unetpp_model_factory, all_data_loaders=all_data_loaders, num_epochs=10)\n","print('U-Net Plus Plus Results')\n","print(u_net_pp_results)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"yhYi5dDtQgdm","outputId":"943e5b93-f9d4-4902-e330-6720470c3613","executionInfo":{"status":"ok","timestamp":1745270771162,"user_tz":240,"elapsed":699580,"user":{"displayName":"Ashok Kamath","userId":"01269285531323045015"}}},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","ğŸš€ Training on: CombinedDataset\n","\n","Epoch 1/100\n","  ğŸ§ª Batch 1/55 â€” Loss: 1.4446 â€” Time: 0.91s\n","  ğŸ§ª Batch 2/55 â€” Loss: 1.3799 â€” Time: 0.89s\n","  ğŸ§ª Batch 3/55 â€” Loss: 1.3521 â€” Time: 0.88s\n","  ğŸ§ª Batch 4/55 â€” Loss: 1.3426 â€” Time: 0.87s\n","  ğŸ§ª Batch 5/55 â€” Loss: 1.3372 â€” Time: 0.89s\n","  ğŸ§ª Batch 6/55 â€” Loss: 1.2106 â€” Time: 0.88s\n","  ğŸ§ª Batch 7/55 â€” Loss: 1.2402 â€” Time: 0.88s\n","  ğŸ§ª Batch 8/55 â€” Loss: 1.2324 â€” Time: 0.89s\n","  ğŸ§ª Batch 9/55 â€” Loss: 1.2138 â€” Time: 0.89s\n","  ğŸ§ª Batch 10/55 â€” Loss: 1.1926 â€” Time: 0.89s\n","  ğŸ§ª Batch 11/55 â€” Loss: 1.2037 â€” Time: 0.89s\n","  ğŸ§ª Batch 12/55 â€” Loss: 1.1893 â€” Time: 0.89s\n","  ğŸ§ª Batch 13/55 â€” Loss: 1.1141 â€” Time: 0.89s\n","  ğŸ§ª Batch 14/55 â€” Loss: 1.1224 â€” Time: 0.91s\n","  ğŸ§ª Batch 15/55 â€” Loss: 1.1830 â€” Time: 0.90s\n","  ğŸ§ª Batch 16/55 â€” Loss: 1.1139 â€” Time: 0.91s\n","  ğŸ§ª Batch 17/55 â€” Loss: 1.1111 â€” Time: 0.91s\n","  ğŸ§ª Batch 18/55 â€” Loss: 1.0969 â€” Time: 0.91s\n","  ğŸ§ª Batch 19/55 â€” Loss: 1.0349 â€” Time: 0.91s\n","  ğŸ§ª Batch 20/55 â€” Loss: 1.0250 â€” Time: 0.90s\n","  ğŸ§ª Batch 21/55 â€” Loss: 1.0195 â€” Time: 0.91s\n","  ğŸ§ª Batch 22/55 â€” Loss: 0.9356 â€” Time: 0.91s\n","  ğŸ§ª Batch 23/55 â€” Loss: 1.0427 â€” Time: 0.93s\n","  ğŸ§ª Batch 24/55 â€” Loss: 1.0025 â€” Time: 0.93s\n","  ğŸ§ª Batch 25/55 â€” Loss: 0.9272 â€” Time: 0.92s\n","  ğŸ§ª Batch 26/55 â€” Loss: 1.0002 â€” Time: 0.93s\n","  ğŸ§ª Batch 27/55 â€” Loss: 0.9768 â€” Time: 0.93s\n","  ğŸ§ª Batch 28/55 â€” Loss: 0.8515 â€” Time: 0.92s\n","  ğŸ§ª Batch 29/55 â€” Loss: 1.0292 â€” Time: 0.92s\n","  ğŸ§ª Batch 30/55 â€” Loss: 0.9903 â€” Time: 0.92s\n","  ğŸ§ª Batch 31/55 â€” Loss: 0.9652 â€” Time: 0.93s\n","  ğŸ§ª Batch 32/55 â€” Loss: 0.8807 â€” Time: 0.92s\n","  ğŸ§ª Batch 33/55 â€” Loss: 0.9332 â€” Time: 0.92s\n","  ğŸ§ª Batch 34/55 â€” Loss: 0.9192 â€” Time: 0.92s\n","  ğŸ§ª Batch 35/55 â€” Loss: 0.8777 â€” Time: 0.91s\n","  ğŸ§ª Batch 36/55 â€” Loss: 0.8445 â€” Time: 0.90s\n","  ğŸ§ª Batch 37/55 â€” Loss: 0.9198 â€” Time: 0.91s\n","  ğŸ§ª Batch 38/55 â€” Loss: 0.8735 â€” Time: 0.91s\n","  ğŸ§ª Batch 39/55 â€” Loss: 0.8682 â€” Time: 0.91s\n","  ğŸ§ª Batch 40/55 â€” Loss: 0.7862 â€” Time: 0.90s\n","  ğŸ§ª Batch 41/55 â€” Loss: 0.9072 â€” Time: 0.90s\n","  ğŸ§ª Batch 42/55 â€” Loss: 0.8093 â€” Time: 0.89s\n","  ğŸ§ª Batch 43/55 â€” Loss: 0.8385 â€” Time: 0.89s\n","  ğŸ§ª Batch 44/55 â€” Loss: 0.9494 â€” Time: 0.89s\n","  ğŸ§ª Batch 45/55 â€” Loss: 0.9224 â€” Time: 0.90s\n","  ğŸ§ª Batch 46/55 â€” Loss: 0.8758 â€” Time: 0.89s\n","  ğŸ§ª Batch 47/55 â€” Loss: 0.8554 â€” Time: 0.89s\n","  ğŸ§ª Batch 48/55 â€” Loss: 0.7354 â€” Time: 0.89s\n","  ğŸ§ª Batch 49/55 â€” Loss: 0.8602 â€” Time: 0.89s\n","  ğŸ§ª Batch 50/55 â€” Loss: 0.8361 â€” Time: 0.89s\n","  ğŸ§ª Batch 51/55 â€” Loss: 0.8184 â€” Time: 0.89s\n","  ğŸ§ª Batch 52/55 â€” Loss: 0.8514 â€” Time: 0.89s\n","  ğŸ§ª Batch 53/55 â€” Loss: 0.8647 â€” Time: 0.89s\n","  ğŸ§ª Batch 54/55 â€” Loss: 0.8640 â€” Time: 0.89s\n","  ğŸ§ª Batch 55/55 â€” Loss: 0.8111 â€” Time: 0.65s\n","ğŸ•’ Epoch Time: 52.73s | Avg Batch Time: 0.90s\n","ğŸ“Š Train Loss: 1.0106 | Val Loss: 0.8043 | IoU: 0.6302 | Dice: 0.8016 | Jaccard: 0.6694 | Precision: 0.7483 | Recall: 0.8665 | Accuracy: 0.9527\n","\n","Epoch 2/100\n","  ğŸ§ª Batch 1/55 â€” Loss: 0.7568 â€” Time: 0.87s\n","  ğŸ§ª Batch 2/55 â€” Loss: 0.7902 â€” Time: 0.87s\n","  ğŸ§ª Batch 3/55 â€” Loss: 0.7798 â€” Time: 0.89s\n","  ğŸ§ª Batch 4/55 â€” Loss: 0.7251 â€” Time: 0.88s\n","  ğŸ§ª Batch 5/55 â€” Loss: 0.7547 â€” Time: 0.88s\n","  ğŸ§ª Batch 6/55 â€” Loss: 0.7334 â€” Time: 0.88s\n","  ğŸ§ª Batch 7/55 â€” Loss: 0.7188 â€” Time: 0.88s\n","  ğŸ§ª Batch 8/55 â€” Loss: 0.7514 â€” Time: 0.87s\n","  ğŸ§ª Batch 9/55 â€” Loss: 0.6973 â€” Time: 0.87s\n","  ğŸ§ª Batch 10/55 â€” Loss: 0.6889 â€” Time: 0.89s\n","  ğŸ§ª Batch 11/55 â€” Loss: 0.7555 â€” Time: 0.88s\n","  ğŸ§ª Batch 12/55 â€” Loss: 0.6748 â€” Time: 0.87s\n","  ğŸ§ª Batch 13/55 â€” Loss: 0.7298 â€” Time: 0.87s\n","  ğŸ§ª Batch 14/55 â€” Loss: 0.8099 â€” Time: 0.87s\n","  ğŸ§ª Batch 15/55 â€” Loss: 0.7140 â€” Time: 0.88s\n","  ğŸ§ª Batch 16/55 â€” Loss: 0.7000 â€” Time: 0.88s\n","  ğŸ§ª Batch 17/55 â€” Loss: 0.6423 â€” Time: 0.87s\n","  ğŸ§ª Batch 18/55 â€” Loss: 0.6630 â€” Time: 0.87s\n","  ğŸ§ª Batch 19/55 â€” Loss: 0.6707 â€” Time: 0.87s\n","  ğŸ§ª Batch 20/55 â€” Loss: 0.6222 â€” Time: 0.88s\n","  ğŸ§ª Batch 21/55 â€” Loss: 0.6637 â€” Time: 0.88s\n","  ğŸ§ª Batch 22/55 â€” Loss: 0.7461 â€” Time: 0.87s\n","  ğŸ§ª Batch 23/55 â€” Loss: 0.6332 â€” Time: 0.87s\n","  ğŸ§ª Batch 24/55 â€” Loss: 0.6507 â€” Time: 0.89s\n","  ğŸ§ª Batch 25/55 â€” Loss: 0.6169 â€” Time: 0.88s\n","  ğŸ§ª Batch 26/55 â€” Loss: 0.6668 â€” Time: 0.88s\n","  ğŸ§ª Batch 27/55 â€” Loss: 0.6087 â€” Time: 0.87s\n","  ğŸ§ª Batch 28/55 â€” Loss: 0.6422 â€” Time: 0.89s\n","  ğŸ§ª Batch 29/55 â€” Loss: 0.7939 â€” Time: 0.88s\n","  ğŸ§ª Batch 30/55 â€” Loss: 0.6169 â€” Time: 0.88s\n","  ğŸ§ª Batch 31/55 â€” Loss: 0.6376 â€” Time: 0.87s\n","  ğŸ§ª Batch 32/55 â€” Loss: 0.5779 â€” Time: 0.89s\n","  ğŸ§ª Batch 33/55 â€” Loss: 0.6426 â€” Time: 0.89s\n","  ğŸ§ª Batch 34/55 â€” Loss: 0.6750 â€” Time: 0.89s\n","  ğŸ§ª Batch 35/55 â€” Loss: 0.6103 â€” Time: 0.88s\n","  ğŸ§ª Batch 36/55 â€” Loss: 0.6301 â€” Time: 0.89s\n","  ğŸ§ª Batch 37/55 â€” Loss: 0.5704 â€” Time: 0.89s\n","  ğŸ§ª Batch 38/55 â€” Loss: 0.7090 â€” Time: 0.89s\n","  ğŸ§ª Batch 39/55 â€” Loss: 0.7223 â€” Time: 0.88s\n","  ğŸ§ª Batch 40/55 â€” Loss: 0.6148 â€” Time: 0.89s\n","  ğŸ§ª Batch 41/55 â€” Loss: 0.6479 â€” Time: 0.89s\n","  ğŸ§ª Batch 42/55 â€” Loss: 0.6180 â€” Time: 0.89s\n","  ğŸ§ª Batch 43/55 â€” Loss: 0.5152 â€” Time: 0.89s\n","  ğŸ§ª Batch 44/55 â€” Loss: 0.6457 â€” Time: 0.89s\n","  ğŸ§ª Batch 45/55 â€” Loss: 0.5762 â€” Time: 0.89s\n","  ğŸ§ª Batch 46/55 â€” Loss: 0.5511 â€” Time: 0.89s\n","  ğŸ§ª Batch 47/55 â€” Loss: 0.6398 â€” Time: 0.89s\n","  ğŸ§ª Batch 48/55 â€” Loss: 0.6141 â€” Time: 0.89s\n","  ğŸ§ª Batch 49/55 â€” Loss: 0.5251 â€” Time: 0.89s\n","  ğŸ§ª Batch 50/55 â€” Loss: 0.5660 â€” Time: 0.89s\n","  ğŸ§ª Batch 51/55 â€” Loss: 0.5574 â€” Time: 0.89s\n","  ğŸ§ª Batch 52/55 â€” Loss: 0.5700 â€” Time: 0.89s\n","  ğŸ§ª Batch 53/55 â€” Loss: 0.5184 â€” Time: 0.89s\n","  ğŸ§ª Batch 54/55 â€” Loss: 0.6288 â€” Time: 0.89s\n","  ğŸ§ª Batch 55/55 â€” Loss: 0.5971 â€” Time: 0.65s\n","ğŸ•’ Epoch Time: 51.66s | Avg Batch Time: 0.88s\n","ğŸ“Š Train Loss: 0.6578 | Val Loss: 0.6254 | IoU: 0.6776 | Dice: 0.8220 | Jaccard: 0.6999 | Precision: 0.7430 | Recall: 0.9236 | Accuracy: 0.9523\n","\n","Epoch 3/100\n","  ğŸ§ª Batch 1/55 â€” Loss: 0.5532 â€” Time: 0.88s\n","  ğŸ§ª Batch 2/55 â€” Loss: 0.5425 â€” Time: 0.89s\n","  ğŸ§ª Batch 3/55 â€” Loss: 0.5355 â€” Time: 0.90s\n","  ğŸ§ª Batch 4/55 â€” Loss: 0.4667 â€” Time: 0.89s\n","  ğŸ§ª Batch 5/55 â€” Loss: 0.6031 â€” Time: 0.89s\n","  ğŸ§ª Batch 6/55 â€” Loss: 0.6132 â€” Time: 0.89s\n","  ğŸ§ª Batch 7/55 â€” Loss: 0.4801 â€” Time: 0.89s\n","  ğŸ§ª Batch 8/55 â€” Loss: 0.4950 â€” Time: 0.88s\n","  ğŸ§ª Batch 9/55 â€” Loss: 0.5375 â€” Time: 0.89s\n","  ğŸ§ª Batch 10/55 â€” Loss: 0.5614 â€” Time: 0.89s\n","  ğŸ§ª Batch 11/55 â€” Loss: 0.5461 â€” Time: 0.89s\n","  ğŸ§ª Batch 12/55 â€” Loss: 0.5039 â€” Time: 0.89s\n","  ğŸ§ª Batch 13/55 â€” Loss: 0.5729 â€” Time: 0.89s\n","  ğŸ§ª Batch 14/55 â€” Loss: 0.4732 â€” Time: 0.89s\n","  ğŸ§ª Batch 15/55 â€” Loss: 0.5268 â€” Time: 0.89s\n","  ğŸ§ª Batch 16/55 â€” Loss: 0.4450 â€” Time: 0.89s\n","  ğŸ§ª Batch 17/55 â€” Loss: 0.5279 â€” Time: 0.89s\n","  ğŸ§ª Batch 18/55 â€” Loss: 0.5990 â€” Time: 0.89s\n","  ğŸ§ª Batch 19/55 â€” Loss: 0.5299 â€” Time: 0.89s\n","  ğŸ§ª Batch 20/55 â€” Loss: 0.6013 â€” Time: 0.89s\n","  ğŸ§ª Batch 21/55 â€” Loss: 0.4755 â€” Time: 0.89s\n","  ğŸ§ª Batch 22/55 â€” Loss: 0.5116 â€” Time: 0.89s\n","  ğŸ§ª Batch 23/55 â€” Loss: 0.5135 â€” Time: 0.89s\n","  ğŸ§ª Batch 24/55 â€” Loss: 0.4581 â€” Time: 0.89s\n","  ğŸ§ª Batch 25/55 â€” Loss: 0.4960 â€” Time: 0.89s\n","  ğŸ§ª Batch 26/55 â€” Loss: 0.5715 â€” Time: 0.89s\n","  ğŸ§ª Batch 27/55 â€” Loss: 0.4927 â€” Time: 0.89s\n","  ğŸ§ª Batch 28/55 â€” Loss: 0.4584 â€” Time: 0.89s\n","  ğŸ§ª Batch 29/55 â€” Loss: 0.4570 â€” Time: 0.89s\n","  ğŸ§ª Batch 30/55 â€” Loss: 0.5422 â€” Time: 0.89s\n","  ğŸ§ª Batch 31/55 â€” Loss: 0.6626 â€” Time: 0.88s\n","  ğŸ§ª Batch 32/55 â€” Loss: 0.5263 â€” Time: 0.88s\n","  ğŸ§ª Batch 33/55 â€” Loss: 0.4172 â€” Time: 0.89s\n","  ğŸ§ª Batch 34/55 â€” Loss: 0.4439 â€” Time: 0.89s\n","  ğŸ§ª Batch 35/55 â€” Loss: 0.4985 â€” Time: 0.89s\n","  ğŸ§ª Batch 36/55 â€” Loss: 0.5458 â€” Time: 0.89s\n","  ğŸ§ª Batch 37/55 â€” Loss: 0.4807 â€” Time: 0.88s\n","  ğŸ§ª Batch 38/55 â€” Loss: 0.4310 â€” Time: 0.89s\n","  ğŸ§ª Batch 39/55 â€” Loss: 0.5200 â€” Time: 0.88s\n","  ğŸ§ª Batch 40/55 â€” Loss: 0.4532 â€” Time: 0.88s\n","  ğŸ§ª Batch 41/55 â€” Loss: 0.4341 â€” Time: 0.88s\n","  ğŸ§ª Batch 42/55 â€” Loss: 0.4146 â€” Time: 0.89s\n","  ğŸ§ª Batch 43/55 â€” Loss: 0.4993 â€” Time: 0.87s\n","  ğŸ§ª Batch 44/55 â€” Loss: 0.3888 â€” Time: 0.89s\n","  ğŸ§ª Batch 45/55 â€” Loss: 0.4065 â€” Time: 0.89s\n","  ğŸ§ª Batch 46/55 â€” Loss: 0.4873 â€” Time: 0.89s\n","  ğŸ§ª Batch 47/55 â€” Loss: 0.4711 â€” Time: 0.87s\n","  ğŸ§ª Batch 48/55 â€” Loss: 0.5391 â€” Time: 0.89s\n","  ğŸ§ª Batch 49/55 â€” Loss: 0.3521 â€” Time: 0.89s\n","  ğŸ§ª Batch 50/55 â€” Loss: 0.4329 â€” Time: 0.88s\n","  ğŸ§ª Batch 51/55 â€” Loss: 0.4775 â€” Time: 0.87s\n","  ğŸ§ª Batch 52/55 â€” Loss: 0.3988 â€” Time: 0.87s\n","  ğŸ§ª Batch 53/55 â€” Loss: 0.4418 â€” Time: 0.89s\n","  ğŸ§ª Batch 54/55 â€” Loss: 0.4267 â€” Time: 0.89s\n","  ğŸ§ª Batch 55/55 â€” Loss: 0.4100 â€” Time: 0.65s\n","ğŸ•’ Epoch Time: 51.91s | Avg Batch Time: 0.88s\n","ğŸ“Š Train Loss: 0.4955 | Val Loss: 0.5081 | IoU: 0.7352 | Dice: 0.8674 | Jaccard: 0.7674 | Precision: 0.8556 | Recall: 0.8817 | Accuracy: 0.9685\n","\n","Epoch 4/100\n","  ğŸ§ª Batch 1/55 â€” Loss: 0.3634 â€” Time: 0.87s\n","  ğŸ§ª Batch 2/55 â€” Loss: 0.4153 â€” Time: 0.88s\n","  ğŸ§ª Batch 3/55 â€” Loss: 0.4217 â€” Time: 0.88s\n","  ğŸ§ª Batch 4/55 â€” Loss: 0.4349 â€” Time: 0.89s\n","  ğŸ§ª Batch 5/55 â€” Loss: 0.4996 â€” Time: 0.89s\n","  ğŸ§ª Batch 6/55 â€” Loss: 0.4861 â€” Time: 0.88s\n","  ğŸ§ª Batch 7/55 â€” Loss: 0.4341 â€” Time: 0.89s\n","  ğŸ§ª Batch 8/55 â€” Loss: 0.4191 â€” Time: 0.88s\n","  ğŸ§ª Batch 9/55 â€” Loss: 0.3747 â€” Time: 0.88s\n","  ğŸ§ª Batch 10/55 â€” Loss: 0.4058 â€” Time: 0.89s\n","  ğŸ§ª Batch 11/55 â€” Loss: 0.3350 â€” Time: 0.87s\n","  ğŸ§ª Batch 12/55 â€” Loss: 0.3966 â€” Time: 0.89s\n","  ğŸ§ª Batch 13/55 â€” Loss: 0.3977 â€” Time: 0.89s\n","  ğŸ§ª Batch 14/55 â€” Loss: 0.3428 â€” Time: 0.89s\n","  ğŸ§ª Batch 15/55 â€” Loss: 0.3692 â€” Time: 0.89s\n","  ğŸ§ª Batch 16/55 â€” Loss: 0.4596 â€” Time: 0.88s\n","  ğŸ§ª Batch 17/55 â€” Loss: 0.4211 â€” Time: 0.89s\n","  ğŸ§ª Batch 18/55 â€” Loss: 0.3873 â€” Time: 0.89s\n","  ğŸ§ª Batch 19/55 â€” Loss: 0.4423 â€” Time: 0.88s\n","  ğŸ§ª Batch 20/55 â€” Loss: 0.3362 â€” Time: 0.88s\n","  ğŸ§ª Batch 21/55 â€” Loss: 0.3824 â€” Time: 0.89s\n","  ğŸ§ª Batch 22/55 â€” Loss: 0.4101 â€” Time: 0.89s\n","  ğŸ§ª Batch 23/55 â€” Loss: 0.4543 â€” Time: 0.89s\n","  ğŸ§ª Batch 24/55 â€” Loss: 0.3566 â€” Time: 0.89s\n","  ğŸ§ª Batch 25/55 â€” Loss: 0.4133 â€” Time: 0.89s\n","  ğŸ§ª Batch 26/55 â€” Loss: 0.3206 â€” Time: 0.89s\n","  ğŸ§ª Batch 27/55 â€” Loss: 0.3503 â€” Time: 0.89s\n","  ğŸ§ª Batch 28/55 â€” Loss: 0.3634 â€” Time: 0.89s\n","  ğŸ§ª Batch 29/55 â€” Loss: 0.4518 â€” Time: 0.89s\n","  ğŸ§ª Batch 30/55 â€” Loss: 0.3271 â€” Time: 0.89s\n","  ğŸ§ª Batch 31/55 â€” Loss: 0.3590 â€” Time: 0.89s\n","  ğŸ§ª Batch 32/55 â€” Loss: 0.3435 â€” Time: 0.89s\n","  ğŸ§ª Batch 33/55 â€” Loss: 0.3064 â€” Time: 0.89s\n","  ğŸ§ª Batch 34/55 â€” Loss: 0.3956 â€” Time: 0.89s\n","  ğŸ§ª Batch 35/55 â€” Loss: 0.3535 â€” Time: 0.89s\n","  ğŸ§ª Batch 36/55 â€” Loss: 0.4143 â€” Time: 0.89s\n","  ğŸ§ª Batch 37/55 â€” Loss: 0.3306 â€” Time: 0.89s\n","  ğŸ§ª Batch 38/55 â€” Loss: 0.3925 â€” Time: 0.89s\n","  ğŸ§ª Batch 39/55 â€” Loss: 0.3564 â€” Time: 0.89s\n","  ğŸ§ª Batch 40/55 â€” Loss: 0.3045 â€” Time: 0.88s\n","  ğŸ§ª Batch 41/55 â€” Loss: 0.3522 â€” Time: 0.88s\n","  ğŸ§ª Batch 42/55 â€” Loss: 0.3362 â€” Time: 0.89s\n","  ğŸ§ª Batch 43/55 â€” Loss: 0.4041 â€” Time: 0.89s\n","  ğŸ§ª Batch 44/55 â€” Loss: 0.3459 â€” Time: 0.89s\n","  ğŸ§ª Batch 45/55 â€” Loss: 0.3392 â€” Time: 0.89s\n","  ğŸ§ª Batch 46/55 â€” Loss: 0.4103 â€” Time: 0.89s\n","  ğŸ§ª Batch 47/55 â€” Loss: 0.3224 â€” Time: 0.89s\n","  ğŸ§ª Batch 48/55 â€” Loss: 0.3581 â€” Time: 0.89s\n","  ğŸ§ª Batch 49/55 â€” Loss: 0.3141 â€” Time: 0.89s\n","  ğŸ§ª Batch 50/55 â€” Loss: 0.3775 â€” Time: 0.89s\n","  ğŸ§ª Batch 51/55 â€” Loss: 0.3195 â€” Time: 0.89s\n","  ğŸ§ª Batch 52/55 â€” Loss: 0.3361 â€” Time: 0.89s\n","  ğŸ§ª Batch 53/55 â€” Loss: 0.3133 â€” Time: 0.89s\n","  ğŸ§ª Batch 54/55 â€” Loss: 0.3149 â€” Time: 0.89s\n","  ğŸ§ª Batch 55/55 â€” Loss: 0.3496 â€” Time: 0.65s\n","ğŸ•’ Epoch Time: 52.24s | Avg Batch Time: 0.88s\n","ğŸ“Š Train Loss: 0.3768 | Val Loss: 0.4424 | IoU: 0.7504 | Dice: 0.8763 | Jaccard: 0.7821 | Precision: 0.8676 | Recall: 0.8875 | Accuracy: 0.9696\n","\n","Epoch 5/100\n","  ğŸ§ª Batch 1/55 â€” Loss: 0.3267 â€” Time: 0.88s\n","  ğŸ§ª Batch 2/55 â€” Loss: 0.3133 â€” Time: 0.88s\n","  ğŸ§ª Batch 3/55 â€” Loss: 0.3154 â€” Time: 0.89s\n","  ğŸ§ª Batch 4/55 â€” Loss: 0.3373 â€” Time: 0.89s\n","  ğŸ§ª Batch 5/55 â€” Loss: 0.2769 â€” Time: 0.89s\n","  ğŸ§ª Batch 6/55 â€” Loss: 0.4590 â€” Time: 0.88s\n","  ğŸ§ª Batch 7/55 â€” Loss: 0.2731 â€” Time: 0.88s\n","  ğŸ§ª Batch 8/55 â€” Loss: 0.3225 â€” Time: 0.89s\n","  ğŸ§ª Batch 9/55 â€” Loss: 0.2858 â€” Time: 0.89s\n","  ğŸ§ª Batch 10/55 â€” Loss: 0.2975 â€” Time: 0.89s\n","  ğŸ§ª Batch 11/55 â€” Loss: 0.3498 â€” Time: 0.88s\n","  ğŸ§ª Batch 12/55 â€” Loss: 0.2878 â€” Time: 0.87s\n","  ğŸ§ª Batch 13/55 â€” Loss: 0.3298 â€” Time: 0.89s\n","  ğŸ§ª Batch 14/55 â€” Loss: 0.2702 â€” Time: 0.89s\n","  ğŸ§ª Batch 15/55 â€” Loss: 0.2645 â€” Time: 0.89s\n","  ğŸ§ª Batch 16/55 â€” Loss: 0.2862 â€” Time: 0.88s\n","  ğŸ§ª Batch 17/55 â€” Loss: 0.3088 â€” Time: 0.88s\n","  ğŸ§ª Batch 18/55 â€” Loss: 0.3493 â€” Time: 0.89s\n","  ğŸ§ª Batch 19/55 â€” Loss: 0.2437 â€” Time: 0.89s\n","  ğŸ§ª Batch 20/55 â€” Loss: 0.3505 â€” Time: 0.89s\n","  ğŸ§ª Batch 21/55 â€” Loss: 0.2924 â€” Time: 0.89s\n","  ğŸ§ª Batch 22/55 â€” Loss: 0.2763 â€” Time: 0.89s\n","  ğŸ§ª Batch 23/55 â€” Loss: 0.4108 â€” Time: 0.88s\n","  ğŸ§ª Batch 24/55 â€” Loss: 0.2824 â€” Time: 0.89s\n","  ğŸ§ª Batch 25/55 â€” Loss: 0.3058 â€” Time: 0.89s\n","  ğŸ§ª Batch 26/55 â€” Loss: 0.3665 â€” Time: 0.89s\n","  ğŸ§ª Batch 27/55 â€” Loss: 0.2859 â€” Time: 0.88s\n","  ğŸ§ª Batch 28/55 â€” Loss: 0.2772 â€” Time: 0.89s\n","  ğŸ§ª Batch 29/55 â€” Loss: 0.3265 â€” Time: 0.89s\n","  ğŸ§ª Batch 30/55 â€” Loss: 0.3474 â€” Time: 0.88s\n","  ğŸ§ª Batch 31/55 â€” Loss: 0.3172 â€” Time: 0.88s\n","  ğŸ§ª Batch 32/55 â€” Loss: 0.3833 â€” Time: 0.88s\n","  ğŸ§ª Batch 33/55 â€” Loss: 0.3145 â€” Time: 0.87s\n","  ğŸ§ª Batch 34/55 â€” Loss: 0.2666 â€” Time: 0.89s\n","  ğŸ§ª Batch 35/55 â€” Loss: 0.3014 â€” Time: 0.89s\n","  ğŸ§ª Batch 36/55 â€” Loss: 0.2599 â€” Time: 0.88s\n","  ğŸ§ª Batch 37/55 â€” Loss: 0.3629 â€” Time: 0.88s\n","  ğŸ§ª Batch 38/55 â€” Loss: 0.2431 â€” Time: 0.89s\n","  ğŸ§ª Batch 39/55 â€” Loss: 0.2676 â€” Time: 0.89s\n","  ğŸ§ª Batch 40/55 â€” Loss: 0.2584 â€” Time: 0.89s\n","  ğŸ§ª Batch 41/55 â€” Loss: 0.2855 â€” Time: 0.89s\n","  ğŸ§ª Batch 42/55 â€” Loss: 0.3029 â€” Time: 0.89s\n","  ğŸ§ª Batch 43/55 â€” Loss: 0.3179 â€” Time: 0.89s\n","  ğŸ§ª Batch 44/55 â€” Loss: 0.4525 â€” Time: 0.87s\n","  ğŸ§ª Batch 45/55 â€” Loss: 0.3311 â€” Time: 0.88s\n","  ğŸ§ª Batch 46/55 â€” Loss: 0.3331 â€” Time: 0.89s\n","  ğŸ§ª Batch 47/55 â€” Loss: 0.2805 â€” Time: 0.88s\n","  ğŸ§ª Batch 48/55 â€” Loss: 0.3330 â€” Time: 0.88s\n","  ğŸ§ª Batch 49/55 â€” Loss: 0.2553 â€” Time: 0.89s\n","  ğŸ§ª Batch 50/55 â€” Loss: 0.2708 â€” Time: 0.89s\n","  ğŸ§ª Batch 51/55 â€” Loss: 0.2673 â€” Time: 0.89s\n","  ğŸ§ª Batch 52/55 â€” Loss: 0.2569 â€” Time: 0.87s\n","  ğŸ§ª Batch 53/55 â€” Loss: 0.3227 â€” Time: 0.89s\n","  ğŸ§ª Batch 54/55 â€” Loss: 0.2951 â€” Time: 0.89s\n","  ğŸ§ª Batch 55/55 â€” Loss: 0.2526 â€” Time: 0.65s\n","ğŸ•’ Epoch Time: 52.65s | Avg Batch Time: 0.88s\n","ğŸ“Š Train Loss: 0.3082 | Val Loss: 0.3991 | IoU: 0.7326 | Dice: 0.8677 | Jaccard: 0.7681 | Precision: 0.8421 | Recall: 0.8960 | Accuracy: 0.9676\n","\n","Epoch 6/100\n","  ğŸ§ª Batch 1/55 â€” Loss: 0.2804 â€” Time: 0.86s\n","  ğŸ§ª Batch 2/55 â€” Loss: 0.2326 â€” Time: 0.89s\n","  ğŸ§ª Batch 3/55 â€” Loss: 0.2575 â€” Time: 0.88s\n","  ğŸ§ª Batch 4/55 â€” Loss: 0.2430 â€” Time: 0.89s\n","  ğŸ§ª Batch 5/55 â€” Loss: 0.2640 â€” Time: 0.88s\n","  ğŸ§ª Batch 6/55 â€” Loss: 0.2328 â€” Time: 0.88s\n","  ğŸ§ª Batch 7/55 â€” Loss: 0.3017 â€” Time: 0.88s\n","  ğŸ§ª Batch 8/55 â€” Loss: 0.2182 â€” Time: 0.88s\n","  ğŸ§ª Batch 9/55 â€” Loss: 0.2874 â€” Time: 0.89s\n","  ğŸ§ª Batch 10/55 â€” Loss: 0.2888 â€” Time: 0.89s\n","  ğŸ§ª Batch 11/55 â€” Loss: 0.2199 â€” Time: 0.88s\n","  ğŸ§ª Batch 12/55 â€” Loss: 0.2434 â€” Time: 0.89s\n","  ğŸ§ª Batch 13/55 â€” Loss: 0.2645 â€” Time: 0.88s\n","  ğŸ§ª Batch 14/55 â€” Loss: 0.2905 â€” Time: 0.88s\n","  ğŸ§ª Batch 15/55 â€” Loss: 0.2668 â€” Time: 0.89s\n","  ğŸ§ª Batch 16/55 â€” Loss: 0.2630 â€” Time: 0.88s\n","  ğŸ§ª Batch 17/55 â€” Loss: 0.2208 â€” Time: 0.89s\n","  ğŸ§ª Batch 18/55 â€” Loss: 0.2337 â€” Time: 0.89s\n","  ğŸ§ª Batch 19/55 â€” Loss: 0.3003 â€” Time: 0.88s\n","  ğŸ§ª Batch 20/55 â€” Loss: 0.2354 â€” Time: 0.89s\n","  ğŸ§ª Batch 21/55 â€” Loss: 0.2313 â€” Time: 0.89s\n","  ğŸ§ª Batch 22/55 â€” Loss: 0.2485 â€” Time: 0.89s\n","  ğŸ§ª Batch 23/55 â€” Loss: 0.2659 â€” Time: 0.89s\n","  ğŸ§ª Batch 24/55 â€” Loss: 0.2558 â€” Time: 0.89s\n","  ğŸ§ª Batch 25/55 â€” Loss: 0.2464 â€” Time: 0.88s\n","  ğŸ§ª Batch 26/55 â€” Loss: 0.2286 â€” Time: 0.88s\n","  ğŸ§ª Batch 27/55 â€” Loss: 0.2232 â€” Time: 0.89s\n","  ğŸ§ª Batch 28/55 â€” Loss: 0.2269 â€” Time: 0.88s\n","  ğŸ§ª Batch 29/55 â€” Loss: 0.2702 â€” Time: 0.89s\n","  ğŸ§ª Batch 30/55 â€” Loss: 0.3002 â€” Time: 0.89s\n","  ğŸ§ª Batch 31/55 â€” Loss: 0.2951 â€” Time: 0.89s\n","  ğŸ§ª Batch 32/55 â€” Loss: 0.2187 â€” Time: 0.89s\n","  ğŸ§ª Batch 33/55 â€” Loss: 0.2159 â€” Time: 0.89s\n","  ğŸ§ª Batch 34/55 â€” Loss: 0.2351 â€” Time: 0.89s\n","  ğŸ§ª Batch 35/55 â€” Loss: 0.2295 â€” Time: 0.89s\n","  ğŸ§ª Batch 36/55 â€” Loss: 0.3610 â€” Time: 0.89s\n","  ğŸ§ª Batch 37/55 â€” Loss: 0.1987 â€” Time: 0.89s\n","  ğŸ§ª Batch 38/55 â€” Loss: 0.3064 â€” Time: 0.89s\n","  ğŸ§ª Batch 39/55 â€” Loss: 0.2492 â€” Time: 0.89s\n","  ğŸ§ª Batch 40/55 â€” Loss: 0.2556 â€” Time: 0.89s\n","  ğŸ§ª Batch 41/55 â€” Loss: 0.2824 â€” Time: 0.90s\n","  ğŸ§ª Batch 42/55 â€” Loss: 0.2393 â€” Time: 0.88s\n","  ğŸ§ª Batch 43/55 â€” Loss: 0.2226 â€” Time: 0.89s\n","  ğŸ§ª Batch 44/55 â€” Loss: 0.2721 â€” Time: 0.89s\n","  ğŸ§ª Batch 45/55 â€” Loss: 0.2306 â€” Time: 0.89s\n","  ğŸ§ª Batch 46/55 â€” Loss: 0.2894 â€” Time: 0.89s\n","  ğŸ§ª Batch 47/55 â€” Loss: 0.2696 â€” Time: 0.89s\n","  ğŸ§ª Batch 48/55 â€” Loss: 0.2497 â€” Time: 0.89s\n","  ğŸ§ª Batch 49/55 â€” Loss: 0.2736 â€” Time: 0.89s\n","  ğŸ§ª Batch 50/55 â€” Loss: 0.2587 â€” Time: 0.89s\n","  ğŸ§ª Batch 51/55 â€” Loss: 0.2343 â€” Time: 0.89s\n","  ğŸ§ª Batch 52/55 â€” Loss: 0.2260 â€” Time: 0.89s\n","  ğŸ§ª Batch 53/55 â€” Loss: 0.2508 â€” Time: 0.89s\n","  ğŸ§ª Batch 54/55 â€” Loss: 0.3162 â€” Time: 0.89s\n","  ğŸ§ª Batch 55/55 â€” Loss: 0.2124 â€” Time: 0.65s\n","ğŸ•’ Epoch Time: 53.16s | Avg Batch Time: 0.88s\n","ğŸ“Š Train Loss: 0.2552 | Val Loss: 0.3693 | IoU: 0.7593 | Dice: 0.8746 | Jaccard: 0.7807 | Precision: 0.9054 | Recall: 0.8479 | Accuracy: 0.9703\n","\n","Epoch 7/100\n","  ğŸ§ª Batch 1/55 â€” Loss: 0.2173 â€” Time: 0.87s\n","  ğŸ§ª Batch 2/55 â€” Loss: 0.2207 â€” Time: 0.89s\n","  ğŸ§ª Batch 3/55 â€” Loss: 0.2338 â€” Time: 0.89s\n","  ğŸ§ª Batch 4/55 â€” Loss: 0.2117 â€” Time: 0.89s\n","  ğŸ§ª Batch 5/55 â€” Loss: 0.1798 â€” Time: 0.89s\n","  ğŸ§ª Batch 6/55 â€” Loss: 0.2792 â€” Time: 0.88s\n","  ğŸ§ª Batch 7/55 â€” Loss: 0.2235 â€” Time: 0.89s\n","  ğŸ§ª Batch 8/55 â€” Loss: 0.2510 â€” Time: 0.89s\n","  ğŸ§ª Batch 9/55 â€” Loss: 0.2602 â€” Time: 0.89s\n","  ğŸ§ª Batch 10/55 â€” Loss: 0.2767 â€” Time: 0.87s\n","  ğŸ§ª Batch 11/55 â€” Loss: 0.2106 â€” Time: 0.88s\n","  ğŸ§ª Batch 12/55 â€” Loss: 0.2153 â€” Time: 0.89s\n","  ğŸ§ª Batch 13/55 â€” Loss: 0.2292 â€” Time: 0.88s\n","  ğŸ§ª Batch 14/55 â€” Loss: 0.2034 â€” Time: 0.87s\n","  ğŸ§ª Batch 15/55 â€” Loss: 0.2241 â€” Time: 0.89s\n","  ğŸ§ª Batch 16/55 â€” Loss: 0.2211 â€” Time: 0.89s\n","  ğŸ§ª Batch 17/55 â€” Loss: 0.1736 â€” Time: 0.88s\n","  ğŸ§ª Batch 18/55 â€” Loss: 0.2168 â€” Time: 0.88s\n","  ğŸ§ª Batch 19/55 â€” Loss: 0.2140 â€” Time: 0.87s\n","  ğŸ§ª Batch 20/55 â€” Loss: 0.2082 â€” Time: 0.89s\n","  ğŸ§ª Batch 21/55 â€” Loss: 0.1737 â€” Time: 0.89s\n","  ğŸ§ª Batch 22/55 â€” Loss: 0.2279 â€” Time: 0.89s\n","  ğŸ§ª Batch 23/55 â€” Loss: 0.2203 â€” Time: 0.88s\n","  ğŸ§ª Batch 24/55 â€” Loss: 0.1859 â€” Time: 0.88s\n","  ğŸ§ª Batch 25/55 â€” Loss: 0.2606 â€” Time: 0.88s\n","  ğŸ§ª Batch 26/55 â€” Loss: 0.2012 â€” Time: 0.88s\n","  ğŸ§ª Batch 27/55 â€” Loss: 0.2441 â€” Time: 0.89s\n","  ğŸ§ª Batch 28/55 â€” Loss: 0.2156 â€” Time: 0.88s\n","  ğŸ§ª Batch 29/55 â€” Loss: 0.2066 â€” Time: 0.89s\n","  ğŸ§ª Batch 30/55 â€” Loss: 0.2320 â€” Time: 0.88s\n","  ğŸ§ª Batch 31/55 â€” Loss: 0.1929 â€” Time: 0.88s\n","  ğŸ§ª Batch 32/55 â€” Loss: 0.2097 â€” Time: 0.88s\n","  ğŸ§ª Batch 33/55 â€” Loss: 0.2022 â€” Time: 0.89s\n","  ğŸ§ª Batch 34/55 â€” Loss: 0.2125 â€” Time: 0.88s\n","  ğŸ§ª Batch 35/55 â€” Loss: 0.1988 â€” Time: 0.88s\n","  ğŸ§ª Batch 36/55 â€” Loss: 0.1811 â€” Time: 0.88s\n","  ğŸ§ª Batch 37/55 â€” Loss: 0.2490 â€” Time: 0.88s\n","  ğŸ§ª Batch 38/55 â€” Loss: 0.2606 â€” Time: 0.88s\n","  ğŸ§ª Batch 39/55 â€” Loss: 0.2898 â€” Time: 0.88s\n","  ğŸ§ª Batch 40/55 â€” Loss: 0.1978 â€” Time: 0.89s\n","  ğŸ§ª Batch 41/55 â€” Loss: 0.2240 â€” Time: 0.89s\n","  ğŸ§ª Batch 42/55 â€” Loss: 0.1980 â€” Time: 0.88s\n","  ğŸ§ª Batch 43/55 â€” Loss: 0.1820 â€” Time: 0.88s\n","  ğŸ§ª Batch 44/55 â€” Loss: 0.2200 â€” Time: 0.88s\n","  ğŸ§ª Batch 45/55 â€” Loss: 0.1980 â€” Time: 0.88s\n","  ğŸ§ª Batch 46/55 â€” Loss: 0.1823 â€” Time: 0.89s\n","  ğŸ§ª Batch 47/55 â€” Loss: 0.1810 â€” Time: 0.89s\n","  ğŸ§ª Batch 48/55 â€” Loss: 0.2013 â€” Time: 0.88s\n","  ğŸ§ª Batch 49/55 â€” Loss: 0.2096 â€” Time: 0.88s\n","  ğŸ§ª Batch 50/55 â€” Loss: 0.1833 â€” Time: 0.88s\n","  ğŸ§ª Batch 51/55 â€” Loss: 0.1802 â€” Time: 0.88s\n","  ğŸ§ª Batch 52/55 â€” Loss: 0.1837 â€” Time: 0.88s\n","  ğŸ§ª Batch 53/55 â€” Loss: 0.2225 â€” Time: 0.88s\n","  ğŸ§ª Batch 54/55 â€” Loss: 0.1806 â€” Time: 0.88s\n","  ğŸ§ª Batch 55/55 â€” Loss: 0.1648 â€” Time: 0.65s\n","ğŸ•’ Epoch Time: 52.35s | Avg Batch Time: 0.88s\n","ğŸ“Š Train Loss: 0.2135 | Val Loss: 0.3095 | IoU: 0.7646 | Dice: 0.8949 | Jaccard: 0.8115 | Precision: 0.9122 | Recall: 0.8796 | Accuracy: 0.9748\n","\n","Epoch 8/100\n","  ğŸ§ª Batch 1/55 â€” Loss: 0.1613 â€” Time: 0.87s\n","  ğŸ§ª Batch 2/55 â€” Loss: 0.1921 â€” Time: 0.87s\n","  ğŸ§ª Batch 3/55 â€” Loss: 0.2489 â€” Time: 0.89s\n","  ğŸ§ª Batch 4/55 â€” Loss: 0.1602 â€” Time: 0.89s\n","  ğŸ§ª Batch 5/55 â€” Loss: 0.1666 â€” Time: 0.87s\n","  ğŸ§ª Batch 6/55 â€” Loss: 0.2384 â€” Time: 0.89s\n","  ğŸ§ª Batch 7/55 â€” Loss: 0.1710 â€” Time: 0.88s\n","  ğŸ§ª Batch 8/55 â€” Loss: 0.1671 â€” Time: 0.89s\n","  ğŸ§ª Batch 9/55 â€” Loss: 0.2851 â€” Time: 0.89s\n","  ğŸ§ª Batch 10/55 â€” Loss: 0.1652 â€” Time: 0.89s\n","  ğŸ§ª Batch 11/55 â€” Loss: 0.1697 â€” Time: 0.88s\n","  ğŸ§ª Batch 12/55 â€” Loss: 0.1979 â€” Time: 0.89s\n","  ğŸ§ª Batch 13/55 â€” Loss: 0.1802 â€” Time: 0.89s\n","  ğŸ§ª Batch 14/55 â€” Loss: 0.1702 â€” Time: 0.88s\n","  ğŸ§ª Batch 15/55 â€” Loss: 0.1661 â€” Time: 0.89s\n","  ğŸ§ª Batch 16/55 â€” Loss: 0.1797 â€” Time: 0.88s\n","  ğŸ§ª Batch 17/55 â€” Loss: 0.1585 â€” Time: 0.88s\n","  ğŸ§ª Batch 18/55 â€” Loss: 0.1481 â€” Time: 0.89s\n","  ğŸ§ª Batch 19/55 â€” Loss: 0.2163 â€” Time: 0.89s\n","  ğŸ§ª Batch 20/55 â€” Loss: 0.1405 â€” Time: 0.89s\n","  ğŸ§ª Batch 21/55 â€” Loss: 0.1681 â€” Time: 0.89s\n","  ğŸ§ª Batch 22/55 â€” Loss: 0.1957 â€” Time: 0.88s\n","  ğŸ§ª Batch 23/55 â€” Loss: 0.1696 â€” Time: 0.89s\n","  ğŸ§ª Batch 24/55 â€” Loss: 0.1843 â€” Time: 0.89s\n","  ğŸ§ª Batch 25/55 â€” Loss: 0.1590 â€” Time: 0.89s\n","  ğŸ§ª Batch 26/55 â€” Loss: 0.1726 â€” Time: 0.89s\n","  ğŸ§ª Batch 27/55 â€” Loss: 0.2257 â€” Time: 0.88s\n","  ğŸ§ª Batch 28/55 â€” Loss: 0.1600 â€” Time: 0.89s\n","  ğŸ§ª Batch 29/55 â€” Loss: 0.1872 â€” Time: 0.88s\n","  ğŸ§ª Batch 30/55 â€” Loss: 0.1716 â€” Time: 0.89s\n","  ğŸ§ª Batch 31/55 â€” Loss: 0.1926 â€” Time: 0.89s\n","  ğŸ§ª Batch 32/55 â€” Loss: 0.1881 â€” Time: 0.89s\n","  ğŸ§ª Batch 33/55 â€” Loss: 0.1607 â€” Time: 0.87s\n","  ğŸ§ª Batch 34/55 â€” Loss: 0.2427 â€” Time: 0.88s\n","  ğŸ§ª Batch 35/55 â€” Loss: 0.1657 â€” Time: 0.88s\n","  ğŸ§ª Batch 36/55 â€” Loss: 0.1770 â€” Time: 0.89s\n","  ğŸ§ª Batch 37/55 â€” Loss: 0.1763 â€” Time: 0.89s\n","  ğŸ§ª Batch 38/55 â€” Loss: 0.1752 â€” Time: 0.88s\n","  ğŸ§ª Batch 39/55 â€” Loss: 0.1810 â€” Time: 0.89s\n","  ğŸ§ª Batch 40/55 â€” Loss: 0.2054 â€” Time: 0.89s\n","  ğŸ§ª Batch 41/55 â€” Loss: 0.2151 â€” Time: 0.89s\n","  ğŸ§ª Batch 42/55 â€” Loss: 0.1897 â€” Time: 0.88s\n","  ğŸ§ª Batch 43/55 â€” Loss: 0.1414 â€” Time: 0.88s\n","  ğŸ§ª Batch 44/55 â€” Loss: 0.2342 â€” Time: 0.89s\n","  ğŸ§ª Batch 45/55 â€” Loss: 0.1919 â€” Time: 0.89s\n","  ğŸ§ª Batch 46/55 â€” Loss: 0.2495 â€” Time: 0.89s\n","  ğŸ§ª Batch 47/55 â€” Loss: 0.1684 â€” Time: 0.89s\n","  ğŸ§ª Batch 48/55 â€” Loss: 0.1782 â€” Time: 0.89s\n","  ğŸ§ª Batch 49/55 â€” Loss: 0.1693 â€” Time: 0.89s\n","  ğŸ§ª Batch 50/55 â€” Loss: 0.1824 â€” Time: 0.88s\n","  ğŸ§ª Batch 51/55 â€” Loss: 0.1640 â€” Time: 0.88s\n","  ğŸ§ª Batch 52/55 â€” Loss: 0.2044 â€” Time: 0.89s\n","  ğŸ§ª Batch 53/55 â€” Loss: 0.1483 â€” Time: 0.89s\n","  ğŸ§ª Batch 54/55 â€” Loss: 0.2114 â€” Time: 0.89s\n","  ğŸ§ª Batch 55/55 â€” Loss: 0.1610 â€” Time: 0.65s\n","ğŸ•’ Epoch Time: 52.21s | Avg Batch Time: 0.88s\n","ğŸ“Š Train Loss: 0.1846 | Val Loss: 0.3167 | IoU: 0.7610 | Dice: 0.8782 | Jaccard: 0.7856 | Precision: 0.8594 | Recall: 0.9002 | Accuracy: 0.9696\n","\n","Epoch 9/100\n","  ğŸ§ª Batch 1/55 â€” Loss: 0.1581 â€” Time: 0.89s\n","  ğŸ§ª Batch 2/55 â€” Loss: 0.1468 â€” Time: 0.88s\n","  ğŸ§ª Batch 3/55 â€” Loss: 0.2539 â€” Time: 0.89s\n","  ğŸ§ª Batch 4/55 â€” Loss: 0.1578 â€” Time: 0.89s\n","  ğŸ§ª Batch 5/55 â€” Loss: 0.1890 â€” Time: 0.89s\n","  ğŸ§ª Batch 6/55 â€” Loss: 0.1715 â€” Time: 0.89s\n","  ğŸ§ª Batch 7/55 â€” Loss: 0.2030 â€” Time: 0.89s\n","  ğŸ§ª Batch 8/55 â€” Loss: 0.1515 â€” Time: 0.89s\n","  ğŸ§ª Batch 9/55 â€” Loss: 0.1620 â€” Time: 0.89s\n","  ğŸ§ª Batch 10/55 â€” Loss: 0.1617 â€” Time: 0.89s\n","  ğŸ§ª Batch 11/55 â€” Loss: 0.1574 â€” Time: 0.89s\n","  ğŸ§ª Batch 12/55 â€” Loss: 0.1655 â€” Time: 0.89s\n","  ğŸ§ª Batch 13/55 â€” Loss: 0.1801 â€” Time: 0.89s\n","  ğŸ§ª Batch 14/55 â€” Loss: 0.1829 â€” Time: 0.89s\n","  ğŸ§ª Batch 15/55 â€” Loss: 0.1868 â€” Time: 0.89s\n","  ğŸ§ª Batch 16/55 â€” Loss: 0.1486 â€” Time: 0.89s\n","  ğŸ§ª Batch 17/55 â€” Loss: 0.2383 â€” Time: 0.89s\n","  ğŸ§ª Batch 18/55 â€” Loss: 0.1929 â€” Time: 0.89s\n","  ğŸ§ª Batch 19/55 â€” Loss: 0.1446 â€” Time: 0.89s\n","  ğŸ§ª Batch 20/55 â€” Loss: 0.1684 â€” Time: 0.89s\n","  ğŸ§ª Batch 21/55 â€” Loss: 0.1404 â€” Time: 0.89s\n","  ğŸ§ª Batch 22/55 â€” Loss: 0.1779 â€” Time: 0.89s\n","  ğŸ§ª Batch 23/55 â€” Loss: 0.2399 â€” Time: 0.89s\n","  ğŸ§ª Batch 24/55 â€” Loss: 0.1680 â€” Time: 0.89s\n","  ğŸ§ª Batch 25/55 â€” Loss: 0.1651 â€” Time: 0.89s\n","  ğŸ§ª Batch 26/55 â€” Loss: 0.1990 â€” Time: 0.89s\n","  ğŸ§ª Batch 27/55 â€” Loss: 0.1695 â€” Time: 0.89s\n","  ğŸ§ª Batch 28/55 â€” Loss: 0.1617 â€” Time: 0.89s\n","  ğŸ§ª Batch 29/55 â€” Loss: 0.1681 â€” Time: 0.89s\n","  ğŸ§ª Batch 30/55 â€” Loss: 0.1688 â€” Time: 0.89s\n","  ğŸ§ª Batch 31/55 â€” Loss: 0.1633 â€” Time: 0.89s\n","  ğŸ§ª Batch 32/55 â€” Loss: 0.1566 â€” Time: 0.89s\n","  ğŸ§ª Batch 33/55 â€” Loss: 0.1528 â€” Time: 0.88s\n","  ğŸ§ª Batch 34/55 â€” Loss: 0.1629 â€” Time: 0.89s\n","  ğŸ§ª Batch 35/55 â€” Loss: 0.1863 â€” Time: 0.89s\n","  ğŸ§ª Batch 36/55 â€” Loss: 0.1489 â€” Time: 0.89s\n","  ğŸ§ª Batch 37/55 â€” Loss: 0.1642 â€” Time: 0.89s\n","  ğŸ§ª Batch 38/55 â€” Loss: 0.1376 â€” Time: 0.89s\n","  ğŸ§ª Batch 39/55 â€” Loss: 0.1700 â€” Time: 0.89s\n","  ğŸ§ª Batch 40/55 â€” Loss: 0.1630 â€” Time: 0.89s\n","  ğŸ§ª Batch 41/55 â€” Loss: 0.1502 â€” Time: 0.89s\n","  ğŸ§ª Batch 42/55 â€” Loss: 0.1422 â€” Time: 0.89s\n","  ğŸ§ª Batch 43/55 â€” Loss: 0.1296 â€” Time: 0.89s\n","  ğŸ§ª Batch 44/55 â€” Loss: 0.1585 â€” Time: 0.89s\n","  ğŸ§ª Batch 45/55 â€” Loss: 0.1608 â€” Time: 0.89s\n","  ğŸ§ª Batch 46/55 â€” Loss: 0.1737 â€” Time: 0.90s\n","  ğŸ§ª Batch 47/55 â€” Loss: 0.1498 â€” Time: 0.88s\n","  ğŸ§ª Batch 48/55 â€” Loss: 0.1326 â€” Time: 0.89s\n","  ğŸ§ª Batch 49/55 â€” Loss: 0.1314 â€” Time: 0.89s\n","  ğŸ§ª Batch 50/55 â€” Loss: 0.1529 â€” Time: 0.89s\n","  ğŸ§ª Batch 51/55 â€” Loss: 0.1437 â€” Time: 0.89s\n","  ğŸ§ª Batch 52/55 â€” Loss: 0.2005 â€” Time: 0.88s\n","  ğŸ§ª Batch 53/55 â€” Loss: 0.1328 â€” Time: 0.89s\n","  ğŸ§ª Batch 54/55 â€” Loss: 0.1795 â€” Time: 0.89s\n","  ğŸ§ª Batch 55/55 â€” Loss: 0.1614 â€” Time: 0.65s\n","ğŸ•’ Epoch Time: 52.48s | Avg Batch Time: 0.88s\n","ğŸ“Š Train Loss: 0.1670 | Val Loss: 0.2764 | IoU: 0.7866 | Dice: 0.8941 | Jaccard: 0.8110 | Precision: 0.9324 | Recall: 0.8615 | Accuracy: 0.9754\n","\n","Epoch 10/100\n","  ğŸ§ª Batch 1/55 â€” Loss: 0.1525 â€” Time: 0.87s\n","  ğŸ§ª Batch 2/55 â€” Loss: 0.1243 â€” Time: 0.88s\n","  ğŸ§ª Batch 3/55 â€” Loss: 0.1651 â€” Time: 0.89s\n","  ğŸ§ª Batch 4/55 â€” Loss: 0.1387 â€” Time: 0.89s\n","  ğŸ§ª Batch 5/55 â€” Loss: 0.1401 â€” Time: 0.89s\n","  ğŸ§ª Batch 6/55 â€” Loss: 0.1782 â€” Time: 0.88s\n","  ğŸ§ª Batch 7/55 â€” Loss: 0.1348 â€” Time: 0.89s\n","  ğŸ§ª Batch 8/55 â€” Loss: 0.1759 â€” Time: 0.88s\n","  ğŸ§ª Batch 9/55 â€” Loss: 0.1330 â€” Time: 0.89s\n","  ğŸ§ª Batch 10/55 â€” Loss: 0.1913 â€” Time: 0.89s\n","  ğŸ§ª Batch 11/55 â€” Loss: 0.1282 â€” Time: 0.89s\n","  ğŸ§ª Batch 12/55 â€” Loss: 0.1372 â€” Time: 0.88s\n","  ğŸ§ª Batch 13/55 â€” Loss: 0.1574 â€” Time: 0.89s\n","  ğŸ§ª Batch 14/55 â€” Loss: 0.1281 â€” Time: 0.89s\n","  ğŸ§ª Batch 15/55 â€” Loss: 0.1392 â€” Time: 0.89s\n","  ğŸ§ª Batch 16/55 â€” Loss: 0.1253 â€” Time: 0.89s\n","  ğŸ§ª Batch 17/55 â€” Loss: 0.1378 â€” Time: 0.89s\n","  ğŸ§ª Batch 18/55 â€” Loss: 0.1789 â€” Time: 0.89s\n","  ğŸ§ª Batch 19/55 â€” Loss: 0.1891 â€” Time: 0.88s\n","  ğŸ§ª Batch 20/55 â€” Loss: 0.1759 â€” Time: 0.89s\n","  ğŸ§ª Batch 21/55 â€” Loss: 0.1494 â€” Time: 0.88s\n","  ğŸ§ª Batch 22/55 â€” Loss: 0.1332 â€” Time: 0.89s\n","  ğŸ§ª Batch 23/55 â€” Loss: 0.1253 â€” Time: 0.89s\n","  ğŸ§ª Batch 24/55 â€” Loss: 0.1392 â€” Time: 0.89s\n","  ğŸ§ª Batch 25/55 â€” Loss: 0.1239 â€” Time: 0.89s\n","  ğŸ§ª Batch 26/55 â€” Loss: 0.1716 â€” Time: 0.88s\n","  ğŸ§ª Batch 27/55 â€” Loss: 0.1273 â€” Time: 0.88s\n","  ğŸ§ª Batch 28/55 â€” Loss: 0.1862 â€” Time: 0.89s\n","  ğŸ§ª Batch 29/55 â€” Loss: 0.1615 â€” Time: 0.88s\n","  ğŸ§ª Batch 30/55 â€” Loss: 0.1420 â€” Time: 0.88s\n","  ğŸ§ª Batch 31/55 â€” Loss: 0.1166 â€” Time: 0.89s\n","  ğŸ§ª Batch 32/55 â€” Loss: 0.1315 â€” Time: 0.89s\n","  ğŸ§ª Batch 33/55 â€” Loss: 0.1104 â€” Time: 0.89s\n","  ğŸ§ª Batch 34/55 â€” Loss: 0.1154 â€” Time: 0.89s\n","  ğŸ§ª Batch 35/55 â€” Loss: 0.1987 â€” Time: 0.89s\n","  ğŸ§ª Batch 36/55 â€” Loss: 0.1287 â€” Time: 0.89s\n","  ğŸ§ª Batch 37/55 â€” Loss: 0.1138 â€” Time: 0.88s\n","  ğŸ§ª Batch 38/55 â€” Loss: 0.1226 â€” Time: 0.88s\n","  ğŸ§ª Batch 39/55 â€” Loss: 0.1131 â€” Time: 0.89s\n","  ğŸ§ª Batch 40/55 â€” Loss: 0.1720 â€” Time: 0.89s\n","  ğŸ§ª Batch 41/55 â€” Loss: 0.1230 â€” Time: 0.89s\n","  ğŸ§ª Batch 42/55 â€” Loss: 0.2333 â€” Time: 0.89s\n","  ğŸ§ª Batch 43/55 â€” Loss: 0.1280 â€” Time: 0.89s\n","  ğŸ§ª Batch 44/55 â€” Loss: 0.1060 â€” Time: 0.89s\n","  ğŸ§ª Batch 45/55 â€” Loss: 0.1315 â€” Time: 0.89s\n","  ğŸ§ª Batch 46/55 â€” Loss: 0.1430 â€” Time: 0.89s\n","  ğŸ§ª Batch 47/55 â€” Loss: 0.1286 â€” Time: 0.89s\n","  ğŸ§ª Batch 48/55 â€” Loss: 0.1609 â€” Time: 0.90s\n","  ğŸ§ª Batch 49/55 â€” Loss: 0.1292 â€” Time: 0.89s\n","  ğŸ§ª Batch 50/55 â€” Loss: 0.1226 â€” Time: 0.89s\n","  ğŸ§ª Batch 51/55 â€” Loss: 0.1242 â€” Time: 0.89s\n","  ğŸ§ª Batch 52/55 â€” Loss: 0.1298 â€” Time: 0.89s\n","  ğŸ§ª Batch 53/55 â€” Loss: 0.1256 â€” Time: 0.89s\n","  ğŸ§ª Batch 54/55 â€” Loss: 0.1204 â€” Time: 0.89s\n","  ğŸ§ª Batch 55/55 â€” Loss: 0.1365 â€” Time: 0.65s\n","ğŸ•’ Epoch Time: 51.97s | Avg Batch Time: 0.88s\n","ğŸ“Š Train Loss: 0.1428 | Val Loss: 0.2690 | IoU: 0.7906 | Dice: 0.8923 | Jaccard: 0.8086 | Precision: 0.9359 | Recall: 0.8559 | Accuracy: 0.9751\n","\n","Epoch 11/100\n","  ğŸ§ª Batch 1/55 â€” Loss: 0.1905 â€” Time: 0.87s\n","  ğŸ§ª Batch 2/55 â€” Loss: 0.1153 â€” Time: 0.88s\n","  ğŸ§ª Batch 3/55 â€” Loss: 0.1364 â€” Time: 0.89s\n","  ğŸ§ª Batch 4/55 â€” Loss: 0.1269 â€” Time: 0.88s\n","  ğŸ§ª Batch 5/55 â€” Loss: 0.1145 â€” Time: 0.89s\n","  ğŸ§ª Batch 6/55 â€” Loss: 0.1248 â€” Time: 0.90s\n","  ğŸ§ª Batch 7/55 â€” Loss: 0.1301 â€” Time: 0.89s\n","  ğŸ§ª Batch 8/55 â€” Loss: 0.1057 â€” Time: 0.89s\n","  ğŸ§ª Batch 9/55 â€” Loss: 0.1018 â€” Time: 0.89s\n","  ğŸ§ª Batch 10/55 â€” Loss: 0.1174 â€” Time: 0.89s\n","  ğŸ§ª Batch 11/55 â€” Loss: 0.1275 â€” Time: 0.88s\n","  ğŸ§ª Batch 12/55 â€” Loss: 0.1506 â€” Time: 0.88s\n","  ğŸ§ª Batch 13/55 â€” Loss: 0.1155 â€” Time: 0.89s\n","  ğŸ§ª Batch 14/55 â€” Loss: 0.1111 â€” Time: 0.89s\n","  ğŸ§ª Batch 15/55 â€” Loss: 0.1239 â€” Time: 0.88s\n","  ğŸ§ª Batch 16/55 â€” Loss: 0.1310 â€” Time: 0.89s\n","  ğŸ§ª Batch 17/55 â€” Loss: 0.1412 â€” Time: 0.89s\n","  ğŸ§ª Batch 18/55 â€” Loss: 0.1140 â€” Time: 0.89s\n","  ğŸ§ª Batch 19/55 â€” Loss: 0.1294 â€” Time: 0.89s\n","  ğŸ§ª Batch 20/55 â€” Loss: 0.1191 â€” Time: 0.89s\n","  ğŸ§ª Batch 21/55 â€” Loss: 0.1146 â€” Time: 0.89s\n","  ğŸ§ª Batch 22/55 â€” Loss: 0.1028 â€” Time: 0.89s\n","  ğŸ§ª Batch 23/55 â€” Loss: 0.1256 â€” Time: 0.89s\n","  ğŸ§ª Batch 24/55 â€” Loss: 0.2256 â€” Time: 0.89s\n","  ğŸ§ª Batch 25/55 â€” Loss: 0.1036 â€” Time: 0.88s\n","  ğŸ§ª Batch 26/55 â€” Loss: 0.1369 â€” Time: 0.89s\n","  ğŸ§ª Batch 27/55 â€” Loss: 0.1030 â€” Time: 0.89s\n","  ğŸ§ª Batch 28/55 â€” Loss: 0.1212 â€” Time: 0.89s\n","  ğŸ§ª Batch 29/55 â€” Loss: 0.1175 â€” Time: 0.89s\n","  ğŸ§ª Batch 30/55 â€” Loss: 0.1150 â€” Time: 0.89s\n","  ğŸ§ª Batch 31/55 â€” Loss: 0.1574 â€” Time: 0.89s\n","  ğŸ§ª Batch 32/55 â€” Loss: 0.1322 â€” Time: 0.89s\n","  ğŸ§ª Batch 33/55 â€” Loss: 0.1071 â€” Time: 0.89s\n","  ğŸ§ª Batch 34/55 â€” Loss: 0.1220 â€” Time: 0.89s\n","  ğŸ§ª Batch 35/55 â€” Loss: 0.1790 â€” Time: 0.89s\n","  ğŸ§ª Batch 36/55 â€” Loss: 0.1158 â€” Time: 0.88s\n","  ğŸ§ª Batch 37/55 â€” Loss: 0.1088 â€” Time: 0.89s\n","  ğŸ§ª Batch 38/55 â€” Loss: 0.1181 â€” Time: 0.89s\n","  ğŸ§ª Batch 39/55 â€” Loss: 0.1071 â€” Time: 0.89s\n","  ğŸ§ª Batch 40/55 â€” Loss: 0.1141 â€” Time: 0.89s\n","  ğŸ§ª Batch 41/55 â€” Loss: 0.1218 â€” Time: 0.89s\n","  ğŸ§ª Batch 42/55 â€” Loss: 0.1186 â€” Time: 0.89s\n","  ğŸ§ª Batch 43/55 â€” Loss: 0.1166 â€” Time: 0.89s\n","  ğŸ§ª Batch 44/55 â€” Loss: 0.1482 â€” Time: 0.89s\n","  ğŸ§ª Batch 45/55 â€” Loss: 0.1119 â€” Time: 0.89s\n","  ğŸ§ª Batch 46/55 â€” Loss: 0.1341 â€” Time: 0.89s\n","  ğŸ§ª Batch 47/55 â€” Loss: 0.0981 â€” Time: 0.89s\n","  ğŸ§ª Batch 48/55 â€” Loss: 0.2138 â€” Time: 0.89s\n","  ğŸ§ª Batch 49/55 â€” Loss: 0.1191 â€” Time: 0.89s\n","  ğŸ§ª Batch 50/55 â€” Loss: 0.1336 â€” Time: 0.89s\n","  ğŸ§ª Batch 51/55 â€” Loss: 0.1184 â€” Time: 0.89s\n","  ğŸ§ª Batch 52/55 â€” Loss: 0.1244 â€” Time: 0.89s\n","  ğŸ§ª Batch 53/55 â€” Loss: 0.1282 â€” Time: 0.89s\n","  ğŸ§ª Batch 54/55 â€” Loss: 0.1314 â€” Time: 0.89s\n","  ğŸ§ª Batch 55/55 â€” Loss: 0.1533 â€” Time: 0.65s\n","ğŸ•’ Epoch Time: 51.96s | Avg Batch Time: 0.88s\n","ğŸ“Š Train Loss: 0.1277 | Val Loss: 0.3033 | IoU: 0.7694 | Dice: 0.8720 | Jaccard: 0.7775 | Precision: 0.9217 | Recall: 0.8341 | Accuracy: 0.9707\n","\n","Epoch 12/100\n","  ğŸ§ª Batch 1/55 â€” Loss: 0.1096 â€” Time: 0.87s\n","  ğŸ§ª Batch 2/55 â€” Loss: 0.1335 â€” Time: 0.89s\n","  ğŸ§ª Batch 3/55 â€” Loss: 0.0995 â€” Time: 0.89s\n","  ğŸ§ª Batch 4/55 â€” Loss: 0.1528 â€” Time: 0.90s\n","  ğŸ§ª Batch 5/55 â€” Loss: 0.1125 â€” Time: 0.87s\n","  ğŸ§ª Batch 6/55 â€” Loss: 0.1202 â€” Time: 0.89s\n","  ğŸ§ª Batch 7/55 â€” Loss: 0.1386 â€” Time: 0.89s\n","  ğŸ§ª Batch 8/55 â€” Loss: 0.1025 â€” Time: 0.89s\n","  ğŸ§ª Batch 9/55 â€” Loss: 0.2838 â€” Time: 0.88s\n","  ğŸ§ª Batch 10/55 â€” Loss: 0.1012 â€” Time: 0.88s\n","  ğŸ§ª Batch 11/55 â€” Loss: 0.1290 â€” Time: 0.89s\n","  ğŸ§ª Batch 12/55 â€” Loss: 0.1209 â€” Time: 0.89s\n","  ğŸ§ª Batch 13/55 â€” Loss: 0.1083 â€” Time: 0.89s\n","  ğŸ§ª Batch 14/55 â€” Loss: 0.1039 â€” Time: 0.88s\n","  ğŸ§ª Batch 15/55 â€” Loss: 0.1220 â€” Time: 0.88s\n","  ğŸ§ª Batch 16/55 â€” Loss: 0.1382 â€” Time: 0.89s\n","  ğŸ§ª Batch 17/55 â€” Loss: 0.1158 â€” Time: 0.87s\n","  ğŸ§ª Batch 18/55 â€” Loss: 0.1067 â€” Time: 0.89s\n","  ğŸ§ª Batch 19/55 â€” Loss: 0.1243 â€” Time: 0.89s\n","  ğŸ§ª Batch 20/55 â€” Loss: 0.1302 â€” Time: 0.88s\n","  ğŸ§ª Batch 21/55 â€” Loss: 0.1481 â€” Time: 0.88s\n","  ğŸ§ª Batch 22/55 â€” Loss: 0.1033 â€” Time: 0.88s\n","  ğŸ§ª Batch 23/55 â€” Loss: 0.0981 â€” Time: 0.88s\n","  ğŸ§ª Batch 24/55 â€” Loss: 0.1032 â€” Time: 0.88s\n","  ğŸ§ª Batch 25/55 â€” Loss: 0.1019 â€” Time: 0.89s\n","  ğŸ§ª Batch 26/55 â€” Loss: 0.1220 â€” Time: 0.89s\n","  ğŸ§ª Batch 27/55 â€” Loss: 0.1067 â€” Time: 0.88s\n","  ğŸ§ª Batch 28/55 â€” Loss: 0.2015 â€” Time: 0.88s\n","  ğŸ§ª Batch 29/55 â€” Loss: 0.1036 â€” Time: 0.89s\n","  ğŸ§ª Batch 30/55 â€” Loss: 0.1441 â€” Time: 0.89s\n","  ğŸ§ª Batch 31/55 â€” Loss: 0.0993 â€” Time: 0.87s\n","  ğŸ§ª Batch 32/55 â€” Loss: 0.1145 â€” Time: 0.89s\n","  ğŸ§ª Batch 33/55 â€” Loss: 0.1041 â€” Time: 0.89s\n","  ğŸ§ª Batch 34/55 â€” Loss: 0.1249 â€” Time: 0.88s\n","  ğŸ§ª Batch 35/55 â€” Loss: 0.1096 â€” Time: 0.89s\n","  ğŸ§ª Batch 36/55 â€” Loss: 0.1713 â€” Time: 0.88s\n","  ğŸ§ª Batch 37/55 â€” Loss: 0.1142 â€” Time: 0.88s\n","  ğŸ§ª Batch 38/55 â€” Loss: 0.1258 â€” Time: 0.88s\n","  ğŸ§ª Batch 39/55 â€” Loss: 0.1366 â€” Time: 0.89s\n","  ğŸ§ª Batch 40/55 â€” Loss: 0.1097 â€” Time: 0.88s\n","  ğŸ§ª Batch 41/55 â€” Loss: 0.1109 â€” Time: 0.89s\n","  ğŸ§ª Batch 42/55 â€” Loss: 0.0918 â€” Time: 0.88s\n","  ğŸ§ª Batch 43/55 â€” Loss: 0.0963 â€” Time: 0.88s\n","  ğŸ§ª Batch 44/55 â€” Loss: 0.1074 â€” Time: 0.88s\n","  ğŸ§ª Batch 45/55 â€” Loss: 0.1366 â€” Time: 0.88s\n","  ğŸ§ª Batch 46/55 â€” Loss: 0.1105 â€” Time: 0.89s\n","  ğŸ§ª Batch 47/55 â€” Loss: 0.1067 â€” Time: 0.87s\n","  ğŸ§ª Batch 48/55 â€” Loss: 0.1009 â€” Time: 0.89s\n","  ğŸ§ª Batch 49/55 â€” Loss: 0.1255 â€” Time: 0.89s\n","  ğŸ§ª Batch 50/55 â€” Loss: 0.2482 â€” Time: 0.87s\n","  ğŸ§ª Batch 51/55 â€” Loss: 0.1106 â€” Time: 0.88s\n","  ğŸ§ª Batch 52/55 â€” Loss: 0.1333 â€” Time: 0.89s\n","  ğŸ§ª Batch 53/55 â€” Loss: 0.1217 â€” Time: 0.88s\n","  ğŸ§ª Batch 54/55 â€” Loss: 0.1474 â€” Time: 0.88s\n","  ğŸ§ª Batch 55/55 â€” Loss: 0.1306 â€” Time: 0.65s\n","ğŸ•’ Epoch Time: 51.80s | Avg Batch Time: 0.88s\n","ğŸ“Š Train Loss: 0.1250 | Val Loss: 0.3037 | IoU: 0.7691 | Dice: 0.8699 | Jaccard: 0.7732 | Precision: 0.9279 | Recall: 0.8237 | Accuracy: 0.9708\n","\n","Epoch 13/100\n","  ğŸ§ª Batch 1/55 â€” Loss: 0.1120 â€” Time: 0.87s\n","  ğŸ§ª Batch 2/55 â€” Loss: 0.1140 â€” Time: 0.88s\n","  ğŸ§ª Batch 3/55 â€” Loss: 0.1104 â€” Time: 0.89s\n","  ğŸ§ª Batch 4/55 â€” Loss: 0.1061 â€” Time: 0.88s\n","  ğŸ§ª Batch 5/55 â€” Loss: 0.1370 â€” Time: 0.89s\n","  ğŸ§ª Batch 6/55 â€” Loss: 0.1057 â€” Time: 0.89s\n","  ğŸ§ª Batch 7/55 â€” Loss: 0.1163 â€” Time: 0.88s\n","  ğŸ§ª Batch 8/55 â€” Loss: 0.1042 â€” Time: 0.88s\n","  ğŸ§ª Batch 9/55 â€” Loss: 0.1058 â€” Time: 0.87s\n","  ğŸ§ª Batch 10/55 â€” Loss: 0.1138 â€” Time: 0.89s\n","  ğŸ§ª Batch 11/55 â€” Loss: 0.1215 â€” Time: 0.89s\n","  ğŸ§ª Batch 12/55 â€” Loss: 0.1542 â€” Time: 0.89s\n","  ğŸ§ª Batch 13/55 â€” Loss: 0.0892 â€” Time: 0.87s\n","  ğŸ§ª Batch 14/55 â€” Loss: 0.0920 â€” Time: 0.89s\n","  ğŸ§ª Batch 15/55 â€” Loss: 0.1053 â€” Time: 0.88s\n","  ğŸ§ª Batch 16/55 â€” Loss: 0.1102 â€” Time: 0.88s\n","  ğŸ§ª Batch 17/55 â€” Loss: 0.1356 â€” Time: 0.89s\n","  ğŸ§ª Batch 18/55 â€” Loss: 0.0952 â€” Time: 0.89s\n","  ğŸ§ª Batch 19/55 â€” Loss: 0.1108 â€” Time: 0.88s\n","  ğŸ§ª Batch 20/55 â€” Loss: 0.1056 â€” Time: 0.88s\n","  ğŸ§ª Batch 21/55 â€” Loss: 0.1169 â€” Time: 0.89s\n","  ğŸ§ª Batch 22/55 â€” Loss: 0.1070 â€” Time: 0.89s\n","  ğŸ§ª Batch 23/55 â€” Loss: 0.0976 â€” Time: 0.88s\n","  ğŸ§ª Batch 24/55 â€” Loss: 0.1105 â€” Time: 0.88s\n","  ğŸ§ª Batch 25/55 â€” Loss: 0.0921 â€” Time: 0.88s\n","  ğŸ§ª Batch 26/55 â€” Loss: 0.1906 â€” Time: 0.89s\n","  ğŸ§ª Batch 27/55 â€” Loss: 0.0800 â€” Time: 0.89s\n","  ğŸ§ª Batch 28/55 â€” Loss: 0.0853 â€” Time: 0.89s\n","  ğŸ§ª Batch 29/55 â€” Loss: 0.0972 â€” Time: 0.89s\n","  ğŸ§ª Batch 30/55 â€” Loss: 0.1034 â€” Time: 0.89s\n","  ğŸ§ª Batch 31/55 â€” Loss: 0.0947 â€” Time: 0.88s\n","  ğŸ§ª Batch 32/55 â€” Loss: 0.1093 â€” Time: 0.88s\n","  ğŸ§ª Batch 33/55 â€” Loss: 0.1127 â€” Time: 0.89s\n","  ğŸ§ª Batch 34/55 â€” Loss: 0.0901 â€” Time: 0.89s\n","  ğŸ§ª Batch 35/55 â€” Loss: 0.1113 â€” Time: 0.89s\n","  ğŸ§ª Batch 36/55 â€” Loss: 0.1412 â€” Time: 0.89s\n","  ğŸ§ª Batch 37/55 â€” Loss: 0.0884 â€” Time: 0.89s\n","  ğŸ§ª Batch 38/55 â€” Loss: 0.1037 â€” Time: 0.89s\n","  ğŸ§ª Batch 39/55 â€” Loss: 0.1095 â€” Time: 0.89s\n","  ğŸ§ª Batch 40/55 â€” Loss: 0.1012 â€” Time: 0.89s\n","  ğŸ§ª Batch 41/55 â€” Loss: 0.0875 â€” Time: 0.88s\n","  ğŸ§ª Batch 42/55 â€” Loss: 0.0903 â€” Time: 0.89s\n","  ğŸ§ª Batch 43/55 â€” Loss: 0.0976 â€” Time: 0.89s\n","  ğŸ§ª Batch 44/55 â€” Loss: 0.1252 â€” Time: 0.89s\n","  ğŸ§ª Batch 45/55 â€” Loss: 0.0959 â€” Time: 0.89s\n","  ğŸ§ª Batch 46/55 â€” Loss: 0.1241 â€” Time: 0.88s\n","  ğŸ§ª Batch 47/55 â€” Loss: 0.1025 â€” Time: 0.89s\n","  ğŸ§ª Batch 48/55 â€” Loss: 0.1138 â€” Time: 0.89s\n","  ğŸ§ª Batch 49/55 â€” Loss: 0.0971 â€” Time: 0.89s\n","  ğŸ§ª Batch 50/55 â€” Loss: 0.1015 â€” Time: 0.89s\n","  ğŸ§ª Batch 51/55 â€” Loss: 0.1186 â€” Time: 0.89s\n","  ğŸ§ª Batch 52/55 â€” Loss: 0.1264 â€” Time: 0.89s\n","  ğŸ§ª Batch 53/55 â€” Loss: 0.1003 â€” Time: 0.89s\n","  ğŸ§ª Batch 54/55 â€” Loss: 0.0934 â€” Time: 0.88s\n","  ğŸ§ª Batch 55/55 â€” Loss: 0.0856 â€” Time: 0.64s\n","ğŸ•’ Epoch Time: 52.22s | Avg Batch Time: 0.88s\n","ğŸ“Š Train Loss: 0.1081 | Val Loss: 0.2594 | IoU: 0.7923 | Dice: 0.8906 | Jaccard: 0.8055 | Precision: 0.9201 | Recall: 0.8665 | Accuracy: 0.9743\n","\n","Epoch 14/100\n","  ğŸ§ª Batch 1/55 â€” Loss: 0.1181 â€” Time: 0.87s\n","  ğŸ§ª Batch 2/55 â€” Loss: 0.0920 â€” Time: 0.87s\n","  ğŸ§ª Batch 3/55 â€” Loss: 0.0935 â€” Time: 0.89s\n","  ğŸ§ª Batch 4/55 â€” Loss: 0.0947 â€” Time: 0.89s\n","  ğŸ§ª Batch 5/55 â€” Loss: 0.1877 â€” Time: 0.89s\n","  ğŸ§ª Batch 6/55 â€” Loss: 0.1435 â€” Time: 0.89s\n","  ğŸ§ª Batch 7/55 â€” Loss: 0.1033 â€” Time: 0.88s\n","  ğŸ§ª Batch 8/55 â€” Loss: 0.1016 â€” Time: 0.88s\n","  ğŸ§ª Batch 9/55 â€” Loss: 0.1137 â€” Time: 0.89s\n","  ğŸ§ª Batch 10/55 â€” Loss: 0.1095 â€” Time: 0.89s\n","  ğŸ§ª Batch 11/55 â€” Loss: 0.1086 â€” Time: 0.89s\n","  ğŸ§ª Batch 12/55 â€” Loss: 0.0945 â€” Time: 0.89s\n","  ğŸ§ª Batch 13/55 â€” Loss: 0.0985 â€” Time: 0.89s\n","  ğŸ§ª Batch 14/55 â€” Loss: 0.0883 â€” Time: 0.89s\n","  ğŸ§ª Batch 15/55 â€” Loss: 0.1212 â€” Time: 0.89s\n","  ğŸ§ª Batch 16/55 â€” Loss: 0.0933 â€” Time: 0.89s\n","  ğŸ§ª Batch 17/55 â€” Loss: 0.1136 â€” Time: 0.89s\n","  ğŸ§ª Batch 18/55 â€” Loss: 0.0909 â€” Time: 0.89s\n","  ğŸ§ª Batch 19/55 â€” Loss: 0.0827 â€” Time: 0.89s\n","  ğŸ§ª Batch 20/55 â€” Loss: 0.0901 â€” Time: 0.89s\n","  ğŸ§ª Batch 21/55 â€” Loss: 0.0871 â€” Time: 0.89s\n","  ğŸ§ª Batch 22/55 â€” Loss: 0.0867 â€” Time: 0.89s\n","  ğŸ§ª Batch 23/55 â€” Loss: 0.0988 â€” Time: 0.89s\n","  ğŸ§ª Batch 24/55 â€” Loss: 0.0831 â€” Time: 0.88s\n","  ğŸ§ª Batch 25/55 â€” Loss: 0.0828 â€” Time: 0.88s\n","  ğŸ§ª Batch 26/55 â€” Loss: 0.0847 â€” Time: 0.88s\n","  ğŸ§ª Batch 27/55 â€” Loss: 0.0928 â€” Time: 0.89s\n","  ğŸ§ª Batch 28/55 â€” Loss: 0.0841 â€” Time: 0.89s\n","  ğŸ§ª Batch 29/55 â€” Loss: 0.1232 â€” Time: 0.90s\n","  ğŸ§ª Batch 30/55 â€” Loss: 0.0974 â€” Time: 0.88s\n","  ğŸ§ª Batch 31/55 â€” Loss: 0.0997 â€” Time: 0.90s\n","  ğŸ§ª Batch 32/55 â€” Loss: 0.0824 â€” Time: 0.89s\n","  ğŸ§ª Batch 33/55 â€” Loss: 0.0887 â€” Time: 0.89s\n","  ğŸ§ª Batch 34/55 â€” Loss: 0.0895 â€” Time: 0.89s\n","  ğŸ§ª Batch 35/55 â€” Loss: 0.0858 â€” Time: 0.89s\n","  ğŸ§ª Batch 36/55 â€” Loss: 0.0884 â€” Time: 0.89s\n","  ğŸ§ª Batch 37/55 â€” Loss: 0.0825 â€” Time: 0.89s\n","  ğŸ§ª Batch 38/55 â€” Loss: 0.0789 â€” Time: 0.89s\n","  ğŸ§ª Batch 39/55 â€” Loss: 0.0986 â€” Time: 0.89s\n","  ğŸ§ª Batch 40/55 â€” Loss: 0.0750 â€” Time: 0.89s\n","  ğŸ§ª Batch 41/55 â€” Loss: 0.0948 â€” Time: 0.89s\n","  ğŸ§ª Batch 42/55 â€” Loss: 0.0954 â€” Time: 0.89s\n","  ğŸ§ª Batch 43/55 â€” Loss: 0.0795 â€” Time: 0.89s\n","  ğŸ§ª Batch 44/55 â€” Loss: 0.0970 â€” Time: 0.89s\n","  ğŸ§ª Batch 45/55 â€” Loss: 0.0812 â€” Time: 0.89s\n","  ğŸ§ª Batch 46/55 â€” Loss: 0.0810 â€” Time: 0.89s\n","  ğŸ§ª Batch 47/55 â€” Loss: 0.0729 â€” Time: 0.89s\n","  ğŸ§ª Batch 48/55 â€” Loss: 0.0894 â€” Time: 0.89s\n","  ğŸ§ª Batch 49/55 â€” Loss: 0.0798 â€” Time: 0.89s\n","  ğŸ§ª Batch 50/55 â€” Loss: 0.0817 â€” Time: 0.89s\n","  ğŸ§ª Batch 51/55 â€” Loss: 0.0936 â€” Time: 0.89s\n","  ğŸ§ª Batch 52/55 â€” Loss: 0.1058 â€” Time: 0.89s\n","  ğŸ§ª Batch 53/55 â€” Loss: 0.0989 â€” Time: 0.89s\n","  ğŸ§ª Batch 54/55 â€” Loss: 0.0816 â€” Time: 0.89s\n","  ğŸ§ª Batch 55/55 â€” Loss: 0.1024 â€” Time: 0.65s\n","ğŸ•’ Epoch Time: 52.87s | Avg Batch Time: 0.88s\n","ğŸ“Š Train Loss: 0.0957 | Val Loss: 0.2478 | IoU: 0.7952 | Dice: 0.8959 | Jaccard: 0.8143 | Precision: 0.9317 | Recall: 0.8655 | Accuracy: 0.9753\n","\n","Epoch 15/100\n","  ğŸ§ª Batch 1/55 â€” Loss: 0.0880 â€” Time: 0.87s\n","  ğŸ§ª Batch 2/55 â€” Loss: 0.0972 â€” Time: 0.87s\n","  ğŸ§ª Batch 3/55 â€” Loss: 0.0865 â€” Time: 0.89s\n","  ğŸ§ª Batch 4/55 â€” Loss: 0.0844 â€” Time: 0.89s\n","  ğŸ§ª Batch 5/55 â€” Loss: 0.0781 â€” Time: 0.89s\n","  ğŸ§ª Batch 6/55 â€” Loss: 0.0910 â€” Time: 0.88s\n","  ğŸ§ª Batch 7/55 â€” Loss: 0.0748 â€” Time: 0.89s\n","  ğŸ§ª Batch 8/55 â€” Loss: 0.0739 â€” Time: 0.89s\n","  ğŸ§ª Batch 9/55 â€” Loss: 0.1158 â€” Time: 0.89s\n","  ğŸ§ª Batch 10/55 â€” Loss: 0.0708 â€” Time: 0.89s\n","  ğŸ§ª Batch 11/55 â€” Loss: 0.0864 â€” Time: 0.88s\n","  ğŸ§ª Batch 12/55 â€” Loss: 0.0828 â€” Time: 0.88s\n","  ğŸ§ª Batch 13/55 â€” Loss: 0.0789 â€” Time: 0.89s\n","  ğŸ§ª Batch 14/55 â€” Loss: 0.0886 â€” Time: 0.89s\n","  ğŸ§ª Batch 15/55 â€” Loss: 0.0957 â€” Time: 0.87s\n","  ğŸ§ª Batch 16/55 â€” Loss: 0.0763 â€” Time: 0.88s\n","  ğŸ§ª Batch 17/55 â€” Loss: 0.0821 â€” Time: 0.88s\n","  ğŸ§ª Batch 18/55 â€” Loss: 0.0701 â€” Time: 0.89s\n","  ğŸ§ª Batch 19/55 â€” Loss: 0.1650 â€” Time: 0.89s\n","  ğŸ§ª Batch 20/55 â€” Loss: 0.0773 â€” Time: 0.89s\n","  ğŸ§ª Batch 21/55 â€” Loss: 0.0996 â€” Time: 0.88s\n","  ğŸ§ª Batch 22/55 â€” Loss: 0.0885 â€” Time: 0.88s\n","  ğŸ§ª Batch 23/55 â€” Loss: 0.0802 â€” Time: 0.87s\n","  ğŸ§ª Batch 24/55 â€” Loss: 0.0860 â€” Time: 0.89s\n","  ğŸ§ª Batch 25/55 â€” Loss: 0.0716 â€” Time: 0.89s\n","  ğŸ§ª Batch 26/55 â€” Loss: 0.0872 â€” Time: 0.89s\n","  ğŸ§ª Batch 27/55 â€” Loss: 0.0784 â€” Time: 0.89s\n","  ğŸ§ª Batch 28/55 â€” Loss: 0.0710 â€” Time: 0.89s\n","  ğŸ§ª Batch 29/55 â€” Loss: 0.0942 â€” Time: 0.89s\n","  ğŸ§ª Batch 30/55 â€” Loss: 0.0919 â€” Time: 0.89s\n","  ğŸ§ª Batch 31/55 â€” Loss: 0.0977 â€” Time: 0.89s\n","  ğŸ§ª Batch 32/55 â€” Loss: 0.0748 â€” Time: 0.89s\n","  ğŸ§ª Batch 33/55 â€” Loss: 0.0701 â€” Time: 0.88s\n","  ğŸ§ª Batch 34/55 â€” Loss: 0.0664 â€” Time: 0.88s\n","  ğŸ§ª Batch 35/55 â€” Loss: 0.1018 â€” Time: 0.89s\n","  ğŸ§ª Batch 36/55 â€” Loss: 0.0878 â€” Time: 0.89s\n","  ğŸ§ª Batch 37/55 â€” Loss: 0.1198 â€” Time: 0.89s\n","  ğŸ§ª Batch 38/55 â€” Loss: 0.0750 â€” Time: 0.88s\n","  ğŸ§ª Batch 39/55 â€” Loss: 0.0685 â€” Time: 0.89s\n","  ğŸ§ª Batch 40/55 â€” Loss: 0.0912 â€” Time: 0.89s\n","  ğŸ§ª Batch 41/55 â€” Loss: 0.0890 â€” Time: 0.89s\n","  ğŸ§ª Batch 42/55 â€” Loss: 0.0735 â€” Time: 0.89s\n","  ğŸ§ª Batch 43/55 â€” Loss: 0.1018 â€” Time: 0.89s\n","  ğŸ§ª Batch 44/55 â€” Loss: 0.0666 â€” Time: 0.89s\n","  ğŸ§ª Batch 45/55 â€” Loss: 0.0720 â€” Time: 0.88s\n","  ğŸ§ª Batch 46/55 â€” Loss: 0.0814 â€” Time: 0.89s\n","  ğŸ§ª Batch 47/55 â€” Loss: 0.0972 â€” Time: 0.89s\n","  ğŸ§ª Batch 48/55 â€” Loss: 0.0896 â€” Time: 0.89s\n","  ğŸ§ª Batch 49/55 â€” Loss: 0.0696 â€” Time: 0.89s\n","  ğŸ§ª Batch 50/55 â€” Loss: 0.0737 â€” Time: 0.89s\n","  ğŸ§ª Batch 51/55 â€” Loss: 0.0965 â€” Time: 0.89s\n","  ğŸ§ª Batch 52/55 â€” Loss: 0.0864 â€” Time: 0.89s\n","  ğŸ§ª Batch 53/55 â€” Loss: 0.0836 â€” Time: 0.89s\n","  ğŸ§ª Batch 54/55 â€” Loss: 0.0834 â€” Time: 0.87s\n","  ğŸ§ª Batch 55/55 â€” Loss: 0.0857 â€” Time: 0.65s\n","ğŸ•’ Epoch Time: 52.79s | Avg Batch Time: 0.88s\n","ğŸ“Š Train Loss: 0.0857 | Val Loss: 0.2460 | IoU: 0.7942 | Dice: 0.8952 | Jaccard: 0.8133 | Precision: 0.9368 | Recall: 0.8600 | Accuracy: 0.9753\n","\n","Epoch 16/100\n","  ğŸ§ª Batch 1/55 â€” Loss: 0.0820 â€” Time: 0.87s\n","  ğŸ§ª Batch 2/55 â€” Loss: 0.0691 â€” Time: 0.89s\n","  ğŸ§ª Batch 3/55 â€” Loss: 0.0752 â€” Time: 0.89s\n","  ğŸ§ª Batch 4/55 â€” Loss: 0.1003 â€” Time: 0.89s\n","  ğŸ§ª Batch 5/55 â€” Loss: 0.0641 â€” Time: 0.87s\n","  ğŸ§ª Batch 6/55 â€” Loss: 0.0751 â€” Time: 0.89s\n","  ğŸ§ª Batch 7/55 â€” Loss: 0.0972 â€” Time: 0.88s\n","  ğŸ§ª Batch 8/55 â€” Loss: 0.0765 â€” Time: 0.88s\n","  ğŸ§ª Batch 9/55 â€” Loss: 0.0756 â€” Time: 0.89s\n","  ğŸ§ª Batch 10/55 â€” Loss: 0.0765 â€” Time: 0.89s\n","  ğŸ§ª Batch 11/55 â€” Loss: 0.0684 â€” Time: 0.89s\n","  ğŸ§ª Batch 12/55 â€” Loss: 0.0722 â€” Time: 0.88s\n","  ğŸ§ª Batch 13/55 â€” Loss: 0.0631 â€” Time: 0.89s\n","  ğŸ§ª Batch 14/55 â€” Loss: 0.0965 â€” Time: 0.87s\n","  ğŸ§ª Batch 15/55 â€” Loss: 0.0912 â€” Time: 0.88s\n","  ğŸ§ª Batch 16/55 â€” Loss: 0.0812 â€” Time: 0.88s\n","  ğŸ§ª Batch 17/55 â€” Loss: 0.0729 â€” Time: 0.89s\n","  ğŸ§ª Batch 18/55 â€” Loss: 0.0659 â€” Time: 0.89s\n","  ğŸ§ª Batch 19/55 â€” Loss: 0.0629 â€” Time: 0.88s\n","  ğŸ§ª Batch 20/55 â€” Loss: 0.0951 â€” Time: 0.89s\n","  ğŸ§ª Batch 21/55 â€” Loss: 0.0764 â€” Time: 0.88s\n","  ğŸ§ª Batch 22/55 â€” Loss: 0.0693 â€” Time: 0.89s\n","  ğŸ§ª Batch 23/55 â€” Loss: 0.0682 â€” Time: 0.89s\n","  ğŸ§ª Batch 24/55 â€” Loss: 0.0656 â€” Time: 0.88s\n","  ğŸ§ª Batch 25/55 â€” Loss: 0.0729 â€” Time: 0.89s\n","  ğŸ§ª Batch 26/55 â€” Loss: 0.0816 â€” Time: 0.89s\n","  ğŸ§ª Batch 27/55 â€” Loss: 0.0789 â€” Time: 0.89s\n","  ğŸ§ª Batch 28/55 â€” Loss: 0.0705 â€” Time: 0.89s\n","  ğŸ§ª Batch 29/55 â€” Loss: 0.0794 â€” Time: 0.89s\n","  ğŸ§ª Batch 30/55 â€” Loss: 0.0738 â€” Time: 0.89s\n","  ğŸ§ª Batch 31/55 â€” Loss: 0.0672 â€” Time: 0.89s\n","  ğŸ§ª Batch 32/55 â€” Loss: 0.0734 â€” Time: 0.89s\n","  ğŸ§ª Batch 33/55 â€” Loss: 0.0750 â€” Time: 0.88s\n","  ğŸ§ª Batch 34/55 â€” Loss: 0.0894 â€” Time: 0.88s\n","  ğŸ§ª Batch 35/55 â€” Loss: 0.1508 â€” Time: 0.89s\n","  ğŸ§ª Batch 36/55 â€” Loss: 0.0834 â€” Time: 0.89s\n","  ğŸ§ª Batch 37/55 â€” Loss: 0.0704 â€” Time: 0.89s\n","  ğŸ§ª Batch 38/55 â€” Loss: 0.0664 â€” Time: 0.89s\n","  ğŸ§ª Batch 39/55 â€” Loss: 0.0908 â€” Time: 0.89s\n","  ğŸ§ª Batch 40/55 â€” Loss: 0.0655 â€” Time: 0.89s\n","  ğŸ§ª Batch 41/55 â€” Loss: 0.0758 â€” Time: 0.89s\n","  ğŸ§ª Batch 42/55 â€” Loss: 0.0847 â€” Time: 0.89s\n","  ğŸ§ª Batch 43/55 â€” Loss: 0.0777 â€” Time: 0.89s\n","  ğŸ§ª Batch 44/55 â€” Loss: 0.0962 â€” Time: 0.89s\n","  ğŸ§ª Batch 45/55 â€” Loss: 0.0857 â€” Time: 0.89s\n","  ğŸ§ª Batch 46/55 â€” Loss: 0.0808 â€” Time: 0.89s\n","  ğŸ§ª Batch 47/55 â€” Loss: 0.0740 â€” Time: 0.89s\n","  ğŸ§ª Batch 48/55 â€” Loss: 0.0746 â€” Time: 0.89s\n","  ğŸ§ª Batch 49/55 â€” Loss: 0.0690 â€” Time: 0.89s\n","  ğŸ§ª Batch 50/55 â€” Loss: 0.0734 â€” Time: 0.89s\n","  ğŸ§ª Batch 51/55 â€” Loss: 0.0643 â€” Time: 0.89s\n","  ğŸ§ª Batch 52/55 â€” Loss: 0.0644 â€” Time: 0.89s\n","  ğŸ§ª Batch 53/55 â€” Loss: 0.0665 â€” Time: 0.89s\n","  ğŸ§ª Batch 54/55 â€” Loss: 0.0686 â€” Time: 0.89s\n","  ğŸ§ª Batch 55/55 â€” Loss: 0.0889 â€” Time: 0.65s\n","ğŸ•’ Epoch Time: 52.86s | Avg Batch Time: 0.88s\n","ğŸ“Š Train Loss: 0.0777 | Val Loss: 0.2425 | IoU: 0.7962 | Dice: 0.8957 | Jaccard: 0.8139 | Precision: 0.9221 | Recall: 0.8735 | Accuracy: 0.9750\n","\n","Epoch 17/100\n","  ğŸ§ª Batch 1/55 â€” Loss: 0.0735 â€” Time: 0.89s\n","  ğŸ§ª Batch 2/55 â€” Loss: 0.0889 â€” Time: 0.87s\n","  ğŸ§ª Batch 3/55 â€” Loss: 0.0759 â€” Time: 0.89s\n","  ğŸ§ª Batch 4/55 â€” Loss: 0.0714 â€” Time: 0.89s\n","  ğŸ§ª Batch 5/55 â€” Loss: 0.0714 â€” Time: 0.89s\n","  ğŸ§ª Batch 6/55 â€” Loss: 0.0714 â€” Time: 0.88s\n","  ğŸ§ª Batch 7/55 â€” Loss: 0.0676 â€” Time: 0.88s\n","  ğŸ§ª Batch 8/55 â€” Loss: 0.0648 â€” Time: 0.89s\n","  ğŸ§ª Batch 9/55 â€” Loss: 0.0917 â€” Time: 0.89s\n","  ğŸ§ª Batch 10/55 â€” Loss: 0.0667 â€” Time: 0.89s\n","  ğŸ§ª Batch 11/55 â€” Loss: 0.0597 â€” Time: 0.89s\n","  ğŸ§ª Batch 12/55 â€” Loss: 0.0682 â€” Time: 0.89s\n","  ğŸ§ª Batch 13/55 â€” Loss: 0.0725 â€” Time: 0.89s\n","  ğŸ§ª Batch 14/55 â€” Loss: 0.0655 â€” Time: 0.89s\n","  ğŸ§ª Batch 15/55 â€” Loss: 0.0798 â€” Time: 0.89s\n","  ğŸ§ª Batch 16/55 â€” Loss: 0.0579 â€” Time: 0.89s\n","  ğŸ§ª Batch 17/55 â€” Loss: 0.0679 â€” Time: 0.89s\n","  ğŸ§ª Batch 18/55 â€” Loss: 0.0713 â€” Time: 0.89s\n","  ğŸ§ª Batch 19/55 â€” Loss: 0.0818 â€” Time: 0.89s\n","  ğŸ§ª Batch 20/55 â€” Loss: 0.0620 â€” Time: 0.89s\n","  ğŸ§ª Batch 21/55 â€” Loss: 0.0838 â€” Time: 0.89s\n","  ğŸ§ª Batch 22/55 â€” Loss: 0.0766 â€” Time: 0.89s\n","  ğŸ§ª Batch 23/55 â€” Loss: 0.0926 â€” Time: 0.89s\n","  ğŸ§ª Batch 24/55 â€” Loss: 0.0703 â€” Time: 0.89s\n","  ğŸ§ª Batch 25/55 â€” Loss: 0.0754 â€” Time: 0.89s\n","  ğŸ§ª Batch 26/55 â€” Loss: 0.0623 â€” Time: 0.88s\n","  ğŸ§ª Batch 27/55 â€” Loss: 0.0664 â€” Time: 0.89s\n","  ğŸ§ª Batch 28/55 â€” Loss: 0.0654 â€” Time: 0.88s\n","  ğŸ§ª Batch 29/55 â€” Loss: 0.0617 â€” Time: 0.89s\n","  ğŸ§ª Batch 30/55 â€” Loss: 0.0638 â€” Time: 0.89s\n","  ğŸ§ª Batch 31/55 â€” Loss: 0.0800 â€” Time: 0.89s\n","  ğŸ§ª Batch 32/55 â€” Loss: 0.0645 â€” Time: 0.89s\n","  ğŸ§ª Batch 33/55 â€” Loss: 0.0713 â€” Time: 0.89s\n","  ğŸ§ª Batch 34/55 â€” Loss: 0.0646 â€” Time: 0.89s\n","  ğŸ§ª Batch 35/55 â€” Loss: 0.0616 â€” Time: 0.89s\n","  ğŸ§ª Batch 36/55 â€” Loss: 0.0662 â€” Time: 0.89s\n","  ğŸ§ª Batch 37/55 â€” Loss: 0.0780 â€” Time: 0.89s\n","  ğŸ§ª Batch 38/55 â€” Loss: 0.0614 â€” Time: 0.89s\n","  ğŸ§ª Batch 39/55 â€” Loss: 0.0629 â€” Time: 0.89s\n","  ğŸ§ª Batch 40/55 â€” Loss: 0.0556 â€” Time: 0.89s\n","  ğŸ§ª Batch 41/55 â€” Loss: 0.0656 â€” Time: 0.90s\n","  ğŸ§ª Batch 42/55 â€” Loss: 0.0662 â€” Time: 0.89s\n","  ğŸ§ª Batch 43/55 â€” Loss: 0.0627 â€” Time: 0.88s\n","  ğŸ§ª Batch 44/55 â€” Loss: 0.0779 â€” Time: 0.89s\n","  ğŸ§ª Batch 45/55 â€” Loss: 0.0551 â€” Time: 0.89s\n","  ğŸ§ª Batch 46/55 â€” Loss: 0.0633 â€” Time: 0.89s\n","  ğŸ§ª Batch 47/55 â€” Loss: 0.0667 â€” Time: 0.89s\n","  ğŸ§ª Batch 48/55 â€” Loss: 0.0575 â€” Time: 0.89s\n","  ğŸ§ª Batch 49/55 â€” Loss: 0.0745 â€” Time: 0.89s\n","  ğŸ§ª Batch 50/55 â€” Loss: 0.0785 â€” Time: 0.89s\n","  ğŸ§ª Batch 51/55 â€” Loss: 0.0606 â€” Time: 0.89s\n","  ğŸ§ª Batch 52/55 â€” Loss: 0.0833 â€” Time: 0.89s\n","  ğŸ§ª Batch 53/55 â€” Loss: 0.0979 â€” Time: 0.89s\n","  ğŸ§ª Batch 54/55 â€” Loss: 0.1276 â€” Time: 0.89s\n","  ğŸ§ª Batch 55/55 â€” Loss: 0.0746 â€” Time: 0.65s\n","ğŸ•’ Epoch Time: 52.86s | Avg Batch Time: 0.88s\n","ğŸ“Š Train Loss: 0.0714 | Val Loss: 0.2410 | IoU: 0.7967 | Dice: 0.8971 | Jaccard: 0.8164 | Precision: 0.9318 | Recall: 0.8676 | Accuracy: 0.9754\n","\n","Epoch 18/100\n","  ğŸ§ª Batch 1/55 â€” Loss: 0.0649 â€” Time: 0.88s\n","  ğŸ§ª Batch 2/55 â€” Loss: 0.0620 â€” Time: 0.88s\n","  ğŸ§ª Batch 3/55 â€” Loss: 0.0799 â€” Time: 0.89s\n","  ğŸ§ª Batch 4/55 â€” Loss: 0.0607 â€” Time: 0.89s\n","  ğŸ§ª Batch 5/55 â€” Loss: 0.0535 â€” Time: 0.89s\n","  ğŸ§ª Batch 6/55 â€” Loss: 0.0635 â€” Time: 0.88s\n","  ğŸ§ª Batch 7/55 â€” Loss: 0.0799 â€” Time: 0.89s\n","  ğŸ§ª Batch 8/55 â€” Loss: 0.0638 â€” Time: 0.89s\n","  ğŸ§ª Batch 9/55 â€” Loss: 0.0602 â€” Time: 0.89s\n","  ğŸ§ª Batch 10/55 â€” Loss: 0.0597 â€” Time: 0.89s\n","  ğŸ§ª Batch 11/55 â€” Loss: 0.0811 â€” Time: 0.89s\n","  ğŸ§ª Batch 12/55 â€” Loss: 0.0778 â€” Time: 0.88s\n","  ğŸ§ª Batch 13/55 â€” Loss: 0.0567 â€” Time: 0.89s\n","  ğŸ§ª Batch 14/55 â€” Loss: 0.0622 â€” Time: 0.89s\n","  ğŸ§ª Batch 15/55 â€” Loss: 0.0614 â€” Time: 0.89s\n","  ğŸ§ª Batch 16/55 â€” Loss: 0.1122 â€” Time: 0.89s\n","  ğŸ§ª Batch 17/55 â€” Loss: 0.0733 â€” Time: 0.89s\n","  ğŸ§ª Batch 18/55 â€” Loss: 0.0637 â€” Time: 0.89s\n","  ğŸ§ª Batch 19/55 â€” Loss: 0.0685 â€” Time: 0.89s\n","  ğŸ§ª Batch 20/55 â€” Loss: 0.0623 â€” Time: 0.89s\n","  ğŸ§ª Batch 21/55 â€” Loss: 0.0763 â€” Time: 0.89s\n","  ğŸ§ª Batch 22/55 â€” Loss: 0.0573 â€” Time: 0.89s\n","  ğŸ§ª Batch 23/55 â€” Loss: 0.0722 â€” Time: 0.89s\n","  ğŸ§ª Batch 24/55 â€” Loss: 0.0642 â€” Time: 0.89s\n","  ğŸ§ª Batch 25/55 â€” Loss: 0.0736 â€” Time: 0.89s\n","  ğŸ§ª Batch 26/55 â€” Loss: 0.0627 â€” Time: 0.89s\n","  ğŸ§ª Batch 27/55 â€” Loss: 0.0596 â€” Time: 0.89s\n","  ğŸ§ª Batch 28/55 â€” Loss: 0.0534 â€” Time: 0.88s\n","  ğŸ§ª Batch 29/55 â€” Loss: 0.0624 â€” Time: 0.89s\n","  ğŸ§ª Batch 30/55 â€” Loss: 0.0603 â€” Time: 0.89s\n","  ğŸ§ª Batch 31/55 â€” Loss: 0.0573 â€” Time: 0.89s\n","  ğŸ§ª Batch 32/55 â€” Loss: 0.1135 â€” Time: 0.89s\n","  ğŸ§ª Batch 33/55 â€” Loss: 0.0561 â€” Time: 0.89s\n","  ğŸ§ª Batch 34/55 â€” Loss: 0.0643 â€” Time: 0.89s\n","  ğŸ§ª Batch 35/55 â€” Loss: 0.0577 â€” Time: 0.89s\n","  ğŸ§ª Batch 36/55 â€” Loss: 0.0769 â€” Time: 0.89s\n","  ğŸ§ª Batch 37/55 â€” Loss: 0.0725 â€” Time: 0.89s\n","  ğŸ§ª Batch 38/55 â€” Loss: 0.0661 â€” Time: 0.89s\n","  ğŸ§ª Batch 39/55 â€” Loss: 0.0615 â€” Time: 0.89s\n","  ğŸ§ª Batch 40/55 â€” Loss: 0.0702 â€” Time: 0.89s\n","  ğŸ§ª Batch 41/55 â€” Loss: 0.0561 â€” Time: 0.88s\n","  ğŸ§ª Batch 42/55 â€” Loss: 0.0842 â€” Time: 0.88s\n","  ğŸ§ª Batch 43/55 â€” Loss: 0.0629 â€” Time: 0.88s\n","  ğŸ§ª Batch 44/55 â€” Loss: 0.0629 â€” Time: 0.88s\n","  ğŸ§ª Batch 45/55 â€” Loss: 0.0647 â€” Time: 0.89s\n","  ğŸ§ª Batch 46/55 â€” Loss: 0.0555 â€” Time: 0.88s\n","  ğŸ§ª Batch 47/55 â€” Loss: 0.0741 â€” Time: 0.88s\n","  ğŸ§ª Batch 48/55 â€” Loss: 0.0693 â€” Time: 0.89s\n","  ğŸ§ª Batch 49/55 â€” Loss: 0.0683 â€” Time: 0.89s\n","  ğŸ§ª Batch 50/55 â€” Loss: 0.0706 â€” Time: 0.89s\n","  ğŸ§ª Batch 51/55 â€” Loss: 0.0732 â€” Time: 0.89s\n","  ğŸ§ª Batch 52/55 â€” Loss: 0.0621 â€” Time: 0.89s\n","  ğŸ§ª Batch 53/55 â€” Loss: 0.0727 â€” Time: 0.89s\n","  ğŸ§ª Batch 54/55 â€” Loss: 0.0761 â€” Time: 0.89s\n","  ğŸ§ª Batch 55/55 â€” Loss: 0.0621 â€” Time: 0.64s\n","ğŸ•’ Epoch Time: 52.34s | Avg Batch Time: 0.88s\n","ğŸ“Š Train Loss: 0.0676 | Val Loss: 0.2305 | IoU: 0.8035 | Dice: 0.9018 | Jaccard: 0.8236 | Precision: 0.9132 | Recall: 0.8934 | Accuracy: 0.9762\n","\n","Epoch 19/100\n","  ğŸ§ª Batch 1/55 â€” Loss: 0.0798 â€” Time: 0.89s\n","  ğŸ§ª Batch 2/55 â€” Loss: 0.0600 â€” Time: 0.88s\n","  ğŸ§ª Batch 3/55 â€” Loss: 0.0605 â€” Time: 0.89s\n","  ğŸ§ª Batch 4/55 â€” Loss: 0.0679 â€” Time: 0.88s\n","  ğŸ§ª Batch 5/55 â€” Loss: 0.0911 â€” Time: 0.89s\n","  ğŸ§ª Batch 6/55 â€” Loss: 0.0529 â€” Time: 0.89s\n","  ğŸ§ª Batch 7/55 â€” Loss: 0.0569 â€” Time: 0.89s\n","  ğŸ§ª Batch 8/55 â€” Loss: 0.0640 â€” Time: 0.88s\n","  ğŸ§ª Batch 9/55 â€” Loss: 0.0551 â€” Time: 0.88s\n","  ğŸ§ª Batch 10/55 â€” Loss: 0.0692 â€” Time: 0.89s\n","  ğŸ§ª Batch 11/55 â€” Loss: 0.0590 â€” Time: 0.89s\n","  ğŸ§ª Batch 12/55 â€” Loss: 0.0630 â€” Time: 0.89s\n","  ğŸ§ª Batch 13/55 â€” Loss: 0.0566 â€” Time: 0.89s\n","  ğŸ§ª Batch 14/55 â€” Loss: 0.0637 â€” Time: 0.88s\n","  ğŸ§ª Batch 15/55 â€” Loss: 0.0601 â€” Time: 0.88s\n","  ğŸ§ª Batch 16/55 â€” Loss: 0.0650 â€” Time: 0.89s\n","  ğŸ§ª Batch 17/55 â€” Loss: 0.0664 â€” Time: 0.89s\n","  ğŸ§ª Batch 18/55 â€” Loss: 0.0819 â€” Time: 0.89s\n","  ğŸ§ª Batch 19/55 â€” Loss: 0.0633 â€” Time: 0.89s\n","  ğŸ§ª Batch 20/55 â€” Loss: 0.0807 â€” Time: 0.88s\n","  ğŸ§ª Batch 21/55 â€” Loss: 0.0571 â€” Time: 0.88s\n","  ğŸ§ª Batch 22/55 â€” Loss: 0.0776 â€” Time: 0.89s\n","  ğŸ§ª Batch 23/55 â€” Loss: 0.0536 â€” Time: 0.89s\n","  ğŸ§ª Batch 24/55 â€” Loss: 0.0709 â€” Time: 0.89s\n","  ğŸ§ª Batch 25/55 â€” Loss: 0.0613 â€” Time: 0.88s\n","  ğŸ§ª Batch 26/55 â€” Loss: 0.0547 â€” Time: 0.88s\n","  ğŸ§ª Batch 27/55 â€” Loss: 0.0588 â€” Time: 0.89s\n","  ğŸ§ª Batch 28/55 â€” Loss: 0.0613 â€” Time: 0.89s\n","  ğŸ§ª Batch 29/55 â€” Loss: 0.0690 â€” Time: 0.88s\n","  ğŸ§ª Batch 30/55 â€” Loss: 0.0600 â€” Time: 0.88s\n","  ğŸ§ª Batch 31/55 â€” Loss: 0.0778 â€” Time: 0.88s\n","  ğŸ§ª Batch 32/55 â€” Loss: 0.0584 â€” Time: 0.89s\n","  ğŸ§ª Batch 33/55 â€” Loss: 0.0607 â€” Time: 0.89s\n","  ğŸ§ª Batch 34/55 â€” Loss: 0.0739 â€” Time: 0.89s\n","  ğŸ§ª Batch 35/55 â€” Loss: 0.0633 â€” Time: 0.89s\n","  ğŸ§ª Batch 36/55 â€” Loss: 0.0668 â€” Time: 0.89s\n","  ğŸ§ª Batch 37/55 â€” Loss: 0.0653 â€” Time: 0.89s\n","  ğŸ§ª Batch 38/55 â€” Loss: 0.0698 â€” Time: 0.89s\n","  ğŸ§ª Batch 39/55 â€” Loss: 0.0622 â€” Time: 0.88s\n","  ğŸ§ª Batch 40/55 â€” Loss: 0.0582 â€” Time: 0.88s\n","  ğŸ§ª Batch 41/55 â€” Loss: 0.0523 â€” Time: 0.89s\n","  ğŸ§ª Batch 42/55 â€” Loss: 0.0607 â€” Time: 0.89s\n","  ğŸ§ª Batch 43/55 â€” Loss: 0.0626 â€” Time: 0.89s\n","  ğŸ§ª Batch 44/55 â€” Loss: 0.0572 â€” Time: 0.89s\n","  ğŸ§ª Batch 45/55 â€” Loss: 0.0564 â€” Time: 0.89s\n","  ğŸ§ª Batch 46/55 â€” Loss: 0.0528 â€” Time: 0.89s\n","  ğŸ§ª Batch 47/55 â€” Loss: 0.0606 â€” Time: 0.89s\n","  ğŸ§ª Batch 48/55 â€” Loss: 0.0550 â€” Time: 0.89s\n","  ğŸ§ª Batch 49/55 â€” Loss: 0.0610 â€” Time: 0.89s\n","  ğŸ§ª Batch 50/55 â€” Loss: 0.0552 â€” Time: 0.89s\n","  ğŸ§ª Batch 51/55 â€” Loss: 0.0549 â€” Time: 0.88s\n","  ğŸ§ª Batch 52/55 â€” Loss: 0.0806 â€” Time: 0.88s\n","  ğŸ§ª Batch 53/55 â€” Loss: 0.0740 â€” Time: 0.89s\n","  ğŸ§ª Batch 54/55 â€” Loss: 0.0617 â€” Time: 0.89s\n","  ğŸ§ª Batch 55/55 â€” Loss: 0.0649 â€” Time: 0.65s\n","ğŸ•’ Epoch Time: 52.30s | Avg Batch Time: 0.88s\n","ğŸ“Š Train Loss: 0.0638 | Val Loss: 0.2404 | IoU: 0.7978 | Dice: 0.8950 | Jaccard: 0.8130 | Precision: 0.9277 | Recall: 0.8670 | Accuracy: 0.9750\n","\n","Epoch 20/100\n","  ğŸ§ª Batch 1/55 â€” Loss: 0.0615 â€” Time: 0.87s\n","  ğŸ§ª Batch 2/55 â€” Loss: 0.0589 â€” Time: 0.88s\n","  ğŸ§ª Batch 3/55 â€” Loss: 0.0634 â€” Time: 0.89s\n","  ğŸ§ª Batch 4/55 â€” Loss: 0.0561 â€” Time: 0.88s\n","  ğŸ§ª Batch 5/55 â€” Loss: 0.0497 â€” Time: 0.88s\n","  ğŸ§ª Batch 6/55 â€” Loss: 0.0652 â€” Time: 0.89s\n","  ğŸ§ª Batch 7/55 â€” Loss: 0.0612 â€” Time: 0.89s\n","  ğŸ§ª Batch 8/55 â€” Loss: 0.0647 â€” Time: 0.88s\n","  ğŸ§ª Batch 9/55 â€” Loss: 0.0512 â€” Time: 0.88s\n","  ğŸ§ª Batch 10/55 â€” Loss: 0.0591 â€” Time: 0.88s\n","  ğŸ§ª Batch 11/55 â€” Loss: 0.0541 â€” Time: 0.89s\n","  ğŸ§ª Batch 12/55 â€” Loss: 0.0566 â€” Time: 0.89s\n","  ğŸ§ª Batch 13/55 â€” Loss: 0.0618 â€” Time: 0.88s\n","  ğŸ§ª Batch 14/55 â€” Loss: 0.0585 â€” Time: 0.88s\n","  ğŸ§ª Batch 15/55 â€” Loss: 0.0613 â€” Time: 0.89s\n","  ğŸ§ª Batch 16/55 â€” Loss: 0.0729 â€” Time: 0.89s\n","  ğŸ§ª Batch 17/55 â€” Loss: 0.0582 â€” Time: 0.89s\n","  ğŸ§ª Batch 18/55 â€” Loss: 0.0616 â€” Time: 0.88s\n","  ğŸ§ª Batch 19/55 â€” Loss: 0.0535 â€” Time: 0.89s\n","  ğŸ§ª Batch 20/55 â€” Loss: 0.0535 â€” Time: 0.88s\n","  ğŸ§ª Batch 21/55 â€” Loss: 0.0635 â€” Time: 0.88s\n","  ğŸ§ª Batch 22/55 â€” Loss: 0.0658 â€” Time: 0.89s\n","  ğŸ§ª Batch 23/55 â€” Loss: 0.0517 â€” Time: 0.90s\n","  ğŸ§ª Batch 24/55 â€” Loss: 0.0724 â€” Time: 0.88s\n","  ğŸ§ª Batch 25/55 â€” Loss: 0.0552 â€” Time: 0.89s\n","  ğŸ§ª Batch 26/55 â€” Loss: 0.0479 â€” Time: 0.89s\n","  ğŸ§ª Batch 27/55 â€” Loss: 0.0566 â€” Time: 0.89s\n","  ğŸ§ª Batch 28/55 â€” Loss: 0.0578 â€” Time: 0.89s\n","  ğŸ§ª Batch 29/55 â€” Loss: 0.0606 â€” Time: 0.89s\n","  ğŸ§ª Batch 30/55 â€” Loss: 0.0749 â€” Time: 0.89s\n","  ğŸ§ª Batch 31/55 â€” Loss: 0.0508 â€” Time: 0.88s\n","  ğŸ§ª Batch 32/55 â€” Loss: 0.0598 â€” Time: 0.89s\n","  ğŸ§ª Batch 33/55 â€” Loss: 0.0642 â€” Time: 0.89s\n","  ğŸ§ª Batch 34/55 â€” Loss: 0.0535 â€” Time: 0.89s\n","  ğŸ§ª Batch 35/55 â€” Loss: 0.0569 â€” Time: 0.89s\n","  ğŸ§ª Batch 36/55 â€” Loss: 0.0554 â€” Time: 0.89s\n","  ğŸ§ª Batch 37/55 â€” Loss: 0.0525 â€” Time: 0.89s\n","  ğŸ§ª Batch 38/55 â€” Loss: 0.0588 â€” Time: 0.89s\n","  ğŸ§ª Batch 39/55 â€” Loss: 0.0559 â€” Time: 0.89s\n","  ğŸ§ª Batch 40/55 â€” Loss: 0.0536 â€” Time: 0.89s\n","  ğŸ§ª Batch 41/55 â€” Loss: 0.0538 â€” Time: 0.89s\n","  ğŸ§ª Batch 42/55 â€” Loss: 0.0554 â€” Time: 0.89s\n","  ğŸ§ª Batch 43/55 â€” Loss: 0.0551 â€” Time: 0.89s\n","  ğŸ§ª Batch 44/55 â€” Loss: 0.0499 â€” Time: 0.89s\n","  ğŸ§ª Batch 45/55 â€” Loss: 0.0678 â€” Time: 0.88s\n","  ğŸ§ª Batch 46/55 â€” Loss: 0.0546 â€” Time: 0.88s\n","  ğŸ§ª Batch 47/55 â€” Loss: 0.0532 â€” Time: 0.89s\n","  ğŸ§ª Batch 48/55 â€” Loss: 0.0552 â€” Time: 0.89s\n","  ğŸ§ª Batch 49/55 â€” Loss: 0.0650 â€” Time: 0.89s\n","  ğŸ§ª Batch 50/55 â€” Loss: 0.0571 â€” Time: 0.89s\n","  ğŸ§ª Batch 51/55 â€” Loss: 0.0500 â€” Time: 0.89s\n","  ğŸ§ª Batch 52/55 â€” Loss: 0.0503 â€” Time: 0.89s\n","  ğŸ§ª Batch 53/55 â€” Loss: 0.0502 â€” Time: 0.89s\n","  ğŸ§ª Batch 54/55 â€” Loss: 0.0583 â€” Time: 0.89s\n","  ğŸ§ª Batch 55/55 â€” Loss: 0.0967 â€” Time: 0.65s\n","ğŸ•’ Epoch Time: 52.76s | Avg Batch Time: 0.88s\n","ğŸ“Š Train Loss: 0.0586 | Val Loss: 0.2379 | IoU: 0.8011 | Dice: 0.8969 | Jaccard: 0.8160 | Precision: 0.9258 | Recall: 0.8731 | Accuracy: 0.9755\n","\n","Epoch 21/100\n","  ğŸ§ª Batch 1/55 â€” Loss: 0.0490 â€” Time: 0.89s\n","  ğŸ§ª Batch 2/55 â€” Loss: 0.0545 â€” Time: 0.88s\n","  ğŸ§ª Batch 3/55 â€” Loss: 0.0499 â€” Time: 0.89s\n","  ğŸ§ª Batch 4/55 â€” Loss: 0.0507 â€” Time: 0.89s\n","  ğŸ§ª Batch 5/55 â€” Loss: 0.0552 â€” Time: 0.88s\n","  ğŸ§ª Batch 6/55 â€” Loss: 0.0472 â€” Time: 0.88s\n","  ğŸ§ª Batch 7/55 â€” Loss: 0.0686 â€” Time: 0.89s\n","  ğŸ§ª Batch 8/55 â€” Loss: 0.0661 â€” Time: 0.88s\n","  ğŸ§ª Batch 9/55 â€” Loss: 0.0550 â€” Time: 0.89s\n","  ğŸ§ª Batch 10/55 â€” Loss: 0.0463 â€” Time: 0.89s\n","  ğŸ§ª Batch 11/55 â€” Loss: 0.0505 â€” Time: 0.89s\n","  ğŸ§ª Batch 12/55 â€” Loss: 0.0525 â€” Time: 0.89s\n","  ğŸ§ª Batch 13/55 â€” Loss: 0.0474 â€” Time: 0.89s\n","  ğŸ§ª Batch 14/55 â€” Loss: 0.0457 â€” Time: 0.89s\n","  ğŸ§ª Batch 15/55 â€” Loss: 0.0591 â€” Time: 0.89s\n","  ğŸ§ª Batch 16/55 â€” Loss: 0.0590 â€” Time: 0.88s\n","  ğŸ§ª Batch 17/55 â€” Loss: 0.0565 â€” Time: 0.88s\n","  ğŸ§ª Batch 18/55 â€” Loss: 0.0501 â€” Time: 0.89s\n","  ğŸ§ª Batch 19/55 â€” Loss: 0.0610 â€” Time: 0.89s\n","  ğŸ§ª Batch 20/55 â€” Loss: 0.0523 â€” Time: 0.88s\n","  ğŸ§ª Batch 21/55 â€” Loss: 0.0521 â€” Time: 0.88s\n","  ğŸ§ª Batch 22/55 â€” Loss: 0.0627 â€” Time: 0.89s\n","  ğŸ§ª Batch 23/55 â€” Loss: 0.0615 â€” Time: 0.89s\n","  ğŸ§ª Batch 24/55 â€” Loss: 0.0473 â€” Time: 0.89s\n","  ğŸ§ª Batch 25/55 â€” Loss: 0.0809 â€” Time: 0.89s\n","  ğŸ§ª Batch 26/55 â€” Loss: 0.0575 â€” Time: 0.89s\n","  ğŸ§ª Batch 27/55 â€” Loss: 0.0535 â€” Time: 0.89s\n","  ğŸ§ª Batch 28/55 â€” Loss: 0.0501 â€” Time: 0.89s\n","  ğŸ§ª Batch 29/55 â€” Loss: 0.0502 â€” Time: 0.89s\n","  ğŸ§ª Batch 30/55 â€” Loss: 0.0626 â€” Time: 0.88s\n","  ğŸ§ª Batch 31/55 â€” Loss: 0.0543 â€” Time: 0.89s\n","  ğŸ§ª Batch 32/55 â€” Loss: 0.0487 â€” Time: 0.89s\n","  ğŸ§ª Batch 33/55 â€” Loss: 0.0522 â€” Time: 0.89s\n","  ğŸ§ª Batch 34/55 â€” Loss: 0.0792 â€” Time: 0.89s\n","  ğŸ§ª Batch 35/55 â€” Loss: 0.0530 â€” Time: 0.89s\n","  ğŸ§ª Batch 36/55 â€” Loss: 0.0535 â€” Time: 0.89s\n","  ğŸ§ª Batch 37/55 â€” Loss: 0.0547 â€” Time: 0.89s\n","  ğŸ§ª Batch 38/55 â€” Loss: 0.0516 â€” Time: 0.88s\n","  ğŸ§ª Batch 39/55 â€” Loss: 0.0562 â€” Time: 0.89s\n","  ğŸ§ª Batch 40/55 â€” Loss: 0.0556 â€” Time: 0.89s\n","  ğŸ§ª Batch 41/55 â€” Loss: 0.0589 â€” Time: 0.89s\n","  ğŸ§ª Batch 42/55 â€” Loss: 0.0506 â€” Time: 0.89s\n","  ğŸ§ª Batch 43/55 â€” Loss: 0.0722 â€” Time: 0.89s\n","  ğŸ§ª Batch 44/55 â€” Loss: 0.0611 â€” Time: 0.89s\n","  ğŸ§ª Batch 45/55 â€” Loss: 0.0648 â€” Time: 0.89s\n","  ğŸ§ª Batch 46/55 â€” Loss: 0.0520 â€” Time: 0.88s\n","  ğŸ§ª Batch 47/55 â€” Loss: 0.0541 â€” Time: 0.89s\n","  ğŸ§ª Batch 48/55 â€” Loss: 0.0479 â€” Time: 0.89s\n","  ğŸ§ª Batch 49/55 â€” Loss: 0.0464 â€” Time: 0.89s\n","  ğŸ§ª Batch 50/55 â€” Loss: 0.0618 â€” Time: 0.89s\n","  ğŸ§ª Batch 51/55 â€” Loss: 0.0482 â€” Time: 0.89s\n","  ğŸ§ª Batch 52/55 â€” Loss: 0.0575 â€” Time: 0.89s\n","  ğŸ§ª Batch 53/55 â€” Loss: 0.0451 â€” Time: 0.89s\n","  ğŸ§ª Batch 54/55 â€” Loss: 0.0511 â€” Time: 0.89s\n","  ğŸ§ª Batch 55/55 â€” Loss: 0.0466 â€” Time: 0.65s\n","ğŸ•’ Epoch Time: 52.53s | Avg Batch Time: 0.88s\n","ğŸ“Š Train Loss: 0.0551 | Val Loss: 0.2392 | IoU: 0.8002 | Dice: 0.8970 | Jaccard: 0.8165 | Precision: 0.9340 | Recall: 0.8658 | Accuracy: 0.9756\n","\n","Epoch 22/100\n","  ğŸ§ª Batch 1/55 â€” Loss: 0.0496 â€” Time: 0.87s\n","  ğŸ§ª Batch 2/55 â€” Loss: 0.0446 â€” Time: 0.89s\n","  ğŸ§ª Batch 3/55 â€” Loss: 0.0738 â€” Time: 0.89s\n","  ğŸ§ª Batch 4/55 â€” Loss: 0.0537 â€” Time: 0.89s\n","  ğŸ§ª Batch 5/55 â€” Loss: 0.0486 â€” Time: 0.89s\n","  ğŸ§ª Batch 6/55 â€” Loss: 0.0463 â€” Time: 0.89s\n","  ğŸ§ª Batch 7/55 â€” Loss: 0.0570 â€” Time: 0.88s\n","  ğŸ§ª Batch 8/55 â€” Loss: 0.0478 â€” Time: 0.89s\n","  ğŸ§ª Batch 9/55 â€” Loss: 0.0470 â€” Time: 0.89s\n","  ğŸ§ª Batch 10/55 â€” Loss: 0.0618 â€” Time: 0.89s\n","  ğŸ§ª Batch 11/55 â€” Loss: 0.0539 â€” Time: 0.89s\n","  ğŸ§ª Batch 12/55 â€” Loss: 0.0501 â€” Time: 0.89s\n","  ğŸ§ª Batch 13/55 â€” Loss: 0.0456 â€” Time: 0.89s\n","  ğŸ§ª Batch 14/55 â€” Loss: 0.0484 â€” Time: 0.89s\n","  ğŸ§ª Batch 15/55 â€” Loss: 0.0458 â€” Time: 0.89s\n","  ğŸ§ª Batch 16/55 â€” Loss: 0.0444 â€” Time: 0.89s\n","  ğŸ§ª Batch 17/55 â€” Loss: 0.0500 â€” Time: 0.89s\n","  ğŸ§ª Batch 18/55 â€” Loss: 0.0547 â€” Time: 0.89s\n","  ğŸ§ª Batch 19/55 â€” Loss: 0.0485 â€” Time: 0.89s\n","  ğŸ§ª Batch 20/55 â€” Loss: 0.0529 â€” Time: 0.89s\n","  ğŸ§ª Batch 21/55 â€” Loss: 0.0417 â€” Time: 0.89s\n","  ğŸ§ª Batch 22/55 â€” Loss: 0.0518 â€” Time: 0.89s\n","  ğŸ§ª Batch 23/55 â€” Loss: 0.0460 â€” Time: 0.89s\n","  ğŸ§ª Batch 24/55 â€” Loss: 0.0544 â€” Time: 0.89s\n","  ğŸ§ª Batch 25/55 â€” Loss: 0.0465 â€” Time: 0.89s\n","  ğŸ§ª Batch 26/55 â€” Loss: 0.0531 â€” Time: 0.89s\n","  ğŸ§ª Batch 27/55 â€” Loss: 0.0449 â€” Time: 0.89s\n","  ğŸ§ª Batch 28/55 â€” Loss: 0.0579 â€” Time: 0.89s\n","  ğŸ§ª Batch 29/55 â€” Loss: 0.0526 â€” Time: 0.89s\n","  ğŸ§ª Batch 30/55 â€” Loss: 0.0562 â€” Time: 0.89s\n","  ğŸ§ª Batch 31/55 â€” Loss: 0.0457 â€” Time: 0.88s\n","  ğŸ§ª Batch 32/55 â€” Loss: 0.0715 â€” Time: 0.88s\n","  ğŸ§ª Batch 33/55 â€” Loss: 0.0454 â€” Time: 0.89s\n","  ğŸ§ª Batch 34/55 â€” Loss: 0.0535 â€” Time: 0.89s\n","  ğŸ§ª Batch 35/55 â€” Loss: 0.0477 â€” Time: 0.89s\n","  ğŸ§ª Batch 36/55 â€” Loss: 0.0520 â€” Time: 0.89s\n","  ğŸ§ª Batch 37/55 â€” Loss: 0.0631 â€” Time: 0.89s\n","  ğŸ§ª Batch 38/55 â€” Loss: 0.0458 â€” Time: 0.89s\n","  ğŸ§ª Batch 39/55 â€” Loss: 0.0566 â€” Time: 0.89s\n","  ğŸ§ª Batch 40/55 â€” Loss: 0.0584 â€” Time: 0.89s\n","  ğŸ§ª Batch 41/55 â€” Loss: 0.0498 â€” Time: 0.89s\n","  ğŸ§ª Batch 42/55 â€” Loss: 0.0443 â€” Time: 0.89s\n","  ğŸ§ª Batch 43/55 â€” Loss: 0.0436 â€” Time: 0.89s\n","  ğŸ§ª Batch 44/55 â€” Loss: 0.0589 â€” Time: 0.89s\n","  ğŸ§ª Batch 45/55 â€” Loss: 0.0501 â€” Time: 0.89s\n","  ğŸ§ª Batch 46/55 â€” Loss: 0.0427 â€” Time: 0.89s\n","  ğŸ§ª Batch 47/55 â€” Loss: 0.0535 â€” Time: 0.88s\n","  ğŸ§ª Batch 48/55 â€” Loss: 0.0474 â€” Time: 0.87s\n","  ğŸ§ª Batch 49/55 â€” Loss: 0.0585 â€” Time: 0.89s\n","  ğŸ§ª Batch 50/55 â€” Loss: 0.0440 â€” Time: 0.89s\n","  ğŸ§ª Batch 51/55 â€” Loss: 0.0485 â€” Time: 0.89s\n","  ğŸ§ª Batch 52/55 â€” Loss: 0.0489 â€” Time: 0.89s\n","  ğŸ§ª Batch 53/55 â€” Loss: 0.0499 â€” Time: 0.89s\n","  ğŸ§ª Batch 54/55 â€” Loss: 0.0469 â€” Time: 0.89s\n","  ğŸ§ª Batch 55/55 â€” Loss: 0.0569 â€” Time: 0.65s\n","ğŸ•’ Epoch Time: 52.59s | Avg Batch Time: 0.88s\n","ğŸ“Š Train Loss: 0.0512 | Val Loss: 0.2400 | IoU: 0.8000 | Dice: 0.8963 | Jaccard: 0.8149 | Precision: 0.9286 | Recall: 0.8693 | Accuracy: 0.9754\n","\n","Epoch 23/100\n","  ğŸ§ª Batch 1/55 â€” Loss: 0.0608 â€” Time: 0.89s\n","  ğŸ§ª Batch 2/55 â€” Loss: 0.0472 â€” Time: 0.89s\n","  ğŸ§ª Batch 3/55 â€” Loss: 0.0553 â€” Time: 0.89s\n","  ğŸ§ª Batch 4/55 â€” Loss: 0.0565 â€” Time: 0.88s\n","  ğŸ§ª Batch 5/55 â€” Loss: 0.0503 â€” Time: 0.89s\n","  ğŸ§ª Batch 6/55 â€” Loss: 0.0557 â€” Time: 0.89s\n","  ğŸ§ª Batch 7/55 â€” Loss: 0.0479 â€” Time: 0.89s\n","  ğŸ§ª Batch 8/55 â€” Loss: 0.0398 â€” Time: 0.89s\n","  ğŸ§ª Batch 9/55 â€” Loss: 0.0584 â€” Time: 0.89s\n","  ğŸ§ª Batch 10/55 â€” Loss: 0.0499 â€” Time: 0.89s\n","  ğŸ§ª Batch 11/55 â€” Loss: 0.0432 â€” Time: 0.89s\n","  ğŸ§ª Batch 12/55 â€” Loss: 0.0431 â€” Time: 0.89s\n","  ğŸ§ª Batch 13/55 â€” Loss: 0.0452 â€” Time: 0.89s\n","  ğŸ§ª Batch 14/55 â€” Loss: 0.0449 â€” Time: 0.89s\n","  ğŸ§ª Batch 15/55 â€” Loss: 0.0490 â€” Time: 0.89s\n","  ğŸ§ª Batch 16/55 â€” Loss: 0.0434 â€” Time: 0.89s\n","  ğŸ§ª Batch 17/55 â€” Loss: 0.0438 â€” Time: 0.88s\n","  ğŸ§ª Batch 18/55 â€” Loss: 0.0484 â€” Time: 0.89s\n","  ğŸ§ª Batch 19/55 â€” Loss: 0.0465 â€” Time: 0.89s\n","  ğŸ§ª Batch 20/55 â€” Loss: 0.0448 â€” Time: 0.88s\n","  ğŸ§ª Batch 21/55 â€” Loss: 0.0409 â€” Time: 0.88s\n","  ğŸ§ª Batch 22/55 â€” Loss: 0.0507 â€” Time: 0.88s\n","  ğŸ§ª Batch 23/55 â€” Loss: 0.0412 â€” Time: 0.88s\n","  ğŸ§ª Batch 24/55 â€” Loss: 0.0593 â€” Time: 0.89s\n","  ğŸ§ª Batch 25/55 â€” Loss: 0.0471 â€” Time: 0.89s\n","  ğŸ§ª Batch 26/55 â€” Loss: 0.0459 â€” Time: 0.89s\n","  ğŸ§ª Batch 27/55 â€” Loss: 0.0463 â€” Time: 0.89s\n","  ğŸ§ª Batch 28/55 â€” Loss: 0.0564 â€” Time: 0.89s\n","  ğŸ§ª Batch 29/55 â€” Loss: 0.0477 â€” Time: 0.89s\n","  ğŸ§ª Batch 30/55 â€” Loss: 0.0454 â€” Time: 0.89s\n","  ğŸ§ª Batch 31/55 â€” Loss: 0.0411 â€” Time: 0.89s\n","  ğŸ§ª Batch 32/55 â€” Loss: 0.0502 â€” Time: 0.89s\n","  ğŸ§ª Batch 33/55 â€” Loss: 0.0505 â€” Time: 0.88s\n","  ğŸ§ª Batch 34/55 â€” Loss: 0.0494 â€” Time: 0.88s\n","  ğŸ§ª Batch 35/55 â€” Loss: 0.0488 â€” Time: 0.88s\n","  ğŸ§ª Batch 36/55 â€” Loss: 0.0531 â€” Time: 0.88s\n","  ğŸ§ª Batch 37/55 â€” Loss: 0.0450 â€” Time: 0.89s\n","  ğŸ§ª Batch 38/55 â€” Loss: 0.0462 â€” Time: 0.89s\n","  ğŸ§ª Batch 39/55 â€” Loss: 0.0630 â€” Time: 0.89s\n","  ğŸ§ª Batch 40/55 â€” Loss: 0.0415 â€” Time: 0.88s\n","  ğŸ§ª Batch 41/55 â€” Loss: 0.0509 â€” Time: 0.88s\n","  ğŸ§ª Batch 42/55 â€” Loss: 0.0473 â€” Time: 0.89s\n","  ğŸ§ª Batch 43/55 â€” Loss: 0.0427 â€” Time: 0.89s\n","  ğŸ§ª Batch 44/55 â€” Loss: 0.0475 â€” Time: 0.89s\n","  ğŸ§ª Batch 45/55 â€” Loss: 0.0479 â€” Time: 0.89s\n","  ğŸ§ª Batch 46/55 â€” Loss: 0.0441 â€” Time: 0.89s\n","  ğŸ§ª Batch 47/55 â€” Loss: 0.0453 â€” Time: 0.89s\n","  ğŸ§ª Batch 48/55 â€” Loss: 0.0518 â€” Time: 0.89s\n","  ğŸ§ª Batch 49/55 â€” Loss: 0.0453 â€” Time: 0.88s\n","  ğŸ§ª Batch 50/55 â€” Loss: 0.0514 â€” Time: 0.88s\n","  ğŸ§ª Batch 51/55 â€” Loss: 0.0433 â€” Time: 0.88s\n","  ğŸ§ª Batch 52/55 â€” Loss: 0.0541 â€” Time: 0.89s\n","  ğŸ§ª Batch 53/55 â€” Loss: 0.0464 â€” Time: 0.88s\n","  ğŸ§ª Batch 54/55 â€” Loss: 0.0454 â€” Time: 0.88s\n","  ğŸ§ª Batch 55/55 â€” Loss: 0.0491 â€” Time: 0.65s\n","ğŸ•’ Epoch Time: 52.04s | Avg Batch Time: 0.88s\n","ğŸ“Š Train Loss: 0.0483 | Val Loss: 0.2407 | IoU: 0.8046 | Dice: 0.8974 | Jaccard: 0.8168 | Precision: 0.9252 | Recall: 0.8742 | Accuracy: 0.9755\n","â¹ï¸ Early stopping triggered after 23 epochs.\n","\n","ğŸ§ª Testing model trained on CombinedDataset on all datasets...\n","| Trained On      | Tested On       |     Loss |      IoU |     Dice |   Jaccard |   Precision |   Recall |   Accuracy |\n","|:----------------|:----------------|---------:|---------:|---------:|----------:|------------:|---------:|-----------:|\n","| CombinedDataset | CombinedDataset | 0.171507 | 0.827139 | 0.924821 |  0.860602 |    0.949715 | 0.901897 |    0.98288 |\n","\n","â±ï¸ Total Time for Training + Testing: 1208.55 seconds\n","\n","ğŸ“‹ Final Cross-Dataset Testing Summary:\n","| Trained On      | Tested On       |     Loss |      IoU |     Dice |   Jaccard |   Precision |   Recall |   Accuracy |\n","|:----------------|:----------------|---------:|---------:|---------:|----------:|------------:|---------:|-----------:|\n","| CombinedDataset | CombinedDataset | 0.171507 | 0.827139 | 0.924821 |  0.860602 |    0.949715 | 0.901897 |    0.98288 |\n","U-Net Plus Plus Results\n","        Trained On        Tested On      Loss       IoU      Dice   Jaccard  \\\n","0  CombinedDataset  CombinedDataset  0.171507  0.827139  0.924821  0.860602   \n","\n","   Precision    Recall  Accuracy  \n","0   0.949715  0.901897   0.98288  \n"]}]},{"cell_type":"code","source":["evaluation_df = evaluate_model_on_datasets(trained_model, separate_loaders)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"20Wh6megBi3L","executionInfo":{"status":"ok","timestamp":1745270775387,"user_tz":240,"elapsed":4191,"user":{"displayName":"Ashok Kamath","userId":"01269285531323045015"}},"outputId":"57b15fb5-99f2-4008-c49d-042e22fce14e"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","ğŸ§ª Evaluating trained model on all test sets...\n","\n","ğŸ” Testing on: CVC-ClinicDB\n","ğŸ” Testing on: CVC-ColonDB\n","ğŸ” Testing on: ETIS-LaribPolypDB\n","ğŸ” Testing on: Kvasir-SEG\n","\n","â±ï¸ Total Evaluation Time: 4.15 seconds\n","\n","ğŸ“‹ Test Results Across Datasets (with Average):\n","| Tested On         |     Loss |      IoU |     Dice |   Jaccard |   Precision |   Recall |   Accuracy |\n","|:------------------|---------:|---------:|---------:|----------:|------------:|---------:|-----------:|\n","| CVC-ClinicDB      | 0.121548 | 0.854252 | 0.942273 |  0.891018 |    0.950179 | 0.935242 |   0.989891 |\n","| CVC-ColonDB       | 0.146963 | 0.76528  | 0.920163 |  0.853685 |    0.94232  | 0.901489 |   0.992375 |\n","| ETIS-LaribPolypDB | 0.500283 | 0.486214 | 0.601018 |  0.507218 |    0.97153  | 0.528764 |   0.986279 |\n","| Kvasir-SEG        | 0.210686 | 0.864189 | 0.915843 |  0.845795 |    0.947629 | 0.888326 |   0.975459 |\n","| Average           | 0.24487  | 0.742484 | 0.844824 |  0.774429 |    0.952914 | 0.813456 |   0.986001 |\n"]}]},{"cell_type":"markdown","source":["# DeepLabV3"],"metadata":{"id":"JpkFiTqDZtoq"}},{"cell_type":"code","source":["def deeplabv3_model_factory():\n","    return smp.DeepLabV3(\n","        encoder_name=\"resnet34\",\n","        encoder_weights=\"imagenet\",\n","        in_channels=3,\n","        classes=1,\n","    )"],"metadata":{"id":"GNOe1unSZK9h"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"wh_GhcBrkXOH"},"source":["# BetterNet"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"J6zpk_KakVra"},"outputs":[],"source":["import sys\n","sys.path.append(\"/content/drive/My Drive/GT/DL/GroupProject/BetterNet\")\n","\n","# sys.path.append(\"../BetterNet\")  # add the BetterNet folder to the import path\n","\n","from model import BetterNet  # import the model definition\n","\n","# Define model parameters\n","params = {\n","    \"img_height\": 224,\n","    \"img_width\": 224,\n","    \"img_channels\": 3,\n","    \"mask_channels\": 1  # binary segmentation\n","}\n","\n","# Initialize the BetterNet model\n","model = BetterNet(\n","    input_shape=(params[\"img_height\"], params[\"img_width\"], params[\"img_channels\"]),\n","    num_classes=params[\"mask_channels\"],\n","    dropout_rate=0.5\n",")\n","\n","# Move to GPU if available\n","model = model.cuda() if torch.cuda.is_available() else model\n"]}],"metadata":{"colab":{"provenance":[],"gpuType":"T4","authorship_tag":"ABX9TyMns9O41Oo17f13DK4PzMxk"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"},"accelerator":"GPU"},"nbformat":4,"nbformat_minor":0}